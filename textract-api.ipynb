{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377cae8e-8e75-49e8-932a-e7109e15e41d",
   "metadata": {},
   "source": [
    "# Document Layout Aware Processing and Retrieval Augmented Generation.\n",
    "\n",
    "This notebook was tested on a SageMaker Studio Notebook `Data Science 3.0` kernel and  `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Document Extraction](#Document-Extraction)\n",
    "1. [Document Processing](#Document-Processing)\n",
    "1. [Document Chunking](#Document-Chunking)\n",
    "1. [Indexing](#Indexing)\n",
    "1. [RAG](#RAG)\n",
    "1. [CleanUp](#CleanUp)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3d274-9977-44be-91e2-bf3b4bbbb745",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This example notebook guides you through the process of utilizing Amazon Textract's layout feature. This feature allows you to extract content from your document while maintaining its layout and reading format. Amazon Textract Layout feature is able to detect the following sections:\n",
    "- Titles\n",
    "- Headers\n",
    "- Sub-headers\n",
    "- Text\n",
    "- Tables\n",
    "- Figures\n",
    "- List \n",
    "- Footers\n",
    "- Page Numbers\n",
    "- Key-Value pairs\n",
    "\n",
    "Here is a snippet of Textract Layout feature on a page of Amazon Sustainability report using the Textract Console UI:\n",
    "<img src=\"images/amazonsus2022.jpg\" width=\"1000\"/>\n",
    "\n",
    "The [Amazon Textract Textractor Library](https://aws-samples.github.io/amazon-textract-textractor/index.html) is a library that seamlessly works with Textract features to aid in document processing. You can start by checking out the [examples in the documentation.](https://aws-samples.github.io/amazon-textract-textractor/notebooks/layout_analysis_for_text_linearization.html)\n",
    "This notebook utilizes the Textractor library to interact with Amazon Textract and interpret its response. It enriches the extracted document text with XML tags to delineate sections, facilitating layout-aware chunking and document indexing into a Vector Database (DB). This process aims to enhance Retrieval Augmented Generation (RAG) performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: \n",
    "RAG serves as a technique aimed at enhancing the effectiveness of Large Language Models (LLMs) on lengthy textual content. While widely adopted, implementing RAG necessitates initial processing to extract and segment text into meaningful chunks, especially challenging for intricate assets like PDFs. Many document parsing approaches overlook layout semantics or use simplistic methods like fixed window carving, lacking awareness of document structure or elements. This can disrupt contextual continuity and diminish the performance of RAG systems. An optimal RAG input pipeline would intelligently divide PDF texts into vectorized segments aligned with layout and content semantics, preserving informational integrity for the LLM. In essence, a context-aware parsing phase is pivotal for enabling RAG techniques to realize their full potential, particularly when handling extensive or intricate documents.\n",
    "\n",
    "- **Our approach**: \n",
    "\n",
    "<img src=\"images/txt-layout-Page-2.jpg\" width=\"800\"/>\n",
    "\n",
    "1. Upload multi-page document to Amazon S3.\n",
    "2. Call Amazon Textract Start Document Analysis api call to extract Document Text including Layout and Tables. The response provides structured text aligned with the original document formatting and the pandas tables of each table detected in the document.\n",
    "3. Enrich this extracted text further with XML tags indicating semantic sections, adding contextual metadata through the Textractor library.\n",
    "4. The textrcat library extracts tables in plain text, maintaining their original layout. However, for improved processing and manipulation, it's advisable to convert them to CSV format. This method replaces the plain text tables with their CSV counterparts obtained from Textract's table feature.\n",
    "5.  In this approach, the extracted text is segmented based on document title sections, the highest hierarchy level in a document. Each subsection within the title section is then chunked according to a maximum word threshold. Below outlines our approach to handling the chunking of subsection elements.:\n",
    "\n",
    "    - **Tables:** Tables are chunked row by row until the maximum number of alphanumeric words is reached. For each table chunk, the column headers are added to the table along with the table header, typically the sentence or paragraph preceding the table in the document. This ensures that the information of the table is retained in each chunk.\n",
    "    \n",
    "    <img src=\"images/table-chunkers.png\" width=\"800\" height=700/>\n",
    "    \n",
    "        To handle tables with merged cells, this solution first unmerges any merged cell ranges, then duplicates the original merged cell value into each of the corresponding individual cells after unmerging.\n",
    "    \n",
    "    <img src=\"images/complex-tables.png\" width=\"800\" height=700/>\n",
    "    \n",
    "    - **List:** Chunking lists found in documents can be challenging. Naive chunking methods often split list items by sentence or newline characters. However, this approach presents issues as only the first list chunk typically contains the list title, which provides essential information about the list items. Consequently, subsequent list chunks become obsolete. In this notebook, lists are chunked based on their individual list items. Additionally, the header of the list is appended to each list chunk to ensure that the information of the list is preserved in each chunk.\n",
    "    <img src=\"images/list-chunker.png\" width=\"800\" height=700/>\n",
    "    \n",
    "    - **Section and subsection:** The structure of a document can generally be categorized into titles, sections, and paragraphs. A paragraph is typically the smallest unit of a document that conveys information independently, particularly within the context of a section or subsection header. In this method, text sections are chunked based on paragraphs, and the section header is added to each paragraph chunk (as well as tables and lists) within that section of the document.\n",
    "    <img src=\"images/text-chunks.png\" width=\"800\" height=700/>\n",
    "    \n",
    "6. Metadata is appended to each respective chunk during indexing, encompassing:\n",
    "    - The entire CSV tables detected within the chunk.\n",
    "    - The section header ID associated with the chunk.\n",
    "    - The section title ID linked to the chunk.\n",
    "    \n",
    "    When retrieving a passage based on hybrid search (combining semantic and text matching), there's flexibility in the amount of content forwarded to the LLM. Some queries may necessitate additional information, allowing users to choose whether to send the corresponding chunk subsection or title section based on the specific use case.\n",
    "\n",
    " *Some chunk may exceed the fixed word count threshold due to preserving paragraphs and dealing with complex tables. \n",
    "\n",
    "**Prerequisite:**\n",
    "- [Amazon Bedrock model access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)\n",
    "- [Deploy Embedding and Text Generation Large Language Models with SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-use.html)\n",
    "- Amazon OpenSearch Cluster (Provisioned Cluster or Serverless):\n",
    "    - [Create OpenSearch Service Domain](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/createupdatedomains.html). This solution uses **IAM** as master user for fine-grained access control. **NOTE:** This solution only works with Amazon Opensearch Service version 2.11 and higher.\n",
    "    OR\n",
    "    - [Create OpenSearch Serverless Collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-getting-started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba85156-5ae0-4e53-8aaf-0ec4ef1d7e2b",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e75351-bc80-4416-a8d2-e2be0cadb07b",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa108161-6352-4d2d-b2c9-cad24879b579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting amazon-textract-textractor==1.7.11\n",
      "  Using cached amazon_textract_textractor-1.7.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting Pillow (from amazon-textract-textractor==1.7.11)\n",
      "  Using cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting XlsxWriter<4,>=3.0 (from amazon-textract-textractor==1.7.11)\n",
      "  Using cached xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting amazon-textract-caller<2,>=0.0.27 (from amazon-textract-textractor==1.7.11)\n",
      "  Using cached amazon_textract_caller-0.2.4-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting editdistance<0.9,>=0.6.2 (from amazon-textract-textractor==1.7.11)\n",
      "  Using cached editdistance-0.8.1-cp313-cp313-macosx_15_0_arm64.whl\n",
      "Collecting tabulate<0.10,>=0.9 (from amazon-textract-textractor==1.7.11)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting boto3>=1.26.35 (from amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached boto3-1.38.45-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore (from amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached botocore-1.38.45-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting amazon-textract-response-parser>=0.1.39 (from amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached amazon_textract_response_parser-1.0.3-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting marshmallow<4,>=3.14 (from amazon-textract-response-parser>=0.1.39->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow<4,>=3.14->amazon-textract-response-parser>=0.1.39->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.35->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.26.35->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore->amazon-textract-caller<2,>=0.0.27->amazon-textract-textractor==1.7.11)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached amazon_textract_textractor-1.7.11-py3-none-any.whl (305 kB)\n",
      "Using cached amazon_textract_caller-0.2.4-py2.py3-none-any.whl (13 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Using cached amazon_textract_response_parser-1.0.3-py2.py3-none-any.whl (30 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached boto3-1.38.45-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.38.45-py3-none-any.whl (13.7 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pillow-11.2.1-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Installing collected packages: XlsxWriter, urllib3, tabulate, six, Pillow, packaging, jmespath, editdistance, python-dateutil, marshmallow, botocore, s3transfer, boto3, amazon-textract-response-parser, amazon-textract-caller, amazon-textract-textractor\n",
      "\u001b[2K  Attempting uninstall: XlsxWriter\n",
      "\u001b[2K    Found existing installation: xlsxwriter 3.2.5\n",
      "\u001b[2K    Uninstalling xlsxwriter-3.2.5:\n",
      "\u001b[2K      Successfully uninstalled xlsxwriter-3.2.5\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K  Attempting uninstall: tabulate\n",
      "\u001b[2K    Found existing installation: tabulate 0.9.0\n",
      "\u001b[2K    Uninstalling tabulate-0.9.0:\n",
      "\u001b[2K      Successfully uninstalled tabulate-0.9.0\n",
      "\u001b[2K  Attempting uninstall: six\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [tabulate]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [tabulate]\n",
      "\u001b[2K    Uninstalling six-1.17.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [tabulate]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [six]]\n",
      "\u001b[2K  Attempting uninstall: Pillow━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: pillow 11.2.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling pillow-11.2.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled pillow-11.2.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: packaging[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K  Attempting uninstall: jmespathm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Found existing installation: jmespath 1.0.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Uninstalling jmespath-1.0.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K      Successfully uninstalled jmespath-1.0.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K  Attempting uninstall: editdistance━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Found existing installation: editdistance 0.8.1━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K    Uninstalling editdistance-0.8.1:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [Pillow]\n",
      "\u001b[2K      Successfully uninstalled editdistance-0.8.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K  Attempting uninstall: python-dateutilm━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K  Attempting uninstall: marshmallow\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Found existing installation: marshmallow 3.26.1━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Uninstalling marshmallow-3.26.1:[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K      Successfully uninstalled marshmallow-3.26.1━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K  Attempting uninstall: botocore[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Found existing installation: botocore 1.38.45━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [editdistance]\n",
      "\u001b[2K    Uninstalling botocore-1.38.45:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.38.45m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: s3transfer[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.13.0━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling s3transfer-0.13.0:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.13.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: boto3\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: boto3 1.38.450m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling boto3-1.38.45:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled boto3-1.38.45[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: amazon-textract-response-parser━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: amazon-textract-response-parser 1.0.32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling amazon-textract-response-parser-1.0.3:━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled amazon-textract-response-parser-1.0.3[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: amazon-textract-caller0m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: amazon-textract-caller 0.2.4━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling amazon-textract-caller-0.2.4:0m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled amazon-textract-caller-0.2.4━━━\u001b[0m \u001b[32m10/16\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: amazon-textract-textractor0m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [amazon-textract-caller]\n",
      "\u001b[2K    Found existing installation: amazon-textract-textractor 1.7.11 \u001b[32m14/16\u001b[0m [amazon-textract-caller]\n",
      "\u001b[2K    Uninstalling amazon-textract-textractor-1.7.11:0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [amazon-textract-caller]\n",
      "\u001b[2K      Successfully uninstalled amazon-textract-textractor-1.7.110m \u001b[32m14/16\u001b[0m [amazon-textract-caller]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [amazon-textract-textractor]ct-caller]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.2.1 XlsxWriter-3.2.5 amazon-textract-caller-0.2.4 amazon-textract-response-parser-1.0.3 amazon-textract-textractor-1.7.11 boto3-1.38.45 botocore-1.38.45 editdistance-0.8.1 jmespath-1.0.1 marshmallow-3.26.1 packaging-25.0 python-dateutil-2.9.0.post0 s3transfer-0.13.0 six-1.17.0 tabulate-0.9.0 urllib3-2.5.0\n",
      "Requirement already satisfied: inflect in ./finbud-book-rag/lib/python3.13/site-packages (7.5.0)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in ./finbud-book-rag/lib/python3.13/site-packages (from inflect) (10.7.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in ./finbud-book-rag/lib/python3.13/site-packages (from inflect) (4.4.4)\n",
      "Requirement already satisfied: typing_extensions>=4.14.0 in ./finbud-book-rag/lib/python3.13/site-packages (from typeguard>=4.0.1->inflect) (4.14.0)\n",
      "Requirement already satisfied: requests-aws4auth in ./finbud-book-rag/lib/python3.13/site-packages (1.3.1)\n",
      "Requirement already satisfied: requests in ./finbud-book-rag/lib/python3.13/site-packages (from requests-aws4auth) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./finbud-book-rag/lib/python3.13/site-packages (from requests->requests-aws4auth) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./finbud-book-rag/lib/python3.13/site-packages (from requests->requests-aws4auth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./finbud-book-rag/lib/python3.13/site-packages (from requests->requests-aws4auth) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./finbud-book-rag/lib/python3.13/site-packages (from requests->requests-aws4auth) (2025.6.15)\n",
      "Requirement already satisfied: opensearch-py in ./finbud-book-rag/lib/python3.13/site-packages (3.0.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in ./finbud-book-rag/lib/python3.13/site-packages (from opensearch-py) (2.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.0 in ./finbud-book-rag/lib/python3.13/site-packages (from opensearch-py) (2.32.4)\n",
      "Requirement already satisfied: python-dateutil in ./finbud-book-rag/lib/python3.13/site-packages (from opensearch-py) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in ./finbud-book-rag/lib/python3.13/site-packages (from opensearch-py) (2025.6.15)\n",
      "Requirement already satisfied: Events in ./finbud-book-rag/lib/python3.13/site-packages (from opensearch-py) (0.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./finbud-book-rag/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./finbud-book-rag/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in ./finbud-book-rag/lib/python3.13/site-packages (from python-dateutil->opensearch-py) (1.17.0)\n",
      "Requirement already satisfied: anthropic in ./finbud-book-rag/lib/python3.13/site-packages (0.55.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./finbud-book-rag/lib/python3.13/site-packages (from anthropic) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./finbud-book-rag/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in ./finbud-book-rag/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./finbud-book-rag/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./finbud-book-rag/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./finbud-book-rag/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./finbud-book-rag/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./finbud-book-rag/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Requirement already satisfied: openpyxl in ./finbud-book-rag/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in ./finbud-book-rag/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --force-reinstall amazon-textract-textractor==1.7.11\n",
    "!pip3 install inflect\n",
    "!pip3 install requests-aws4auth\n",
    "!pip3 install opensearch-py\n",
    "!pip3 install anthropic\n",
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63129e-f3c9-41ea-bf48-3d2608a2531a",
   "metadata": {},
   "source": [
    "Restart the Kernel \\\n",
    "Click **kernel** on the top bar and **Restart Kernel**. Continue with the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d14636c-4f02-4b2d-874a-cc44de0c85c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "import io\n",
    "import inflect\n",
    "from collections import OrderedDict\n",
    "import boto3\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl.cell import Cell\n",
    "from openpyxl.worksheet.cell_range import CellRange\n",
    "s3=boto3.client(\"s3\")\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=600, \n",
    "    retries = dict(\n",
    "        max_attempts = 5 \n",
    "    )\n",
    ")\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2fefa-df80-48ec-8cfa-17c1099039f1",
   "metadata": {},
   "source": [
    "Utility functions are provided for embedding generation using select models from Amazon SageMaker Jumpstart and Amazon Bedrock. These models were chosen arbitrarily, but you have the flexibility to customize them by using models of your preference available on Bedrock, SageMaker JumpStart, or HuggingFace.\\\n",
    "**Change the placeholders for sagemaker endpoint names for the respective models below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8144b0ae-4209-4a0d-aa71-6fa8097d1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dictionary `model_dimension_mapping` maps different model names to their respective embedding dimensions.\n",
    "\"\"\"\n",
    "model_dimension_mapping={\"titanv2\":1024,\"titanv1\":1536,\"bge\":1024,\"all-mini-lm\":384,\"e5\":1024}\n",
    "\n",
    "def _get_emb_(passage, model):\n",
    "    \"\"\"\n",
    "    This function takes a passage of text and a model name as input, and returns the corresponding text embedding.\n",
    "    The function first checks the provided model name and then invokes the appropriate model or API to generate the text embedding.  \n",
    "    After invoking the appropriate model or API, the function extracts the text embedding from the response and returns it.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"titanv1\" in model:\n",
    "        response = bedrock.invoke_model(body=json.dumps({\"inputText\":passage}),\n",
    "                                    modelId=\"amazon.titan-embed-text-v1\", \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding=response_body['embedding']\n",
    "    elif \"titanv2\" in model:\n",
    "        response = bedrock.invoke_model(body=json.dumps({\"inputText\":passage,\"dimensions\":1024,\"normalize\":False}),\n",
    "                                    modelId=\"amazon.titan-embed-text-v2:0\", \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding=response_body['embedding']\n",
    "    elif \"all-mini-lm\" in model:\n",
    "        payload = {'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART ALL MINI LM V6 ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    elif \"e5\" in model:\n",
    "        payload = {\"text_inputs\":[passage],\"mode\":\"embedding\"} #{'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART E5 ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    elif \"bge\" in model:\n",
    "        payload = {\"text_inputs\":[passage],\"mode\":\"embedding\"} #{'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART BGE ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f9a42",
   "metadata": {},
   "source": [
    "Utility function to inference Anthropic Claude models on Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5351c441-f635-412d-9318-0a3b5b55cbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_streemer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "                if \"amazon-bedrock-invocationMetrics\" in chunk_obj:\n",
    "                    input_tokens= chunk_obj['amazon-bedrock-invocationMetrics']['inputTokenCount']\n",
    "                    output_tokens=chunk_obj['amazon-bedrock-invocationMetrics']['outputTokenCount']\n",
    "                    print(f\"\\nInput Tokens: {input_tokens}\\nOutput Tokens: {output_tokens}\")\n",
    "    return answer,input_tokens, output_tokens\n",
    "\n",
    "def bedrock_claude_(chat_history,system_message, prompt,model_id,image_path=None):\n",
    "    content=[]\n",
    "    if image_path:       \n",
    "        if not isinstance(image_path, list):\n",
    "            image_path=[image_path]      \n",
    "        for img in image_path:\n",
    "            s3 = boto3.client('s3')\n",
    "            match = re.match(\"s3://(.+?)/(.+)\", img)\n",
    "            image_name=os.path.basename(img)\n",
    "            _,ext=os.path.splitext(image_name)\n",
    "            if \"jpg\" in ext: ext=\".jpeg\"                        \n",
    "            if match:\n",
    "                bucket_name = match.group(1)\n",
    "                key = match.group(2)    \n",
    "                obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                base_64_encoded_data = base64.b64encode(obj['Body'].read())\n",
    "                base64_string = base_64_encoded_data.decode('utf-8')\n",
    "            content.extend([{\"type\":\"text\",\"text\":image_name},{\n",
    "              \"type\": \"image\",\n",
    "              \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": f\"image/{ext.lower().replace('.','')}\",\n",
    "                \"data\": base64_string\n",
    "              }\n",
    "            }])\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "            })\n",
    "    chat_history.append({\"role\": \"user\",\n",
    "            \"content\": content})\n",
    "    prompt = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"system\":system_message,\n",
    "        \"messages\": chat_history\n",
    "    }\n",
    "    answer = \"\"\n",
    "    prompt = json.dumps(prompt)\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(body=prompt, modelId=model_id, accept=\"application/json\", contentType=\"application/json\")\n",
    "    answer,input_tokens,output_tokens=bedrock_streemer(response) \n",
    "    return answer, input_tokens, output_tokens\n",
    "\n",
    "def _invoke_bedrock_with_retries(current_chat, chat_template, question, model_id, image_path):\n",
    "    max_retries = 5\n",
    "    backoff_base = 2\n",
    "    max_backoff = 3  # Maximum backoff time in seconds\n",
    "    retries = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response,input_tokens,output_tokens = bedrock_claude_(current_chat, chat_template, question, model_id, image_path)\n",
    "            return response,input_tokens,output_tokens\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if retries < max_retries:\n",
    "                    # Throttling, exponential backoff\n",
    "                    sleep_time = min(max_backoff, backoff_base ** retries + random.uniform(0, 1))\n",
    "                    time.sleep(sleep_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    raise e\n",
    "            elif e.response['Error']['Code'] == 'ModelStreamErrorException':\n",
    "                if retries < max_retries:\n",
    "                    # Throttling, exponential backoff\n",
    "                    sleep_time = min(max_backoff, backoff_base ** retries + random.uniform(0, 1))\n",
    "                    time.sleep(sleep_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                # Some other API error, rethrow\n",
    "                raise\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7d155-4e4f-4fca-860f-bce713ceee42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document Extraction\n",
    "We employ the Amazon 2024 10K report as an example document. Using the textractor library, we trigger the Amazon Textract `start document analysis` API to initiate an asynchronous process for extracting document text and identifying additional elements like document layout and tables.\\\n",
    "Change **BUCKET** placeholder to your bucket name in Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1d6899-82bd-49aa-8d64-ff4ebfdb235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET=\"finbud-pdf-extraction-rag\" \n",
    "extractor = Textractor(region_name=\"us-east-2\")\n",
    "file=\"irs_p17.pdf\" #Change to file path either in S3 or Local\n",
    "doc_id= os.path.basename(file)\n",
    "file_name, ext = os.path.splitext(file)\n",
    "if file.startswith(\"s3://\"):\n",
    "    document = extractor.start_document_analysis(\n",
    "        file_source=file,\n",
    "        features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "        # client_request_token=doc_id,\n",
    "        save_image=False,\n",
    "        s3_output_path=f\"s3://{BUCKET}/textract-output/{file_name}/\"\n",
    "  \n",
    "    )\n",
    "else:\n",
    "    document = extractor.start_document_analysis(\n",
    "        file_source=file,\n",
    "        features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "        # client_request_token=doc_id,\n",
    "        save_image=True,\n",
    "        s3_upload_path=f\"s3://{BUCKET}\",\n",
    "        s3_output_path=f\"s3://{BUCKET}/textract-output/{file_name}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebf919-47a1-462d-8f21-e7ffe23cf6c3",
   "metadata": {},
   "source": [
    "By leveraging the Textractor linearization function, we enhance the extracted content with XML tags while concealing certain page sections such as headers, footers, and non-essential images.\n",
    "\n",
    "We opt to tag tables, lists, title sections, and sub-sections to facilitate the efficient identification and chunking of these document elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f086761-25bd-4535-b1b1-10724abceae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1700x2200 at 0x12697CD60>\n"
     ]
    }
   ],
   "source": [
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "config = TextLinearizationConfig(\n",
    "    hide_figure_layout=False,\n",
    "    title_prefix=\"<titles><<title>><title>\",\n",
    "    title_suffix=\"</title><</title>>\",\n",
    "    hide_header_layout=True,\n",
    "    section_header_prefix=\"<headers><<header>><header>\",\n",
    "    section_header_suffix=\"</header><</header>>\",\n",
    "    table_prefix=\"<tables><table>\",\n",
    "    table_suffix=\"</table>\",\n",
    "    list_layout_prefix=\"<<list>><list>\",\n",
    "    list_layout_suffix=\"</list><</list>>\",\n",
    "    hide_footer_layout=True,\n",
    "    hide_page_num_layout=True,\n",
    ")\n",
    "\n",
    "# print(document.pages[5].get_text(config=config))\n",
    "print(document.pages[5].visualize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a89081-022e-4451-b3be-38fd4f697e0d",
   "metadata": {},
   "source": [
    "## Document Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05fa55-fe4e-462e-9908-b8cae7d731d2",
   "metadata": {},
   "source": [
    "This code snippet comprises a Python function `split_list_items_` and a script segment that processes a document containing tables and text, converting tables into CSV format and maintaining the document structure with text and tables.\n",
    "\n",
    "The function `split_list_items_` takes a string as input, likely representing a document with nested lists marked by specific XML tags. It parses this string, extracting items and handling nested lists appropriately. The function then returns a list containing the extracted items.\n",
    "\n",
    "The script segment following the function processes each page of the document. It identifies tables, converts them to CSV format, and wraps them with XML tags for identification. If lists are present in the document, the script utilizes the `split_list_items_` function to handle them. The processed content is stored in dictionaries for further use.\n",
    "\n",
    "The `layout_table_to_excel` loads a pandas dataframe in excel format to handle spanned columns/rows in complex tables. It duplicates the spanned row/columns value across corresponding spanned cells to help keep the intergrity of complex tables.\n",
    "\n",
    "This script segment efficiently manages document content, ensuring tables are properly formatted while preserving the document's structure with text and lists. It serves to handle data extraction and processing tasks involving documents with mixed content types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7211dead-9643-4afb-b9d0-cc59f905ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def strip_newline(cell):\n",
    "    \"\"\"\n",
    "    A utility function to strip newline characters from a cell.\n",
    "    Parameters:\n",
    "    cell (str): The cell value.\n",
    "    Returns:\n",
    "    str: The cell value with newline characters removed.\n",
    "    \"\"\"\n",
    "    return str(cell).strip()\n",
    "\n",
    "def layout_table_to_excel(document, ids,csv_seperator):    \n",
    "    \"\"\"\n",
    "    Converts an Excel table from a document to a Pandas DataFrame, \n",
    "    handling duplicated values across merged cells.\n",
    "\n",
    "    Args:\n",
    "        document: Document containing Excel table \n",
    "        ids: ID of the Excel table in the document\n",
    "        csv_seperator: Separator for CSV string conversion\n",
    "\n",
    "    Returns: \n",
    "        Pandas DataFrame representation of the Excel table\n",
    "    \"\"\"\n",
    "    # save the table in excel format to preserve the structure of any merged cells\n",
    "    buffer = io.BytesIO()    \n",
    "    document.tables[ids].to_excel(buffer)\n",
    "    buffer.seek(0)\n",
    "    # Load workbook, get active worksheet\n",
    "    wb = openpyxl.load_workbook(buffer)\n",
    "    worksheet = wb.active\n",
    "    # Unmerge cells, duplicate merged values to individual cells\n",
    "    all_merged_cell_ranges: list[CellRange] = list(\n",
    "            worksheet.merged_cells.ranges\n",
    "        )\n",
    "    for merged_cell_range in all_merged_cell_ranges:\n",
    "        merged_cell: Cell = merged_cell_range.start_cell\n",
    "        worksheet.unmerge_cells(range_string=merged_cell_range.coord)\n",
    "        for row_index, col_index in merged_cell_range.cells:\n",
    "            cell: Cell = worksheet.cell(row=row_index, column=col_index)\n",
    "            cell.value = merged_cell.value\n",
    "    # determine table header index\n",
    "    df = pd.DataFrame(worksheet.values)\n",
    "    df=df.map(strip_newline)\n",
    "    df0=df.to_csv(sep=csv_seperator,index=False, header=None)\n",
    "    row_count=len([x for x in df0.split(\"\\n\") if x])\n",
    "    if row_count>1:\n",
    "        if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)): \n",
    "            row_count=1\n",
    "    # attach table column names\n",
    "    column_row=0 if row_count==1 else 1\n",
    "    df.columns = df.iloc[column_row] \n",
    "    df = df[column_row+1:]\n",
    "    return df\n",
    "\n",
    "def split_list_items_(items):\n",
    "    \"\"\"\n",
    "    Splits the given string into a list of items, handling nested lists.\n",
    "\n",
    "    Parameters:\n",
    "    items (str): The input string containing items and possibly nested lists.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the items extracted from the input string.\n",
    "    \"\"\"\n",
    "    parts = re.split(\"(<<list>><list>|</list><</list>>)\", items)  \n",
    "    output = []\n",
    "\n",
    "    inside_list = False\n",
    "    list_item = \"\"\n",
    "\n",
    "    for p in parts:\n",
    "        if p == \"<<list>><list>\":\n",
    "            inside_list = True    \n",
    "            list_item=p\n",
    "        elif p == \"</list><</list>>\":\n",
    "            inside_list = False\n",
    "            list_item += p\n",
    "            output.append(list_item)\n",
    "            list_item = \"\" \n",
    "        elif inside_list:\n",
    "            list_item += p.strip()\n",
    "        else:\n",
    "            output.extend(p.split('\\n'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a011ade1-bd57-49e3-a531-297e3c34d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\"\"\"\n",
    "This script processes a document containing tables and text. It converts the tables into CSV format \n",
    "and wraps them with XML tags for easy identification. The document structure with text and tables is maintained.\n",
    "\"\"\"\n",
    "csv_seperator=\"|\" #\"\\t\"\n",
    "document_holder={}\n",
    "table_page={}\n",
    "count=0\n",
    "# Whether to handle merged cells by duplicating merged value across corresponding individual cells\n",
    "unmerge_span_cells=True \n",
    "# Loop through each page in the document\n",
    "for ids,page in enumerate(document.pages):\n",
    "    table_count=len([word for word in page.get_text(config=config).split() if \"<tables><table>\" in word]) # get the number of table in the extracted document page by header we set earlier\n",
    "    assert table_count==len(page.tables) # check that number of tables per page is same as *tables extracted by textract TABLE feature\n",
    "    content=page.get_text(config=config).split(\"<tables>\")\n",
    "    document_holder[ids]=[]    \n",
    "    for idx,item in enumerate(content):\n",
    "        if \"<table>\" in item:           \n",
    "            if unmerge_span_cells:\n",
    "                df=layout_table_to_excel(document, count,csv_seperator)\n",
    "            else:\n",
    "                df0=  document.tables[count].to_pandas(use_columns=False).to_csv(header=False, index=None,sep=csv_seperator)\n",
    "                row_count=len([x for x in df0.split(\"\\n\") if x]) #Check the number of rows in the parsed table to determine how to read the table headers. if table row count is 1 then headers is obviously at 0 else headers may or may not be at 0\n",
    "                #Check if the first row in the csv is empty headers\n",
    "                if row_count>1:\n",
    "                    if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)): \n",
    "                        row_count=1\n",
    "                df=pd.read_csv(io.StringIO(df0), sep=csv_seperator, \n",
    "                               header=0 if row_count==1 else 1, keep_default_na=False) # read table with appropiate column headers\n",
    "                df.rename(columns=lambda x: '' if str(x).startswith('Unnamed:') else x, inplace=True) \n",
    "            table=df.to_csv(index=None, sep=csv_seperator)\n",
    "\n",
    "            if ids in table_page:\n",
    "                table_page[ids].append(table)\n",
    "            else:\n",
    "                table_page[ids]=[table]\n",
    "            # Extract table data and remaining content\n",
    "            pattern = re.compile(r'<table>(.*?)(</table>)', re.DOTALL) \n",
    "            data=item\n",
    "            table_match = re.search(pattern, data)\n",
    "            table_data = table_match.group(1) if table_match else '' \n",
    "            remaining_content = data[table_match.end():] if table_match else data            \n",
    "            content[idx]=f\"<<table>><table>{table}</table><</table>>\" ## attach xml tags to differentiate table from other text\n",
    "            count+=1\n",
    "            # Check for list items in remaining content\n",
    "            if \"<<list>>\" in remaining_content:\n",
    "                output=split_list_items_(remaining_content)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend([content[idx]]+output)           \n",
    "            else:\n",
    "                document_holder[ids].extend([content[idx]]+[x.strip() for x in remaining_content.split('\\n') if x.strip()]) # split other text by new line to be independent items in the python list.\n",
    "        else:   \n",
    "            # Check for list items and tables in remaining content\n",
    "            if \"<<list>>\" in item and \"<table>\" not in item:   \n",
    "                output=split_list_items_(item)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend(output)\n",
    "            else:\n",
    "                document_holder[ids].extend([x.strip() for x in item.split(\"\\n\") if x.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4f4ba-e1f4-4f9d-a7ec-49f9eb3ead44",
   "metadata": {},
   "source": [
    "Here we first flatten a nested list into a single list and then join its elements using newline characters. Subsequently, the string is split into segments based on the `<titles>` tag (split by title section hierarchy), generating a list of sub-section segments. Following this, the function `sub_header_content_splitta` is defined to process a string, splitting it by XML tags and extracting text segments, excluding segments containing specific XML tags such as `<header>`, `<list>`, or `<table>`. This function takes a string as input, applies a regular expression pattern to split it by XML tags, and iterates through the resulting segments to filter out those containing the specified XML tags. The extracted text segments are then returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21eb2fde-7547-4390-831b-5c13b198d775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Flatten the nested list document_holder into a single list and Join the flattened list by \"\\n\"\n",
    "flattened_list = [item for sublist in document_holder.values() for item in sublist]\n",
    "result = \"\\n\".join( flattened_list)\n",
    "header_split=result.split(\"<titles>\")\n",
    "\n",
    "def sub_header_content_splitta(string):   \n",
    "    \"\"\"\n",
    "    Splits the input string by XML tags and returns a list containing the segments of text,\n",
    "    excluding segments containing specific XML tags such as \"<header>\", \"<list>\", or \"<table>\".\n",
    "\n",
    "    Parameters:\n",
    "    string (str): The input string to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the segments of text extracted from the input string.\n",
    "    \"\"\" \n",
    "    pattern = re.compile(r'<<[^>]+>>')\n",
    "    segments = re.split(pattern, string)\n",
    "    result = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            if \"<header>\" not in segment and \"<list>\" not in segment and  \"<table>\" not in segment:\n",
    "                segment=[x.strip() for x in segment.split('\\n') if x.strip()]\n",
    "                result.extend(segment)\n",
    "            else:\n",
    "                result.append(segment)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd9cf4-ee87-4233-bd12-01f50c00b8b7",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "This cell iterates through the document per title section and chunks content within each sub-sections in the following manner:\n",
    "- It uses number of words as chunking threshold.\n",
    "- It looks for the different xml tags to identify the different document content types. \n",
    "    - Iterating through the various sub-section within a section title identified by the **sub-section header** and only chunking contents within each sub-section. No chunk include multiple subsection content even if the max words threshold has not been met.\n",
    "    - If a table xml tag is found, it checks if there is a sentence before that table (the heueristics employed here is that the sentence before a table is usually the table header) and use it as table headers. It then splits table by rows until desired chunk is achieved and appends the corresponding section header to the table chunk.\n",
    "    - If a list is found, split list by items until desired chunk is achieved. Employ same heuristics as above and append list headers to all list chunk.\n",
    "    - For other text, it chunks by paragraphs and appends each sub-section header to the corresponding chunks.\n",
    "- A dicionary containing each complete sub-section is also stored to be used as metadata during indexing.\n",
    "- The complete table found in each chunk is also stored for metadata purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "919c8154-00b5-466b-b3f6-54f91cde19fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "max_words = 200\n",
    "chunks = {}\n",
    "table_header_dict={} \n",
    "chunk_header_mapping={}\n",
    "list_header_dict={}\n",
    "\n",
    "# iterate through each title section\n",
    "for title_ids, items in enumerate(header_split):\n",
    "    title_chunks = []\n",
    "    current_chunk = []\n",
    "    num_words = 0   \n",
    "    table_header_dict[title_ids]={}\n",
    "    chunk_header_mapping[title_ids]={}\n",
    "    list_header_dict[title_ids]={}\n",
    "    chunk_counter=0\n",
    "    for item_ids,item in enumerate(items.split('<headers>')): #headers\n",
    "        # print(\"\".join(current_chunk).strip())\n",
    "        lines=sub_header_content_splitta(item)             \n",
    "        SECTION_HEADER=None \n",
    "        TITLES=None\n",
    "        num_words = 0  \n",
    "        for ids_line,line in enumerate(lines): #header lines  \n",
    "            \n",
    "            if line.strip():\n",
    "                if \"<title>\" in line:   \n",
    "                    TITLES=re.findall(r'<title>(.*?)</title>', line)[0].strip()\n",
    "                    line=TITLES \n",
    "                    if re.sub(r'<[^>]+>', '', \"\".join(lines)).strip()==TITLES:\n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                if \"<header>\" in line:   \n",
    "                    SECTION_HEADER=re.findall(r'<header>(.*?)</header>', line)[0].strip()\n",
    "                    line=SECTION_HEADER    \n",
    "                    first_header_portion=True\n",
    "                next_num_words = num_words + len(re.findall(r'\\w+', line))  \n",
    "\n",
    "                if  \"<table>\" not in line and \"<list>\" not in line:\n",
    "                    if next_num_words > max_words and \"\".join(current_chunk).strip()!=SECTION_HEADER and current_chunk and \"\".join(current_chunk).strip()!=TITLES:\n",
    "                \n",
    "                        if SECTION_HEADER :\n",
    "                            if first_header_portion:\n",
    "                                first_header_portion=False                                            \n",
    "                            else:\n",
    "                                current_chunk.insert(0, SECTION_HEADER.strip())                       \n",
    "                        \n",
    "                        title_chunks.append(current_chunk)                  \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "               \n",
    "                        current_chunk = []\n",
    "                        num_words = 0 \n",
    "                        chunk_counter+=1\n",
    "             \n",
    "                    current_chunk.append(line)    \n",
    "                    num_words += len(re.findall(r'\\w+', line))\n",
    "\n",
    "                \"\"\"\n",
    "                Goal is to segment out table items and chunks intelligently.\n",
    "                We chunk the table by rows and for each chunk of the table we append the table column headers\n",
    "                and table headers if any. This way we preserve the table information across each chunks.\n",
    "                This will help improve semantic search where all the chunks relating to a table would be in the \n",
    "                top k=n response giving the LLM mcomplet information on the table.\n",
    "                \"\"\"\n",
    "\n",
    "                if \"<table>\" in line:\n",
    "                    # Get table header which is usually line before table in document              \n",
    "                    line_index=lines.index(line)\n",
    "                    if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table is first item on the page, then they wont be a header (header may be included it table) and also if table is the the last item in the list\n",
    "                        header=lines[line_index-1].replace(\"<header>\",\"\").replace(\"</header>\",\"\")\n",
    "                    else:\n",
    "                        header=\"\"                   \n",
    "              \n",
    "                    table = line.split(\"<table>\")[-1].split(\"</table>\")[0] # get table from demarcators              \n",
    "                    df=pd.read_csv(io.StringIO(table), sep=csv_seperator, keep_default_na=False,header=None)\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df[1:]\n",
    "                    df.rename(columns=lambda x: '' if str(x).startswith('Unnamed:') else x, inplace=True)                    \n",
    "                    table_chunks = []\n",
    "                    curr_chunk = [df.columns.to_list()] #start current chunk with table column names    \n",
    "                    words=len(re.findall(r'\\w+', str(current_chunk)+\" \"+str(curr_chunk)))  \n",
    "                    # Iterate through the rows in the table\n",
    "                    for row in df.itertuples(index=False):\n",
    "                        curr_chunk.append(row)         \n",
    "                        words+=len(re.findall(r'\\w+', str(row)))\n",
    "                        if words > max_words:                        \n",
    "                            if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                                table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                            else:\n",
    "                                table_header_dict[title_ids][chunk_counter]=[header]+[table]                            \n",
    "                            table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])) #join chunk lines together to for a csv \n",
    "                            tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]) #join chunk lines together to for a csv\n",
    "                            words = len(re.findall(r'\\w+', str(curr_chunk[0]))) # set word count to word length of column header names\n",
    "                            if header: #If header  attach header to table                         \n",
    "                                if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower(): #check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                    current_chunk.pop(-1)\n",
    "                                # Append section header to table\n",
    "                                if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())                             \n",
    "                                current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk]) #enrich table header with ':'\n",
    "                                title_chunks.append(current_chunk)                           \n",
    "                        \n",
    "                            else:\n",
    "                                if SECTION_HEADER:\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())                                \n",
    "                                current_chunk.extend([tab_chunk])\n",
    "                                title_chunks.append(current_chunk)                        \n",
    "                            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                            chunk_counter+=1\n",
    "                            num_words=0\n",
    "                            current_chunk=[]\n",
    "                            curr_chunk = [curr_chunk[0]]\n",
    "                    \n",
    "                    if curr_chunk != [df.columns.to_list()] and lines.index(line) == len(lines)-1: #if table chunk still remaining and table is last item in page append as last chunk\n",
    "                        table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                        tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])                        \n",
    "                        if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                        else:\n",
    "                            table_header_dict[title_ids][chunk_counter]=[header]+[table]   \n",
    "                        \n",
    "                        if header: \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                          \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                            title_chunks.append(current_chunk)                   \n",
    "                        else:\n",
    "                            if SECTION_HEADER:\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                            \n",
    "                            current_chunk.extend([tab_chunk])\n",
    "                            title_chunks.append(current_chunk)             \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                    elif curr_chunk != [df.columns.to_list()] and lines.index(line) != len(lines)-1: #if table is not last item in page and max word threshold is not reached, send no next loop\n",
    "                        table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                        tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                        \n",
    "                        if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                        else:\n",
    "                            table_header_dict[title_ids][chunk_counter]=[header]+[table]                         \n",
    "                        if header:               \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                        else:\n",
    "                            current_chunk.extend([tab_chunk])                  \n",
    "                        num_words=words\n",
    "                     \n",
    "\n",
    "                \"\"\"\n",
    "                Goal is to segment out list items and chunk intelligently.\n",
    "                We chunk each list by items in the list and \n",
    "                for each list chunk we append the list header to the chunk to preserve the information of the list across chunks.\n",
    "                This would boost retrieval process where question pertaining to a list will have all list chunks within\n",
    "                the topK=n responses.\n",
    "                \"\"\"\n",
    "\n",
    "                if \"<list>\" in line:\n",
    "                    # Get list header which is usually line before list in document\n",
    "                    line_index=lines.index(line)\n",
    "                    if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table or list is the previous item on the page, then they wont be a header\n",
    "                        header=lines[line_index-1].replace(\"<header>\",\"\").replace(\"</header>\",\"\")\n",
    "                    else:\n",
    "                        header=\"\"           \n",
    "                    list_pattern = re.compile(r'<list>(.*?)(?:</list>|$)', re.DOTALL)   ## Grab all list contents within the list xml tags        \n",
    "                    list_match = re.search(list_pattern, line)\n",
    "                    list_ = list_match.group(1)\n",
    "                    list_lines=list_.split(\"\\n\")                \n",
    "\n",
    "                    curr_chunk = []  \n",
    "                    words=len(re.findall(r'\\w+', str(current_chunk)))  #start word count from any existing chunk\n",
    "                    # Iterate through the items in the list\n",
    "                    for lyst_item in list_lines:\n",
    "                        curr_chunk.append(lyst_item)         \n",
    "                        words+=len(re.findall(r'\\w+', lyst_item)) \n",
    "                        if words >= max_words: # \n",
    "                            if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                                list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                            else:\n",
    "                                list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                            words=0     \n",
    "                            list_chunk=\"\\n\".join(curr_chunk)\n",
    "                            if header: # attach list header                       \n",
    "                                if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                    current_chunk.pop(-1)  \n",
    "                                # Append section content header to list\n",
    "                                if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())\n",
    "                                    \n",
    "                                current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk]) \n",
    "                                title_chunks.append(current_chunk)                          \n",
    "                         \n",
    "                            else:\n",
    "                                if SECTION_HEADER:\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())\n",
    "                                    \n",
    "                                current_chunk.extend([list_chunk])\n",
    "                                title_chunks.append(current_chunk)                            \n",
    "                            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                            chunk_counter+=1\n",
    "                            num_words=0\n",
    "                            current_chunk=[]\n",
    "                            curr_chunk = []\n",
    "                    if curr_chunk  and lines.index(line) == len(lines)-1: #if list chunk still remaining and list is last item in page append as last chunk\n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                        else:\n",
    "                            list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                        if header: \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower(): #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1)                            \n",
    "                            if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                   \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                            title_chunks.append(current_chunk)                        \n",
    "                        else:\n",
    "                            if SECTION_HEADER:\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                   \n",
    "                            current_chunk.extend([list_chunk])\n",
    "                            title_chunks.append(current_chunk)                     \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                    elif curr_chunk and lines.index(line) != len(lines)-1: #if list is not last item in page and max word threshold is not reached, send to next loop          \n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                        else:\n",
    "                            list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                        if header:               \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                        else:\n",
    "                            current_chunk.extend([list_chunk])                  \n",
    "                        num_words=words\n",
    "\n",
    "\n",
    "        if current_chunk and \"\".join(current_chunk).strip()!=SECTION_HEADER and \"\".join(current_chunk).strip()!=TITLES:\n",
    "    \n",
    "            if SECTION_HEADER:\n",
    "                if first_header_portion:\n",
    "                    first_header_portion=False\n",
    "                else:\n",
    "                    current_chunk.insert(0, SECTION_HEADER.strip())         \n",
    "            title_chunks.append(current_chunk)\n",
    "            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "            current_chunk=[]\n",
    "            chunk_counter+=1\n",
    "    if current_chunk:\n",
    "  \n",
    "        title_chunks.append(current_chunk) \n",
    "        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "    chunks[title_ids] = title_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9501b756-ba00-48f7-a400-49bae1d834d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0:\n",
      "Table 1-2. 2024 Filing Requirements for Dependents\n",
      "See chapter 3 to find out if someone can claim you as a dependent.\n",
      "If your parents (or someone else) can claim you as a dependent, use this table to see if you must file a return. (See Table 1-3 for other situations when you must file.) In this table, unearned income includes taxable interest, ordinary dividends, and capital gain distributions. It also includes unemployment compensation, taxable social security benefits, pensions, annuities, and distributions of unearned income from a trust. Earned income includes salaries, wages, tips, professional fees, and taxable scholarship and fellowship grants. (See Scholarships and fellowships in chapter 8.) Gross income is the total of your earned and unearned income.\n",
      "Single dependents-Were you either age 65 or older or blind?:\n",
      "No. You must file a return if any of the following apply. \n",
      "Your unearned income was more than $1,300. \n",
      "Your earned income was more than $14,600. \n",
      "Your gross income was more than the larger of: \n",
      "$1,300 or \n",
      "Your earned income (up to $14,150) plus $450. \n",
      "Yes. You must file a return if any of the following apply. \n",
      "Your unearned income was more than $3,250 ($5,200 if 65 or older and blind). \n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "Single dependents-Were you either age 65 or older or blind?:\n",
      "Your earned income was more than $16,550 ($18,500 if 65 or older and blind). \n",
      "Your gross income was more than the larger of: \n",
      "$3,250 ($5,200 if 65 or older and blind), or \n",
      "Your earned income (up to $14,150) plus $2,400 ($4,350 if 65 or older and blind). \n",
      "Married dependents-Were you either age 65 or older or blind? \n",
      "No. You must file a return if any of the following apply. \n",
      "Your unearned income was more than $1,300. \n",
      "Your earned income was more than $14,600. \n",
      "Your gross income was at least $5 and your spouse files a separate return and itemizes deductions. \n",
      "Your gross income was more than the larger of: \n",
      "$1,300, or \n",
      "Your earned income (up to $14,150) plus $450. \n",
      "Yes. You must file a return if any of the following apply. \n",
      "Your unearned income was more than $2,850 ($4,400 if 65 or older and blind). \n",
      "Your earned income was more than $16,150 ($17,700 if 65 or older and blind). \n",
      "Your gross income was at least $5 and your spouse files a separate return and itemizes deductions. \n",
      "Your gross income was more than the larger of: \n",
      "\n",
      "\n",
      "Chunk 2:\n",
      "Single dependents-Were you either age 65 or older or blind?:\n",
      "$2,850 ($4,400 if 65 or older and blind), or \n",
      "Your earned income (up to $14,150) plus $2,000 ($3,550 if 65 or older and blind).\n",
      "safely and securely. Using e-file doesn't affect your chances of an IRS examination of your re- turn.\n",
      "Requirements for an electronic return. The requirements for signing an electronic return will be different depending on whether you use tax software or a tax practitioner. If you are filing electronically using Direct File, you should fol- low the instructions provided to you by Direct File for how to sign your electronic return. Oth- erwise, to file your return electronically, you must sign the return electronically using a per- sonal identification number (PIN) and provide the information described below. If you are filing online using software, you must use a Self-Se- lect PIN. If you are filing electronically using a tax practitioner, you can use a Self-Select PIN or a Practitioner PIN.\n",
      "\n",
      "\n",
      "Chunk 3:\n",
      "described in more detail next), all six digits of your IP PIN must appear in the IP PIN spaces provided next to the space for your occupation for your electronic signature to be complete. Failure to include an issued IP PIN on the elec- tronic return will result in an invalid signature and a rejected return. If you are filing a joint re- turn and both taxpayers were issued IP PINs, enter both IP PINs in the spaces provided.\n",
      "If we issued you an identity protection per- sonal identification number (IP PIN) (as\n",
      "Self-Select PIN. The Self-Select PIN method allows you to create your own PIN. If you are married filing jointly, you and your spouse will each need to create a PIN and enter these PINs as your electronic signatures.\n",
      "A PIN is any combination of five digits you choose except five zeros. If you use a PIN, there is nothing to sign and nothing to mail-not even your Forms W-2.\n",
      "Your electronic return signed with a Self-Se- lect PIN is considered a validly signed return\n",
      "\n",
      "\n",
      "Chunk 4:\n",
      "only when it includes your PIN; last name; date of birth; IP PIN, if applicable; and your adjusted gross income (AGI) from your originally filed 2023 federal income tax return, if applicable. If you're filing jointly, your electronic return must also include your spouse's PIN; last name; date of birth; IP PIN, if applicable; and AGI, if appli- cable, in order to be considered validly signed. (You, and your spouse if filing jointly, may each use your own prior-year pin to verify your iden- tity if you filed electronically last year. If you use your prior-year PIN or enter your IP PIN, you are not required to enter your prior-year AGI. The prior-year PIN is the five-digit PIN you used to electronically sign your 2023 return.)\n",
      "\n",
      "\n",
      "Chunk 5:\n",
      "If you need your AGI from your originally filed 2023 federal income tax return, and you don't have your 2023 income tax return, you can access your transcript through your online ac- count at IRS.gov/Account. You can also go to IRS.gov/Transcript or call the IRS at 800-908-9946 to get a free transcript of your re- turn. Don't use your AGI from an amended re- turn (Form 1040-X) or a math error correction made by the IRS. AGI is the amount shown on your 2023 Form 1040 or 1040-SR, line 11.\n",
      "For more information, go to IRS.gov/Efile.\n",
      "!\n",
      "CAUTION\n",
      "You can't use the Self-Select PIN method if you are a first-time filer under age 16 at the end of 2024.\n",
      "\n",
      "\n",
      "Chunk 6:\n",
      "Practitioner PIN. The Practitioner PIN method allows you to authorize your tax practitioner to enter or generate your PIN. Your electronic re- turn is considered a validly signed return only when it includes your PIN; last name; date of birth; and IP PIN, if applicable. If you're filing jointly, your electronic return must also include your spouse's PIN; last name; date of birth; and IP PIN, if applicable, in order to be considered a validly signed return. The practitioner can pro- vide you with details.\n",
      "Form 8453. You must send in a paper Form 8453 if you have to attach certain forms or other documents that can't be electronically filed. See Form 8453.\n",
      "Identity Protection PIN (IP PIN). If the IRS gave you an IP PIN, enter it in the spaces provi- ded on your tax form. If the IRS hasn't given you this type of number, leave these spaces blank. For more information, see the Instructions for Form 1040.\n",
      "TIP\n",
      "All taxpayers are now eligible for an IP PIN. For more information, see Pub. 5477. To apply for an IP PIN, go to IRS.gov/IPPIN and use the Get an IP PIN tool.\n",
      "\n",
      "\n",
      "Chunk 7:\n",
      "Power of attorney. If an agent is signing your return for you, a power of attorney (POA) must be filed. Attach the POA to Form 8453 and file it using that form's instructions. See Signatures, later, for more information on POAs.\n",
      "State returns. In most states, you can file an electronic state return simultaneously with your federal return. For more information, check with your local IRS office, state tax agency, tax pro- fessional, or the IRS website at IRS.gov/efile.\n",
      "Refunds. You can have a refund check mailed to you, or you can have your refund deposited directly to your checking or savings account or split among two or three accounts. With e-file,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print chunks per title section\n",
    "for i, chunk in enumerate(chunks[2][:10], start=0):\n",
    "    print(f'Chunk {i}:')\n",
    "    for item in chunk:\n",
    "        print(item)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c665aeba-cb90-40c4-b40b-367cd5874a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's New \n",
      "Table 1-2. 2024 Filing Requirements for Dependents \n",
      "Table 1-3. Other Situations When You Must File a 2024 Return \n",
      "Filing Status \n",
      "Worksheet 2-1. Cost of Keeping Up a Home \n",
      "Dependents \n",
      "Tax Withholding and Estimated Tax \n",
      "Wages, Salaries, and Other Earnings \n",
      "Interest Income \n",
      "Social Security and Equivalent Railroad Retirement Benefits \n",
      "Other Income \n",
      "Use this worksheet to figure your modified AGI for traditional IRA purposes. \n",
      "When Must You Withdraw IRA Assets? (Required Minimum Distributions) \n",
      "2024 Standard Deduction Tables \n",
      "Worksheet 11-1. Figuring Your State and Local Real Estate Tax Deduction - Taxes on New Home \n",
      "Other Itemized Deductions \n",
      "Part Four. \n",
      "Child Tax Credit and Credit for Other Dependents \n",
      "Sample Table \n",
      "2024 Tax Computation Worksheet-Line 16 \n",
      "2024 Tax Rate Schedules \n",
      "Your Rights as a Taxpayer \n",
      "Where To File \n"
     ]
    }
   ],
   "source": [
    "# List of title header sections document was split into\n",
    "for x in chunk_header_mapping:\n",
    "    if chunk_header_mapping[x]:\n",
    "        try:\n",
    "            title_pattern = re.compile(r'<title>(.*?)(?:</title>|$)', re.DOTALL)       \n",
    "            title_match = re.search(title_pattern, chunk_header_mapping[x][0][0])\n",
    "            title_ = title_match.group(1) if title_match else \"\"\n",
    "            print(title_, end='\\n')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123dba4-c57c-4fe2-85f5-b17b4bf0d29b",
   "metadata": {},
   "source": [
    "Upload section contents (title and headers for each chunk) to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445e5827-3821-4c59-9e02-64c06db499e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (f\"{doc_id}.json\", \"w\") as f:\n",
    "    json.dump(chunk_header_mapping,f)\n",
    "s3.upload_file(f\"{doc_id}.json\", BUCKET, f\"{doc_id}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2ac60-137c-4679-a0de-446e9f531008",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f93e9-4813-4b4e-a040-87e2370ec64b",
   "metadata": {},
   "source": [
    "\n",
    "Here's a sample script for indexing document chunks into an Amazon OpenSearch Service Domain, alternatively [Amazon OpenSearch Serverless](https://aws.amazon.com/blogs/big-data/introducing-the-vector-engine-for-amazon-opensearch-serverless-now-in-preview/) can be used.\n",
    "\n",
    "This code block establishes an index within an Amazon OpenSearch Service (Provisioned Capacity) and proceeds to index the document chunks. The index mapping incorporates metadata fields such as document name, complete chunk tables, header section IDs, and title section IDs. These section IDs can be utilized to retrieve section content stored elsewhere (e.g., S3 or a different OpenSearch Service Domain Index) for matching passages to retrieve additional information to be passed to the LLM.\n",
    "\n",
    "We utilize an embedding model to generate embeddings and subsequently index them. The provided options in this implementation uses `Amazon titan Embedding`. You can switch the embedding model to use any other implemented in the `_get_emb_` function above. I recommend using an embedding model that has a large sequence length (greater than 512 tokens) due to the semantic nature of the chunking process.\n",
    "\n",
    "**Note:** Certain chunks may exceed the threshold set for chunking in the previous cells due to the way tables are chunked by row and section paragraph sizes. This might result in a token limit exceed error for certain embedding models.\n",
    "\n",
    "Ensure to replace the **domain_endpoint** variable with the Amazon OpenSearch Service domain (2.11 and higher) or Serverless collection you created in your account.\n",
    "\n",
    "If using Amazon Opensearch Serverless, change the `openserach_serverless` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b73d1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AIDAU5LH52IQ4GCGXLP33', 'Account': '337909764641', 'Arn': 'arn:aws:iam::337909764641:user/Giap', 'ResponseMetadata': {'RequestId': 'f4f66550-5941-4211-b395-1ed97b37322e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f4f66550-5941-4211-b395-1ed97b37322e', 'content-type': 'text/xml', 'content-length': '401', 'date': 'Fri, 27 Jun 2025 03:48:13 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(boto3.client('sts').get_caller_identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f459d61-102c-44c4-8b65-245b74c5d883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2-titanv2-new Index already exists!\n",
      "Document indexed successfully with ID: 2XCHr5cBgduF7OtYGKsD\n",
      "Document indexed successfully with ID: sEOHr5cBPiy9PEnmGNCx\n",
      "Document indexed successfully with ID: 2nCHr5cBgduF7OtYGatX\n",
      "Document indexed successfully with ID: sUOHr5cBPiy9PEnmGtBL\n",
      "Document indexed successfully with ID: 23CHr5cBgduF7OtYG6sZ\n",
      "Document indexed successfully with ID: skOHr5cBPiy9PEnmG9Dh\n",
      "Document indexed successfully with ID: 3HCHr5cBgduF7OtYHKvB\n",
      "Document indexed successfully with ID: s0OHr5cBPiy9PEnmHdB-\n",
      "Document indexed successfully with ID: 3XCHr5cBgduF7OtYHqsj\n",
      "Document indexed successfully with ID: tEOHr5cBPiy9PEnmHtC4\n",
      "Document indexed successfully with ID: 3nCHr5cBgduF7OtYH6tG\n",
      "Document indexed successfully with ID: tUOHr5cBPiy9PEnmH9Dm\n",
      "Document indexed successfully with ID: 33CHr5cBgduF7OtYIKtu\n",
      "Document indexed successfully with ID: tkOHr5cBPiy9PEnmIND_\n",
      "Document indexed successfully with ID: 4HCHr5cBgduF7OtYIauZ\n",
      "Document indexed successfully with ID: t0OHr5cBPiy9PEnmItAc\n",
      "Document indexed successfully with ID: 4XCHr5cBgduF7OtYIqup\n",
      "Document indexed successfully with ID: uEOHr5cBPiy9PEnmI9A5\n",
      "Document indexed successfully with ID: 4nCHr5cBgduF7OtYI6vO\n",
      "Document indexed successfully with ID: uUOHr5cBPiy9PEnmJNCV\n",
      "Document indexed successfully with ID: 43CHr5cBgduF7OtYJasw\n",
      "Document indexed successfully with ID: ukOHr5cBPiy9PEnmJdDU\n",
      "Document indexed successfully with ID: 5HCHr5cBgduF7OtYJqt3\n",
      "Document indexed successfully with ID: u0OHr5cBPiy9PEnmJ9Aa\n",
      "Document indexed successfully with ID: 5XCHr5cBgduF7OtYJ6u6\n",
      "Document indexed successfully with ID: vEOHr5cBPiy9PEnmKNBN\n",
      "Document indexed successfully with ID: 5nCHr5cBgduF7OtYKKvk\n",
      "Document indexed successfully with ID: vUOHr5cBPiy9PEnmKdCV\n",
      "Document indexed successfully with ID: 53CHr5cBgduF7OtYKqsd\n",
      "Document indexed successfully with ID: vkOHr5cBPiy9PEnmKtC7\n",
      "Document indexed successfully with ID: 6HCHr5cBgduF7OtYK6tI\n",
      "Document indexed successfully with ID: v0OHr5cBPiy9PEnmK9Dh\n",
      "Document indexed successfully with ID: 6XCHr5cBgduF7OtYLKtx\n",
      "Document indexed successfully with ID: wEOHr5cBPiy9PEnmLdAP\n",
      "Document indexed successfully with ID: 6nCHr5cBgduF7OtYLaub\n",
      "Document indexed successfully with ID: wUOHr5cBPiy9PEnmLtBF\n",
      "Document indexed successfully with ID: 63CHr5cBgduF7OtYLqvv\n",
      "Document indexed successfully with ID: wkOHr5cBPiy9PEnmL9Cm\n",
      "Document indexed successfully with ID: 7HCHr5cBgduF7OtYMKtM\n",
      "Document indexed successfully with ID: w0OHr5cBPiy9PEnmMND2\n",
      "Document indexed successfully with ID: 7XCHr5cBgduF7OtYMaua\n",
      "Document indexed successfully with ID: xEOHr5cBPiy9PEnmMtBH\n",
      "Document indexed successfully with ID: 7nCHr5cBgduF7OtYMqvZ\n",
      "Document indexed successfully with ID: xUOHr5cBPiy9PEnmM9B2\n",
      "Document indexed successfully with ID: 73CHr5cBgduF7OtYNKsN\n",
      "Document indexed successfully with ID: xkOHr5cBPiy9PEnmNNDG\n",
      "Document indexed successfully with ID: 8HCHr5cBgduF7OtYNatp\n",
      "Document indexed successfully with ID: x0OHr5cBPiy9PEnmNdD5\n",
      "Document indexed successfully with ID: 8XCHr5cBgduF7OtYNquK\n",
      "Document indexed successfully with ID: yEOHr5cBPiy9PEnmN9A_\n",
      "Document indexed successfully with ID: 8nCHr5cBgduF7OtYN6vs\n",
      "Document indexed successfully with ID: yUOHr5cBPiy9PEnmONCM\n",
      "Document indexed successfully with ID: 83CHr5cBgduF7OtYOask\n",
      "Document indexed successfully with ID: ykOHr5cBPiy9PEnmOdC6\n",
      "Document indexed successfully with ID: 9HCHr5cBgduF7OtYOqtZ\n",
      "Document indexed successfully with ID: y0OHr5cBPiy9PEnmOtDx\n",
      "Document indexed successfully with ID: 9XCHr5cBgduF7OtYO6us\n",
      "Document indexed successfully with ID: zEOHr5cBPiy9PEnmPNA-\n",
      "Document indexed successfully with ID: 9nCHr5cBgduF7OtYPKvM\n",
      "Document indexed successfully with ID: zUOHr5cBPiy9PEnmPdBX\n",
      "Document indexed successfully with ID: 93CHr5cBgduF7OtYPqsE\n",
      "Document indexed successfully with ID: zkOHr5cBPiy9PEnmPtCO\n",
      "Document indexed successfully with ID: -HCHr5cBgduF7OtYP6ss\n",
      "Document indexed successfully with ID: z0OHr5cBPiy9PEnmP9DS\n",
      "Document indexed successfully with ID: -XCHr5cBgduF7OtYQKtt\n",
      "Document indexed successfully with ID: 0EOHr5cBPiy9PEnmQdAW\n",
      "Document indexed successfully with ID: -nCHr5cBgduF7OtYQau0\n",
      "Document indexed successfully with ID: 0UOHr5cBPiy9PEnmQtBI\n",
      "Document indexed successfully with ID: -3CHr5cBgduF7OtYQqvM\n",
      "Document indexed successfully with ID: 0kOHr5cBPiy9PEnmQ9Bc\n",
      "Document indexed successfully with ID: _HCHr5cBgduF7OtYQ6vp\n",
      "Document indexed successfully with ID: 00OHr5cBPiy9PEnmRNCF\n",
      "Document indexed successfully with ID: _XCHr5cBgduF7OtYRasT\n",
      "Document indexed successfully with ID: 1EOHr5cBPiy9PEnmRdCl\n",
      "Document indexed successfully with ID: _nCHr5cBgduF7OtYRqtO\n",
      "Document indexed successfully with ID: 1UOHr5cBPiy9PEnmRtDW\n",
      "Document indexed successfully with ID: _3CHr5cBgduF7OtYR6tr\n",
      "Document indexed successfully with ID: 1kOHr5cBPiy9PEnmR9D5\n",
      "Document indexed successfully with ID: AHCHr5cBgduF7OtYSKx2\n",
      "Document indexed successfully with ID: 10OHr5cBPiy9PEnmSND3\n",
      "Document indexed successfully with ID: AXCHr5cBgduF7OtYSayS\n",
      "Document indexed successfully with ID: 2EOHr5cBPiy9PEnmStAI\n",
      "Document indexed successfully with ID: AnCHr5cBgduF7OtYSqyU\n",
      "Document indexed successfully with ID: 2UOHr5cBPiy9PEnmS9Ay\n",
      "Document indexed successfully with ID: A3CHr5cBgduF7OtYS6zV\n",
      "Document indexed successfully with ID: 2kOHr5cBPiy9PEnmTNBr\n",
      "Document indexed successfully with ID: BHCHr5cBgduF7OtYTKz5\n",
      "Document indexed successfully with ID: 20OHr5cBPiy9PEnmTdCK\n",
      "Document indexed successfully with ID: BXCHr5cBgduF7OtYTqwd\n",
      "Document indexed successfully with ID: 3EOHr5cBPiy9PEnmTtCR\n",
      "Document indexed successfully with ID: BnCHr5cBgduF7OtYT6ws\n",
      "Document indexed successfully with ID: 3UOHr5cBPiy9PEnmT9C2\n",
      "Document indexed successfully with ID: B3CHr5cBgduF7OtYUKxD\n",
      "Document indexed successfully with ID: 3kOHr5cBPiy9PEnmUNDY\n",
      "Document indexed successfully with ID: CHCHr5cBgduF7OtYUaxz\n",
      "Document indexed successfully with ID: 30OHr5cBPiy9PEnmUtAK\n",
      "Document indexed successfully with ID: CXCHr5cBgduF7OtYUqyU\n",
      "Document indexed successfully with ID: 4EOHr5cBPiy9PEnmU9AO\n",
      "Document indexed successfully with ID: CnCHr5cBgduF7OtYU6yo\n",
      "Document indexed successfully with ID: 4UOHr5cBPiy9PEnmVNA9\n",
      "Document indexed successfully with ID: C3CHr5cBgduF7OtYVKzB\n",
      "Document indexed successfully with ID: 4kOHr5cBPiy9PEnmVdBR\n",
      "Document indexed successfully with ID: DHCHr5cBgduF7OtYVazn\n",
      "Document indexed successfully with ID: 40OHr5cBPiy9PEnmVtB7\n",
      "Document indexed successfully with ID: DXCHr5cBgduF7OtYV6xL\n",
      "Document indexed successfully with ID: 5EOHr5cBPiy9PEnmV9Dm\n",
      "Document indexed successfully with ID: DnCHr5cBgduF7OtYWKx4\n",
      "Document indexed successfully with ID: 5UOHr5cBPiy9PEnmWND-\n",
      "Document indexed successfully with ID: D3CHr5cBgduF7OtYWaxz\n",
      "Document indexed successfully with ID: 5kOHr5cBPiy9PEnmWtAY\n",
      "Document indexed successfully with ID: EHCHr5cBgduF7OtYWqyb\n",
      "Document indexed successfully with ID: 50OHr5cBPiy9PEnmW9AZ\n",
      "Document indexed successfully with ID: EXCHr5cBgduF7OtYW6ys\n",
      "Document indexed successfully with ID: 6EOHr5cBPiy9PEnmXNBF\n",
      "Document indexed successfully with ID: EnCHr5cBgduF7OtYXKzR\n",
      "Document indexed successfully with ID: 6UOHr5cBPiy9PEnmXdBX\n",
      "Document indexed successfully with ID: E3CHr5cBgduF7OtYXazO\n",
      "Document indexed successfully with ID: 6kOHr5cBPiy9PEnmXtBf\n",
      "Document indexed successfully with ID: FHCHr5cBgduF7OtYX6wd\n",
      "Document indexed successfully with ID: 60OHr5cBPiy9PEnmX9Cs\n",
      "Document indexed successfully with ID: FXCHr5cBgduF7OtYYKwl\n",
      "Document indexed successfully with ID: 7EOHr5cBPiy9PEnmYNCy\n",
      "Document indexed successfully with ID: FnCHr5cBgduF7OtYYaxh\n",
      "Document indexed successfully with ID: 7UOHr5cBPiy9PEnmYdD6\n",
      "Document indexed successfully with ID: F3CHr5cBgduF7OtYYqxu\n",
      "Document indexed successfully with ID: 7kOHr5cBPiy9PEnmYtD-\n",
      "Document indexed successfully with ID: GHCHr5cBgduF7OtYY6yV\n",
      "Document indexed successfully with ID: 70OHr5cBPiy9PEnmZNAZ\n",
      "Document indexed successfully with ID: GXCHr5cBgduF7OtYZKyr\n",
      "Document indexed successfully with ID: 8EOHr5cBPiy9PEnmZdA7\n",
      "Document indexed successfully with ID: GnCHr5cBgduF7OtYZaza\n",
      "Document indexed successfully with ID: 8UOHr5cBPiy9PEnmZtB6\n",
      "Document indexed successfully with ID: G3CHr5cBgduF7OtYZqz1\n",
      "Document indexed successfully with ID: 8kOHr5cBPiy9PEnmZ9Bl\n",
      "Document indexed successfully with ID: HHCHr5cBgduF7OtYaKwI\n",
      "Document indexed successfully with ID: 80OHr5cBPiy9PEnmaNCa\n",
      "Document indexed successfully with ID: HXCHr5cBgduF7OtYaawe\n",
      "Document indexed successfully with ID: 9EOHr5cBPiy9PEnmadCl\n",
      "Document indexed successfully with ID: HnCHr5cBgduF7OtYaqwt\n",
      "Document indexed successfully with ID: 9UOHr5cBPiy9PEnmatDD\n",
      "Document indexed successfully with ID: H3CHr5cBgduF7OtYa6xb\n",
      "Document indexed successfully with ID: 9kOHr5cBPiy9PEnma9Dm\n",
      "Document indexed successfully with ID: IHCHr5cBgduF7OtYbKxy\n",
      "Document indexed successfully with ID: 90OHr5cBPiy9PEnmbNDs\n",
      "Document indexed successfully with ID: IXCHr5cBgduF7OtYbayR\n",
      "Document indexed successfully with ID: -EOHr5cBPiy9PEnmbtAG\n",
      "Document indexed successfully with ID: InCHr5cBgduF7OtYbqyV\n",
      "Document indexed successfully with ID: -UOHr5cBPiy9PEnmb9AQ\n",
      "Document indexed successfully with ID: I3CHr5cBgduF7OtYb6ye\n",
      "Document indexed successfully with ID: -kOHr5cBPiy9PEnmcNAu\n",
      "Document indexed successfully with ID: JHCHr5cBgduF7OtYcKy6\n",
      "Document indexed successfully with ID: -0OHr5cBPiy9PEnmcdA1\n",
      "Document indexed successfully with ID: JXCHr5cBgduF7OtYcazJ\n",
      "Document indexed successfully with ID: _EOHr5cBPiy9PEnmctA_\n",
      "Document indexed successfully with ID: JnCHr5cBgduF7OtYcqza\n",
      "Document indexed successfully with ID: _UOHr5cBPiy9PEnmc9BP\n",
      "Document indexed successfully with ID: J3CHr5cBgduF7OtYc6ze\n",
      "Document indexed successfully with ID: _kOHr5cBPiy9PEnmdNCB\n",
      "Document indexed successfully with ID: KHCHr5cBgduF7OtYdawU\n",
      "Document indexed successfully with ID: _0OHr5cBPiy9PEnmddCn\n",
      "Document indexed successfully with ID: KXCHr5cBgduF7OtYdqwi\n",
      "Document indexed successfully with ID: AEOHr5cBPiy9PEnmdtGa\n",
      "Document indexed successfully with ID: KnCHr5cBgduF7OtYd6ww\n",
      "Document indexed successfully with ID: AUOHr5cBPiy9PEnmd9G4\n",
      "Document indexed successfully with ID: K3CHr5cBgduF7OtYeKwy\n",
      "Document indexed successfully with ID: AkOHr5cBPiy9PEnmeNGp\n",
      "Document indexed successfully with ID: LHCHr5cBgduF7OtYeaw2\n",
      "Document indexed successfully with ID: A0OHr5cBPiy9PEnmedHN\n",
      "Document indexed successfully with ID: LXCHr5cBgduF7OtYeqxa\n",
      "Document indexed successfully with ID: BEOHr5cBPiy9PEnmetHk\n",
      "Document indexed successfully with ID: LnCHr5cBgduF7OtYe6yB\n",
      "Document indexed successfully with ID: BUOHr5cBPiy9PEnme9H9\n",
      "Document indexed successfully with ID: L3CHr5cBgduF7OtYfKyN\n",
      "Document indexed successfully with ID: BkOHr5cBPiy9PEnmfdEE\n",
      "Document indexed successfully with ID: MHCHr5cBgduF7OtYfayA\n",
      "Document indexed successfully with ID: B0OHr5cBPiy9PEnmftEZ\n",
      "Document indexed successfully with ID: MXCHr5cBgduF7OtYfqyQ\n",
      "Document indexed successfully with ID: CEOHr5cBPiy9PEnmf9EE\n",
      "Document indexed successfully with ID: MnCHr5cBgduF7OtYf6x-\n",
      "Document indexed successfully with ID: CUOHr5cBPiy9PEnmf9H6\n",
      "Document indexed successfully with ID: M3CHr5cBgduF7OtYgKxx\n",
      "Document indexed successfully with ID: CkOHr5cBPiy9PEnmgdER\n",
      "Document indexed successfully with ID: NHCHr5cBgduF7OtYgayK\n",
      "Document indexed successfully with ID: C0OHr5cBPiy9PEnmgtE5\n",
      "Document indexed successfully with ID: NXCHr5cBgduF7OtYgqze\n",
      "Document indexed successfully with ID: DEOHr5cBPiy9PEnmg9Fn\n",
      "Document indexed successfully with ID: NnCHr5cBgduF7OtYg6z7\n",
      "Document indexed successfully with ID: DUOHr5cBPiy9PEnmhNF4\n",
      "Document indexed successfully with ID: N3CHr5cBgduF7OtYhawj\n",
      "Document indexed successfully with ID: DkOHr5cBPiy9PEnmhdG-\n",
      "Document indexed successfully with ID: OHCHr5cBgduF7OtYhqxM\n",
      "Document indexed successfully with ID: D0OHr5cBPiy9PEnmhtHZ\n",
      "Document indexed successfully with ID: OXCHr5cBgduF7OtYh6xb\n",
      "Document indexed successfully with ID: EEOHr5cBPiy9PEnmh9Hv\n",
      "Document indexed successfully with ID: OnCHr5cBgduF7OtYiKyC\n",
      "Document indexed successfully with ID: EUOHr5cBPiy9PEnmidEf\n",
      "Document indexed successfully with ID: O3CHr5cBgduF7OtYiay3\n",
      "Document indexed successfully with ID: EkOHr5cBPiy9PEnmitEx\n",
      "Document indexed successfully with ID: PHCHr5cBgduF7OtYiqzH\n",
      "Document indexed successfully with ID: E0OHr5cBPiy9PEnmi9Fg\n",
      "Document indexed successfully with ID: PXCHr5cBgduF7OtYi6ze\n",
      "Document indexed successfully with ID: FEOHr5cBPiy9PEnmjNF2\n",
      "Document indexed successfully with ID: PnCHr5cBgduF7OtYjKzy\n",
      "Document indexed successfully with ID: FUOHr5cBPiy9PEnmjdGM\n",
      "Document indexed successfully with ID: P3CHr5cBgduF7OtYjqwP\n",
      "Document indexed successfully with ID: FkOHr5cBPiy9PEnmjtGr\n",
      "Document indexed successfully with ID: QHCHr5cBgduF7OtYj6w0\n",
      "Document indexed successfully with ID: F0OHr5cBPiy9PEnmj9HC\n",
      "Document indexed successfully with ID: QXCHr5cBgduF7OtYkKw_\n",
      "Document indexed successfully with ID: GEOHr5cBPiy9PEnmkNG0\n",
      "Document indexed successfully with ID: QnCHr5cBgduF7OtYkawk\n",
      "Document indexed successfully with ID: GUOHr5cBPiy9PEnmkdGj\n",
      "Document indexed successfully with ID: Q3CHr5cBgduF7OtYkqw4\n",
      "Document indexed successfully with ID: GkOHr5cBPiy9PEnmktHK\n",
      "Document indexed successfully with ID: RHCHr5cBgduF7OtYk6xM\n",
      "Document indexed successfully with ID: G0OHr5cBPiy9PEnmk9HJ\n",
      "Document indexed successfully with ID: RXCHr5cBgduF7OtYlKxf\n",
      "Document indexed successfully with ID: HEOHr5cBPiy9PEnmlNHe\n",
      "Document indexed successfully with ID: RnCHr5cBgduF7OtYlax0\n",
      "Document indexed successfully with ID: HUOHr5cBPiy9PEnmldH4\n",
      "Document indexed successfully with ID: R3CHr5cBgduF7OtYlqxx\n",
      "Document indexed successfully with ID: HkOHr5cBPiy9PEnmltHu\n",
      "Document indexed successfully with ID: SHCHr5cBgduF7OtYl6yE\n",
      "Document indexed successfully with ID: H0OHr5cBPiy9PEnml9H5\n",
      "Document indexed successfully with ID: SXCHr5cBgduF7OtYmKxt\n",
      "Document indexed successfully with ID: IEOHr5cBPiy9PEnmmNHi\n",
      "Document indexed successfully with ID: SnCHr5cBgduF7OtYmaxQ\n",
      "Document indexed successfully with ID: IUOHr5cBPiy9PEnmmdHc\n",
      "Document indexed successfully with ID: S3CHr5cBgduF7OtYmqxU\n",
      "Document indexed successfully with ID: IkOHr5cBPiy9PEnmmtHt\n",
      "Document indexed successfully with ID: THCHr5cBgduF7OtYm6xk\n",
      "Document indexed successfully with ID: I0OHr5cBPiy9PEnmnNEC\n",
      "Document indexed successfully with ID: TXCHr5cBgduF7OtYnKyH\n",
      "Document indexed successfully with ID: JEOHr5cBPiy9PEnmnNH_\n",
      "Document indexed successfully with ID: TnCHr5cBgduF7OtYnaxz\n",
      "Document indexed successfully with ID: JUOHr5cBPiy9PEnmndH3\n",
      "Document indexed successfully with ID: T3CHr5cBgduF7OtYnqx7\n",
      "Document indexed successfully with ID: JkOHr5cBPiy9PEnmn9Eg\n",
      "Document indexed successfully with ID: UHCHr5cBgduF7OtYn6zL\n",
      "Document indexed successfully with ID: J0OHr5cBPiy9PEnmoNFS\n",
      "Document indexed successfully with ID: UXCHr5cBgduF7OtYoKzj\n",
      "Document indexed successfully with ID: KEOHr5cBPiy9PEnmodFo\n",
      "Document indexed successfully with ID: UnCHr5cBgduF7OtYoazz\n",
      "Document indexed successfully with ID: KUOHr5cBPiy9PEnmotF2\n",
      "Document indexed successfully with ID: U3CHr5cBgduF7OtYoqzp\n",
      "Document indexed successfully with ID: KkOHr5cBPiy9PEnmo9Ff\n",
      "Document indexed successfully with ID: VHCHr5cBgduF7OtYpKwN\n",
      "Document indexed successfully with ID: K0OHr5cBPiy9PEnmpNGO\n",
      "Document indexed successfully with ID: VXCHr5cBgduF7OtYpawS\n",
      "Document indexed successfully with ID: LEOHr5cBPiy9PEnmpdGV\n",
      "Document indexed successfully with ID: VnCHr5cBgduF7OtYpqwx\n",
      "Document indexed successfully with ID: LUOHr5cBPiy9PEnmptGz\n",
      "Document indexed successfully with ID: V3CHr5cBgduF7OtYp6wq\n",
      "Document indexed successfully with ID: LkOHr5cBPiy9PEnmp9G5\n",
      "Document indexed successfully with ID: WHCHr5cBgduF7OtYqKxZ\n",
      "Document indexed successfully with ID: L0OHr5cBPiy9PEnmqNH0\n",
      "Document indexed successfully with ID: WXCHr5cBgduF7OtYqax2\n",
      "Document indexed successfully with ID: MEOHr5cBPiy9PEnmqdH_\n",
      "Document indexed successfully with ID: WnCHr5cBgduF7OtYqqyL\n",
      "Document indexed successfully with ID: MUOHr5cBPiy9PEnmq9Eg\n",
      "Document indexed successfully with ID: W3CHr5cBgduF7OtYq6y4\n",
      "Document indexed successfully with ID: MkOHr5cBPiy9PEnmrNFN\n",
      "Document indexed successfully with ID: XHCHr5cBgduF7OtYrKzh\n",
      "Document indexed successfully with ID: M0OHr5cBPiy9PEnmrdFX\n",
      "Document indexed successfully with ID: XXCHr5cBgduF7OtYrazP\n",
      "Document indexed successfully with ID: NEOHr5cBPiy9PEnmrtF0\n",
      "Document indexed successfully with ID: XnCHr5cBgduF7OtYrqz5\n",
      "Document indexed successfully with ID: NUOHr5cBPiy9PEnmr9GJ\n",
      "Document indexed successfully with ID: X3CHr5cBgduF7OtYsKwS\n",
      "Document indexed successfully with ID: NkOHr5cBPiy9PEnmsNGi\n",
      "Document indexed successfully with ID: YHCHr5cBgduF7OtYsawg\n",
      "Document indexed successfully with ID: N0OHr5cBPiy9PEnmsdGY\n",
      "Document indexed successfully with ID: YXCHr5cBgduF7OtYsqw4\n",
      "Document indexed successfully with ID: OEOHr5cBPiy9PEnmstGz\n",
      "Document indexed successfully with ID: YnCHr5cBgduF7OtYs6wn\n",
      "Document indexed successfully with ID: OUOHr5cBPiy9PEnms9G9\n",
      "Document indexed successfully with ID: Y3CHr5cBgduF7OtYtKxX\n",
      "Document indexed successfully with ID: OkOHr5cBPiy9PEnmtNH7\n",
      "Document indexed successfully with ID: ZHCHr5cBgduF7OtYtayI\n",
      "Document indexed successfully with ID: O0OHr5cBPiy9PEnmttED\n",
      "Document indexed successfully with ID: ZXCHr5cBgduF7OtYtqyE\n",
      "Document indexed successfully with ID: PEOHr5cBPiy9PEnmt9EQ\n",
      "Document indexed successfully with ID: ZnCHr5cBgduF7OtYt6yG\n",
      "Document indexed successfully with ID: PUOHr5cBPiy9PEnmt9H7\n",
      "Document indexed successfully with ID: Z3CHr5cBgduF7OtYuKyF\n",
      "Document indexed successfully with ID: PkOHr5cBPiy9PEnmudEW\n",
      "Document indexed successfully with ID: aHCHr5cBgduF7OtYuayf\n",
      "Document indexed successfully with ID: P0OHr5cBPiy9PEnmutEj\n",
      "Document indexed successfully with ID: aXCHr5cBgduF7OtYuqyr\n",
      "Document indexed successfully with ID: QEOHr5cBPiy9PEnmu9Ec\n",
      "Document indexed successfully with ID: anCHr5cBgduF7OtYu6ya\n",
      "Document indexed successfully with ID: QUOHr5cBPiy9PEnmvNEx\n",
      "Document indexed successfully with ID: a3CHr5cBgduF7OtYvKyw\n",
      "Document indexed successfully with ID: QkOHr5cBPiy9PEnmvdFG\n",
      "Document indexed successfully with ID: bHCHr5cBgduF7OtYvazO\n",
      "Document indexed successfully with ID: Q0OHr5cBPiy9PEnmvtFW\n",
      "Document indexed successfully with ID: bXCHr5cBgduF7OtYvqzh\n",
      "Document indexed successfully with ID: REOHr5cBPiy9PEnmv9Fn\n",
      "Document indexed successfully with ID: bnCHr5cBgduF7OtYwKwB\n",
      "Document indexed successfully with ID: RUOHr5cBPiy9PEnmwNFx\n",
      "Document indexed successfully with ID: b3CHr5cBgduF7OtYwKzv\n",
      "Document indexed successfully with ID: RkOHr5cBPiy9PEnmwdFs\n",
      "Document indexed successfully with ID: cHCHr5cBgduF7OtYwazu\n",
      "Document indexed successfully with ID: R0OHr5cBPiy9PEnmwtFu\n",
      "Document indexed successfully with ID: cXCHr5cBgduF7OtYwqzg\n",
      "Document indexed successfully with ID: SEOHr5cBPiy9PEnmw9Fd\n",
      "Document indexed successfully with ID: cnCHr5cBgduF7OtYw6zo\n",
      "Document indexed successfully with ID: SUOHr5cBPiy9PEnmxNFZ\n",
      "Document indexed successfully with ID: c3CHr5cBgduF7OtYxKzZ\n",
      "Document indexed successfully with ID: SkOHr5cBPiy9PEnmxdFv\n",
      "Document indexed successfully with ID: dHCHr5cBgduF7OtYxqwL\n",
      "Document indexed successfully with ID: S0OHr5cBPiy9PEnmxtF9\n",
      "Document indexed successfully with ID: dXCHr5cBgduF7OtYx6wL\n",
      "Document indexed successfully with ID: TEOHr5cBPiy9PEnmx9GT\n",
      "Document indexed successfully with ID: dnCHr5cBgduF7OtYx6z9\n",
      "Document indexed successfully with ID: TUOHr5cBPiy9PEnmyNF-\n",
      "Document indexed successfully with ID: d3CHr5cBgduF7OtYyawJ\n",
      "Document indexed successfully with ID: TkOHr5cBPiy9PEnmydF_\n",
      "Document indexed successfully with ID: eHCHr5cBgduF7OtYyqwI\n",
      "Document indexed successfully with ID: T0OHr5cBPiy9PEnmytGq\n",
      "Document indexed successfully with ID: eXCHr5cBgduF7OtYy6wy\n",
      "Document indexed successfully with ID: UEOHr5cBPiy9PEnmy9HI\n",
      "Document indexed successfully with ID: enCHr5cBgduF7OtYzKxE\n",
      "Document indexed successfully with ID: UUOHr5cBPiy9PEnmzNG2\n",
      "Document indexed successfully with ID: e3CHr5cBgduF7OtYzawv\n",
      "Document indexed successfully with ID: UkOHr5cBPiy9PEnmzdHC\n",
      "Document indexed successfully with ID: fHCHr5cBgduF7OtYzqw8\n",
      "Document indexed successfully with ID: U0OHr5cBPiy9PEnmztG1\n",
      "Document indexed successfully with ID: fXCHr5cBgduF7OtYz6w3\n",
      "Document indexed successfully with ID: VEOHr5cBPiy9PEnmz9Gz\n",
      "Document indexed successfully with ID: fnCHr5cBgduF7OtY0KxF\n",
      "Document indexed successfully with ID: VUOHr5cBPiy9PEnm0NHX\n",
      "Document indexed successfully with ID: f3CHr5cBgduF7OtY0axj\n",
      "Document indexed successfully with ID: VkOHr5cBPiy9PEnm0dH9\n",
      "Document indexed successfully with ID: gHCHr5cBgduF7OtY0qxw\n",
      "Document indexed successfully with ID: V0OHr5cBPiy9PEnm0tHh\n",
      "Document indexed successfully with ID: gXCHr5cBgduF7OtY06xe\n",
      "Document indexed successfully with ID: WEOHr5cBPiy9PEnm09Hq\n",
      "Document indexed successfully with ID: gnCHr5cBgduF7OtY1Kxq\n",
      "Document indexed successfully with ID: WUOHr5cBPiy9PEnm1NHg\n",
      "Document indexed successfully with ID: g3CHr5cBgduF7OtY1axu\n",
      "Document indexed successfully with ID: WkOHr5cBPiy9PEnm1dHm\n",
      "Document indexed successfully with ID: hHCHr5cBgduF7OtY1qxm\n",
      "Document indexed successfully with ID: W0OHr5cBPiy9PEnm1tHt\n",
      "Document indexed successfully with ID: hXCHr5cBgduF7OtY16yB\n",
      "Document indexed successfully with ID: XEOHr5cBPiy9PEnm2NEi\n",
      "Document indexed successfully with ID: hnCHr5cBgduF7OtY2Kyk\n",
      "Document indexed successfully with ID: XUOHr5cBPiy9PEnm2dEc\n",
      "Document indexed successfully with ID: h3CHr5cBgduF7OtY2ayg\n",
      "Document indexed successfully with ID: XkOHr5cBPiy9PEnm2tEe\n",
      "Document indexed successfully with ID: iHCHr5cBgduF7OtY2qyX\n",
      "Document indexed successfully with ID: X0OHr5cBPiy9PEnm29Ea\n",
      "Document indexed successfully with ID: iXCHr5cBgduF7OtY26zT\n",
      "Document indexed successfully with ID: YEOHr5cBPiy9PEnm3NFi\n",
      "Document indexed successfully with ID: inCHr5cBgduF7OtY3Kzy\n",
      "Document indexed successfully with ID: YUOHr5cBPiy9PEnm3dFp\n",
      "Document indexed successfully with ID: i3CHr5cBgduF7OtY3azi\n",
      "Document indexed successfully with ID: YkOHr5cBPiy9PEnm3tFq\n",
      "Document indexed successfully with ID: jHCHr5cBgduF7OtY3qz0\n",
      "Document indexed successfully with ID: Y0OHr5cBPiy9PEnm39Fp\n",
      "Document indexed successfully with ID: jXCHr5cBgduF7OtY36zf\n",
      "Document indexed successfully with ID: ZEOHr5cBPiy9PEnm4NFR\n",
      "Document indexed successfully with ID: jnCHr5cBgduF7OtY4Kzn\n",
      "Document indexed successfully with ID: ZUOHr5cBPiy9PEnm4dFx\n",
      "Document indexed successfully with ID: j3CHr5cBgduF7OtY4az5\n",
      "Document indexed successfully with ID: ZkOHr5cBPiy9PEnm4tFw\n",
      "Document indexed successfully with ID: kHCHr5cBgduF7OtY4qzn\n",
      "Document indexed successfully with ID: Z0OHr5cBPiy9PEnm49F-\n",
      "Document indexed successfully with ID: kXCHr5cBgduF7OtY46z5\n",
      "Document indexed successfully with ID: aEOHr5cBPiy9PEnm5NFy\n",
      "Document indexed successfully with ID: knCHr5cBgduF7OtY5awQ\n",
      "Document indexed successfully with ID: aUOHr5cBPiy9PEnm5dGM\n",
      "Document indexed successfully with ID: k3CHr5cBgduF7OtY5qwC\n",
      "Document indexed successfully with ID: akOHr5cBPiy9PEnm5tF2\n",
      "Document indexed successfully with ID: lHCHr5cBgduF7OtY5qzs\n",
      "Document indexed successfully with ID: a0OHr5cBPiy9PEnm59Fj\n",
      "Document indexed successfully with ID: lXCHr5cBgduF7OtY56za\n",
      "Document indexed successfully with ID: bEOHr5cBPiy9PEnm6NFq\n",
      "Document indexed successfully with ID: lnCHr5cBgduF7OtY6Kzj\n",
      "Document indexed successfully with ID: bUOHr5cBPiy9PEnm6dFl\n",
      "Document indexed successfully with ID: l3CHr5cBgduF7OtY6azV\n",
      "Document indexed successfully with ID: bkOHr5cBPiy9PEnm6tFQ\n",
      "Document indexed successfully with ID: mHCHr5cBgduF7OtY6qzH\n",
      "Document indexed successfully with ID: b0OHr5cBPiy9PEnm69FA\n",
      "Document indexed successfully with ID: mXCHr5cBgduF7OtY66zG\n",
      "Document indexed successfully with ID: cEOHr5cBPiy9PEnm7NFN\n",
      "Document indexed successfully with ID: mnCHr5cBgduF7OtY7KzK\n",
      "Document indexed successfully with ID: cUOHr5cBPiy9PEnm7dFA\n",
      "Document indexed successfully with ID: m3CHr5cBgduF7OtY7ay1\n",
      "Document indexed successfully with ID: ckOHr5cBPiy9PEnm7tEv\n",
      "Document indexed successfully with ID: nHCHr5cBgduF7OtY7qzG\n",
      "Document indexed successfully with ID: c0OHr5cBPiy9PEnm79Ff\n",
      "Document indexed successfully with ID: nXCHr5cBgduF7OtY76zU\n",
      "Document indexed successfully with ID: dEOHr5cBPiy9PEnm8NFh\n",
      "Document indexed successfully with ID: nnCHr5cBgduF7OtY8Kze\n",
      "Document indexed successfully with ID: dUOHr5cBPiy9PEnm8dFW\n",
      "Document indexed successfully with ID: n3CHr5cBgduF7OtY8azJ\n",
      "Document indexed successfully with ID: dkOHr5cBPiy9PEnm8tFN\n",
      "Document indexed successfully with ID: oHCHr5cBgduF7OtY8qzu\n",
      "Document indexed successfully with ID: d0OHr5cBPiy9PEnm89Fx\n",
      "Document indexed successfully with ID: oXCHr5cBgduF7OtY9KwI\n",
      "Document indexed successfully with ID: eEOHr5cBPiy9PEnm9NGL\n",
      "Document indexed successfully with ID: onCHr5cBgduF7OtY9ayC\n",
      "Document indexed successfully with ID: eUOHr5cBPiy9PEnm9tEP\n",
      "Document indexed successfully with ID: o3CHr5cBgduF7OtY9qyl\n",
      "Document indexed successfully with ID: ekOHr5cBPiy9PEnm99FA\n",
      "Document indexed successfully with ID: pHCHr5cBgduF7OtY96y-\n",
      "Document indexed successfully with ID: e0OHr5cBPiy9PEnm-NFg\n",
      "Document indexed successfully with ID: pXCHr5cBgduF7OtY-Kze\n",
      "Document indexed successfully with ID: fEOHr5cBPiy9PEnm-dGK\n",
      "Document indexed successfully with ID: pnCHr5cBgduF7OtY-qwX\n",
      "Document indexed successfully with ID: fUOHr5cBPiy9PEnm-tGj\n",
      "Document indexed successfully with ID: p3CHr5cBgduF7OtY-6wc\n",
      "Document indexed successfully with ID: fkOHr5cBPiy9PEnm-9Gs\n",
      "Document indexed successfully with ID: qHCHr5cBgduF7OtY_Kwo\n",
      "Document indexed successfully with ID: f0OHr5cBPiy9PEnm_NGc\n",
      "Document indexed successfully with ID: qXCHr5cBgduF7OtY_awo\n",
      "Document indexed successfully with ID: gEOHr5cBPiy9PEnm_dGe\n",
      "Document indexed successfully with ID: qnCHr5cBgduF7OtY_qw0\n",
      "Document indexed successfully with ID: gUOHr5cBPiy9PEnm_tGv\n",
      "Document indexed successfully with ID: q3CHr5cBgduF7OtY_6xC\n",
      "Document indexed successfully with ID: gkOHr5cBPiy9PEnm_9G6\n",
      "Document indexed successfully with ID: rHCIr5cBgduF7OtYAKw5\n",
      "Document indexed successfully with ID: g0OIr5cBPiy9PEnmANG4\n",
      "Document indexed successfully with ID: rXCIr5cBgduF7OtYAawv\n",
      "Document indexed successfully with ID: hEOIr5cBPiy9PEnmAdHA\n",
      "Document indexed successfully with ID: rnCIr5cBgduF7OtYAqw-\n",
      "Document indexed successfully with ID: hUOIr5cBPiy9PEnmAtGw\n",
      "Document indexed successfully with ID: r3CIr5cBgduF7OtYA6wv\n",
      "Document indexed successfully with ID: hkOIr5cBPiy9PEnmA9Gq\n",
      "Document indexed successfully with ID: sHCIr5cBgduF7OtYBKwx\n",
      "Document indexed successfully with ID: h0OIr5cBPiy9PEnmBNHJ\n",
      "Document indexed successfully with ID: sXCIr5cBgduF7OtYBaxc\n",
      "Document indexed successfully with ID: iEOIr5cBPiy9PEnmBdHV\n",
      "Document indexed successfully with ID: snCIr5cBgduF7OtYBqxS\n",
      "Document indexed successfully with ID: iUOIr5cBPiy9PEnmBtHH\n",
      "Document indexed successfully with ID: s3CIr5cBgduF7OtYB6w-\n",
      "Document indexed successfully with ID: ikOIr5cBPiy9PEnmB9Gy\n",
      "Document indexed successfully with ID: tHCIr5cBgduF7OtYCKwt\n",
      "Document indexed successfully with ID: i0OIr5cBPiy9PEnmCNGe\n",
      "Document indexed successfully with ID: tXCIr5cBgduF7OtYCawW\n",
      "Document indexed successfully with ID: jEOIr5cBPiy9PEnmCdGI\n",
      "Document indexed successfully with ID: tnCIr5cBgduF7OtYCqwD\n",
      "Document indexed successfully with ID: jUOIr5cBPiy9PEnmCtGJ\n",
      "Document indexed successfully with ID: t3CIr5cBgduF7OtYC6wX\n",
      "Document indexed successfully with ID: jkOIr5cBPiy9PEnmC9Gd\n",
      "Document indexed successfully with ID: uHCIr5cBgduF7OtYDKwU\n",
      "Document indexed successfully with ID: j0OIr5cBPiy9PEnmDNGE\n",
      "Document indexed successfully with ID: uXCIr5cBgduF7OtYDKz-\n",
      "Document indexed successfully with ID: kEOIr5cBPiy9PEnmDdGS\n",
      "Document indexed successfully with ID: unCIr5cBgduF7OtYDqwJ\n",
      "Document indexed successfully with ID: kUOIr5cBPiy9PEnmDtGT\n",
      "Document indexed successfully with ID: u3CIr5cBgduF7OtYD6wS\n",
      "Document indexed successfully with ID: kkOIr5cBPiy9PEnmD9GT\n",
      "Document indexed successfully with ID: vHCIr5cBgduF7OtYEKwj\n",
      "Document indexed successfully with ID: k0OIr5cBPiy9PEnmENGV\n",
      "Document indexed successfully with ID: vXCIr5cBgduF7OtYEawK\n",
      "Document indexed successfully with ID: lEOIr5cBPiy9PEnmEdF8\n",
      "Document indexed successfully with ID: vnCIr5cBgduF7OtYEazz\n",
      "Document indexed successfully with ID: lUOIr5cBPiy9PEnmEtFq\n",
      "Document indexed successfully with ID: v3CIr5cBgduF7OtYEqzc\n",
      "Document indexed successfully with ID: lkOIr5cBPiy9PEnmE9Fa\n",
      "Document indexed successfully with ID: wHCIr5cBgduF7OtYE6zM\n",
      "Document indexed successfully with ID: l0OIr5cBPiy9PEnmFNFR\n",
      "Document indexed successfully with ID: wXCIr5cBgduF7OtYFKzh\n",
      "Document indexed successfully with ID: mEOIr5cBPiy9PEnmFdFZ\n",
      "Document indexed successfully with ID: wnCIr5cBgduF7OtYFazr\n",
      "Document indexed successfully with ID: mUOIr5cBPiy9PEnmFtGC\n",
      "Document indexed successfully with ID: w3CIr5cBgduF7OtYF6wY\n",
      "Document indexed successfully with ID: mkOIr5cBPiy9PEnmF9Gu\n",
      "Document indexed successfully with ID: xHCIr5cBgduF7OtYGKww\n",
      "Document indexed successfully with ID: m0OIr5cBPiy9PEnmGNG0\n",
      "Document indexed successfully with ID: xXCIr5cBgduF7OtYGawy\n",
      "Document indexed successfully with ID: nEOIr5cBPiy9PEnmGdGz\n",
      "Document indexed successfully with ID: xnCIr5cBgduF7OtYGqws\n",
      "Document indexed successfully with ID: nUOIr5cBPiy9PEnmGtHC\n",
      "Document indexed successfully with ID: x3CIr5cBgduF7OtYG6xL\n",
      "Document indexed successfully with ID: nkOIr5cBPiy9PEnmG9Hq\n",
      "Document indexed successfully with ID: yHCIr5cBgduF7OtYHKx0\n",
      "Document indexed successfully with ID: n0OIr5cBPiy9PEnmHNH-\n",
      "Document indexed successfully with ID: yXCIr5cBgduF7OtYHayD\n",
      "Document indexed successfully with ID: oEOIr5cBPiy9PEnmHdH6\n",
      "Document indexed successfully with ID: ynCIr5cBgduF7OtYHqx_\n",
      "Document indexed successfully with ID: oUOIr5cBPiy9PEnmHtH0\n",
      "Document indexed successfully with ID: y3CIr5cBgduF7OtYH6xt\n",
      "Document indexed successfully with ID: okOIr5cBPiy9PEnmH9Hq\n",
      "Document indexed successfully with ID: zHCIr5cBgduF7OtYIKxs\n",
      "Document indexed successfully with ID: o0OIr5cBPiy9PEnmINHp\n",
      "Document indexed successfully with ID: zXCIr5cBgduF7OtYIax1\n",
      "Document indexed successfully with ID: pEOIr5cBPiy9PEnmIdHs\n",
      "Document indexed successfully with ID: znCIr5cBgduF7OtYIqxj\n",
      "Document indexed successfully with ID: pUOIr5cBPiy9PEnmItHX\n",
      "Document indexed successfully with ID: z3CIr5cBgduF7OtYI6xt\n",
      "Document indexed successfully with ID: pkOIr5cBPiy9PEnmJNEK\n",
      "Document indexed successfully with ID: 0HCIr5cBgduF7OtYJKye\n",
      "Document indexed successfully with ID: p0OIr5cBPiy9PEnmJdE2\n",
      "Document indexed successfully with ID: 0XCIr5cBgduF7OtYJayw\n",
      "Document indexed successfully with ID: qEOIr5cBPiy9PEnmJtEo\n",
      "Document indexed successfully with ID: 0nCIr5cBgduF7OtYJqyx\n",
      "Document indexed successfully with ID: qUOIr5cBPiy9PEnmJ9E3\n",
      "Document indexed successfully with ID: 03CIr5cBgduF7OtYJ6ys\n",
      "Document indexed successfully with ID: qkOIr5cBPiy9PEnmKNEc\n",
      "Document indexed successfully with ID: 1HCIr5cBgduF7OtYKKyV\n",
      "Document indexed successfully with ID: q0OIr5cBPiy9PEnmKdEO\n",
      "Document indexed successfully with ID: 1XCIr5cBgduF7OtYKayE\n",
      "Document indexed successfully with ID: rEOIr5cBPiy9PEnmKtEP\n",
      "Document indexed successfully with ID: 1nCIr5cBgduF7OtYKqyM\n",
      "Document indexed successfully with ID: rUOIr5cBPiy9PEnmKtH-\n",
      "Document indexed successfully with ID: 13CIr5cBgduF7OtYK6x0\n",
      "Document indexed successfully with ID: rkOIr5cBPiy9PEnmK9Hq\n",
      "Document indexed successfully with ID: 2HCIr5cBgduF7OtYLKxp\n",
      "Document indexed successfully with ID: r0OIr5cBPiy9PEnmLNHZ\n",
      "Document indexed successfully with ID: 2XCIr5cBgduF7OtYLaxq\n",
      "Document indexed successfully with ID: sEOIr5cBPiy9PEnmLdHf\n",
      "Document indexed successfully with ID: 2nCIr5cBgduF7OtYLqxe\n",
      "Document indexed successfully with ID: sUOIr5cBPiy9PEnmLtHV\n",
      "Document indexed successfully with ID: 23CIr5cBgduF7OtYL6xI\n",
      "Document indexed successfully with ID: skOIr5cBPiy9PEnmL9G-\n",
      "Document indexed successfully with ID: 3HCIr5cBgduF7OtYMKw0\n",
      "Document indexed successfully with ID: s0OIr5cBPiy9PEnmMNGz\n",
      "Document indexed successfully with ID: 3XCIr5cBgduF7OtYMaws\n",
      "Document indexed successfully with ID: tEOIr5cBPiy9PEnmMdGx\n",
      "Document indexed successfully with ID: 3nCIr5cBgduF7OtYMqwq\n",
      "Document indexed successfully with ID: tUOIr5cBPiy9PEnmMtGf\n",
      "Document indexed successfully with ID: 33CIr5cBgduF7OtYM6wk\n",
      "Document indexed successfully with ID: tkOIr5cBPiy9PEnmM9Ge\n",
      "Document indexed successfully with ID: 4HCIr5cBgduF7OtYNKwx\n",
      "Document indexed successfully with ID: t0OIr5cBPiy9PEnmNNHS\n",
      "Document indexed successfully with ID: 4XCIr5cBgduF7OtYNaxO\n",
      "Document indexed successfully with ID: uEOIr5cBPiy9PEnmNdHE\n",
      "Document indexed successfully with ID: 4nCIr5cBgduF7OtYNqxA\n",
      "Document indexed successfully with ID: uUOIr5cBPiy9PEnmNtHF\n",
      "Document indexed successfully with ID: 43CIr5cBgduF7OtYN6w7\n",
      "Document indexed successfully with ID: ukOIr5cBPiy9PEnmN9Gx\n",
      "Document indexed successfully with ID: 5HCIr5cBgduF7OtYOKws\n",
      "Document indexed successfully with ID: u0OIr5cBPiy9PEnmONGq\n",
      "Document indexed successfully with ID: 5XCIr5cBgduF7OtYOawd\n",
      "Document indexed successfully with ID: vEOIr5cBPiy9PEnmOdGu\n",
      "Document indexed successfully with ID: 5nCIr5cBgduF7OtYOqxC\n",
      "Document indexed successfully with ID: vUOIr5cBPiy9PEnmOtG_\n",
      "Document indexed successfully with ID: 53CIr5cBgduF7OtYO6xU\n",
      "Document indexed successfully with ID: vkOIr5cBPiy9PEnmO9HM\n",
      "Document indexed successfully with ID: 6HCIr5cBgduF7OtYPKxQ\n",
      "Document indexed successfully with ID: v0OIr5cBPiy9PEnmPNHO\n",
      "Document indexed successfully with ID: 6XCIr5cBgduF7OtYPaxr\n",
      "Document indexed successfully with ID: wEOIr5cBPiy9PEnmPdH6\n",
      "Document indexed successfully with ID: 6nCIr5cBgduF7OtYPqx2\n",
      "Document indexed successfully with ID: wUOIr5cBPiy9PEnmP9EB\n",
      "Document indexed successfully with ID: 63CIr5cBgduF7OtYP6yC\n",
      "Document indexed successfully with ID: wkOIr5cBPiy9PEnmP9H6\n",
      "Document indexed successfully with ID: 7HCIr5cBgduF7OtYQKxs\n",
      "Document indexed successfully with ID: w0OIr5cBPiy9PEnmQNHt\n",
      "Document indexed successfully with ID: 7XCIr5cBgduF7OtYQayK\n",
      "Document indexed successfully with ID: xEOIr5cBPiy9PEnmQtEe\n",
      "Document indexed successfully with ID: 7nCIr5cBgduF7OtYQqya\n",
      "Document indexed successfully with ID: xUOIr5cBPiy9PEnmQ9Eq\n",
      "Document indexed successfully with ID: 73CIr5cBgduF7OtYQ6yu\n",
      "Document indexed successfully with ID: xkOIr5cBPiy9PEnmRNFI\n",
      "Document indexed successfully with ID: 8HCIr5cBgduF7OtYRKzJ\n",
      "Document indexed successfully with ID: x0OIr5cBPiy9PEnmRdFh\n",
      "Document indexed successfully with ID: 8XCIr5cBgduF7OtYRazy\n",
      "Document indexed successfully with ID: yEOIr5cBPiy9PEnmRtFy\n",
      "Document indexed successfully with ID: 8nCIr5cBgduF7OtYRqzq\n",
      "Document indexed successfully with ID: yUOIr5cBPiy9PEnmR9Fm\n",
      "Document indexed successfully with ID: 83CIr5cBgduF7OtYR6z7\n",
      "Document indexed successfully with ID: ykOIr5cBPiy9PEnmSNF3\n",
      "Document indexed successfully with ID: 9HCIr5cBgduF7OtYSKz9\n",
      "Document indexed successfully with ID: y0OIr5cBPiy9PEnmSdGU\n",
      "Document indexed successfully with ID: 9XCIr5cBgduF7OtYSqw5\n",
      "Document indexed successfully with ID: zEOIr5cBPiy9PEnmStG5\n",
      "Document indexed successfully with ID: 9nCIr5cBgduF7OtYS6xA\n",
      "Document indexed successfully with ID: zUOIr5cBPiy9PEnmS9HK\n",
      "Document indexed successfully with ID: 93CIr5cBgduF7OtYTKxN\n",
      "Document indexed successfully with ID: zkOIr5cBPiy9PEnmTNHG\n",
      "Document indexed successfully with ID: -HCIr5cBgduF7OtYTaxB\n",
      "Document indexed successfully with ID: z0OIr5cBPiy9PEnmTdG2\n",
      "Document indexed successfully with ID: -XCIr5cBgduF7OtYTqxN\n",
      "Document indexed successfully with ID: 0EOIr5cBPiy9PEnmTtHA\n",
      "Document indexed successfully with ID: -nCIr5cBgduF7OtYT6w3\n",
      "Document indexed successfully with ID: 0UOIr5cBPiy9PEnmT9Gx\n",
      "Document indexed successfully with ID: -3CIr5cBgduF7OtYUKwz\n",
      "Document indexed successfully with ID: 0kOIr5cBPiy9PEnmUNHJ\n",
      "Document indexed successfully with ID: _HCIr5cBgduF7OtYUaw9\n",
      "Document indexed successfully with ID: 00OIr5cBPiy9PEnmUdGz\n",
      "Document indexed successfully with ID: _XCIr5cBgduF7OtYUqw1\n",
      "Document indexed successfully with ID: 1EOIr5cBPiy9PEnmUtG4\n",
      "Document indexed successfully with ID: _nCIr5cBgduF7OtYU6w-\n",
      "Document indexed successfully with ID: 1UOIr5cBPiy9PEnmU9HY\n",
      "Document indexed successfully with ID: _3CIr5cBgduF7OtYVKxP\n",
      "Document indexed successfully with ID: 1kOIr5cBPiy9PEnmVNHG\n",
      "Document indexed successfully with ID: AHCIr5cBgduF7OtYVa1J\n",
      "Document indexed successfully with ID: 10OIr5cBPiy9PEnmVdG_\n",
      "Document indexed successfully with ID: AXCIr5cBgduF7OtYVq1L\n",
      "Document indexed successfully with ID: 2EOIr5cBPiy9PEnmVtHg\n",
      "Document indexed successfully with ID: AnCIr5cBgduF7OtYV61r\n",
      "Document indexed successfully with ID: 2UOIr5cBPiy9PEnmV9Ht\n",
      "Document indexed successfully with ID: A3CIr5cBgduF7OtYWK1y\n",
      "Document indexed successfully with ID: 2kOIr5cBPiy9PEnmWNHx\n",
      "Document indexed successfully with ID: BHCIr5cBgduF7OtYWa1w\n",
      "Document indexed successfully with ID: 20OIr5cBPiy9PEnmWdHp\n",
      "Document indexed successfully with ID: BXCIr5cBgduF7OtYWq1i\n",
      "Document indexed successfully with ID: 3EOIr5cBPiy9PEnmW9EH\n",
      "Document indexed successfully with ID: BnCIr5cBgduF7OtYW62D\n",
      "Document indexed successfully with ID: 3UOIr5cBPiy9PEnmW9H5\n",
      "Document indexed successfully with ID: B3CIr5cBgduF7OtYXK2E\n",
      "Document indexed successfully with ID: 3kOIr5cBPiy9PEnmXdEA\n",
      "Document indexed successfully with ID: CHCIr5cBgduF7OtYXa17\n",
      "Document indexed successfully with ID: 30OIr5cBPiy9PEnmXdH_\n",
      "Document indexed successfully with ID: CXCIr5cBgduF7OtYXq13\n",
      "Document indexed successfully with ID: 4EOIr5cBPiy9PEnmXtH0\n",
      "Document indexed successfully with ID: CnCIr5cBgduF7OtYX62A\n",
      "Document indexed successfully with ID: 4UOIr5cBPiy9PEnmYNEA\n",
      "Document indexed successfully with ID: C3CIr5cBgduF7OtYYK2F\n",
      "Document indexed successfully with ID: 4kOIr5cBPiy9PEnmYdEH\n",
      "Document indexed successfully with ID: DHCIr5cBgduF7OtYYa2c\n",
      "Document indexed successfully with ID: 40OIr5cBPiy9PEnmYtEg\n",
      "Document indexed successfully with ID: DXCIr5cBgduF7OtYYq2m\n",
      "Document indexed successfully with ID: 5EOIr5cBPiy9PEnmY9Es\n",
      "Document indexed successfully with ID: DnCIr5cBgduF7OtYY62w\n",
      "Document indexed successfully with ID: 5UOIr5cBPiy9PEnmZNEx\n",
      "Document indexed successfully with ID: D3CIr5cBgduF7OtYZK2v\n",
      "Document indexed successfully with ID: 5kOIr5cBPiy9PEnmZdE_\n",
      "Document indexed successfully with ID: EHCIr5cBgduF7OtYZa25\n",
      "Document indexed successfully with ID: 50OIr5cBPiy9PEnmZtE5\n",
      "Document indexed successfully with ID: EXCIr5cBgduF7OtYZq3v\n",
      "Document indexed successfully with ID: 6EOIr5cBPiy9PEnmZ9Fn\n",
      "Document indexed successfully with ID: EnCIr5cBgduF7OtYZ63l\n",
      "Document indexed successfully with ID: 6UOIr5cBPiy9PEnmaNFZ\n",
      "Document indexed successfully with ID: E3CIr5cBgduF7OtYaK3U\n",
      "Document indexed successfully with ID: 6kOIr5cBPiy9PEnmadFI\n",
      "Document indexed successfully with ID: FHCIr5cBgduF7OtYaa3N\n",
      "Document indexed successfully with ID: 60OIr5cBPiy9PEnmatFE\n",
      "Document indexed successfully with ID: FXCIr5cBgduF7OtYaq3Q\n",
      "Document indexed successfully with ID: 7EOIr5cBPiy9PEnma9FV\n",
      "Document indexed successfully with ID: FnCIr5cBgduF7OtYa63b\n",
      "Document indexed successfully with ID: 7UOIr5cBPiy9PEnmbNFQ\n",
      "Document indexed successfully with ID: F3CIr5cBgduF7OtYbK3e\n",
      "Document indexed successfully with ID: 7kOIr5cBPiy9PEnmbdFo\n",
      "Document indexed successfully with ID: GHCIr5cBgduF7OtYbq0A\n",
      "Document indexed successfully with ID: 70OIr5cBPiy9PEnmbtF4\n",
      "Document indexed successfully with ID: GXCIr5cBgduF7OtYbq3s\n",
      "Document indexed successfully with ID: 8EOIr5cBPiy9PEnmb9Fn\n",
      "Document indexed successfully with ID: GnCIr5cBgduF7OtYb63n\n",
      "Document indexed successfully with ID: 8UOIr5cBPiy9PEnmcNFb\n",
      "Document indexed successfully with ID: G3CIr5cBgduF7OtYcK3W\n",
      "Document indexed successfully with ID: 8kOIr5cBPiy9PEnmcdFM\n",
      "Document indexed successfully with ID: HHCIr5cBgduF7OtYca3U\n",
      "Document indexed successfully with ID: 80OIr5cBPiy9PEnmctFf\n",
      "Document indexed successfully with ID: HXCIr5cBgduF7OtYcq3Y\n",
      "Document indexed successfully with ID: 9EOIr5cBPiy9PEnmc9FX\n",
      "Document indexed successfully with ID: HnCIr5cBgduF7OtYc63c\n",
      "Document indexed successfully with ID: 9UOIr5cBPiy9PEnmdNFa\n",
      "Document indexed successfully with ID: H3CIr5cBgduF7OtYdK3n\n",
      "Document indexed successfully with ID: 9kOIr5cBPiy9PEnmddFp\n",
      "Document indexed successfully with ID: IHCIr5cBgduF7OtYda3p\n",
      "Document indexed successfully with ID: 90OIr5cBPiy9PEnmdtFh\n",
      "Document indexed successfully with ID: IXCIr5cBgduF7OtYdq3i\n",
      "Document indexed successfully with ID: -EOIr5cBPiy9PEnmd9Ff\n",
      "Document indexed successfully with ID: InCIr5cBgduF7OtYd63Q\n",
      "Document indexed successfully with ID: -UOIr5cBPiy9PEnmeNFO\n",
      "Document indexed successfully with ID: I3CIr5cBgduF7OtYeK3S\n",
      "Document indexed successfully with ID: -kOIr5cBPiy9PEnmedE_\n",
      "Document indexed successfully with ID: JHCIr5cBgduF7OtYea3F\n",
      "Document indexed successfully with ID: -0OIr5cBPiy9PEnmetFB\n",
      "Document indexed successfully with ID: JXCIr5cBgduF7OtYeq3P\n",
      "Document indexed successfully with ID: _EOIr5cBPiy9PEnme9FR\n",
      "Document indexed successfully with ID: JnCIr5cBgduF7OtYe63F\n",
      "Document indexed successfully with ID: _UOIr5cBPiy9PEnmfNE7\n",
      "Document indexed successfully with ID: J3CIr5cBgduF7OtYfK23\n",
      "Document indexed successfully with ID: _kOIr5cBPiy9PEnmfdFO\n",
      "Document indexed successfully with ID: KHCIr5cBgduF7OtYfa3E\n",
      "Document indexed successfully with ID: _0OIr5cBPiy9PEnmftE3\n",
      "Document indexed successfully with ID: KXCIr5cBgduF7OtYfq2w\n",
      "Document indexed successfully with ID: AEOIr5cBPiy9PEnmf9Ix\n",
      "Document indexed successfully with ID: KnCIr5cBgduF7OtYf626\n",
      "Document indexed successfully with ID: AUOIr5cBPiy9PEnmgNIt\n",
      "Document indexed successfully with ID: K3CIr5cBgduF7OtYgK2e\n",
      "Document indexed successfully with ID: AkOIr5cBPiy9PEnmgdIN\n",
      "Document indexed successfully with ID: LHCIr5cBgduF7OtYga2G\n",
      "Document indexed successfully with ID: A0OIr5cBPiy9PEnmgtIC\n",
      "Document indexed successfully with ID: LXCIr5cBgduF7OtYgq11\n",
      "Document indexed successfully with ID: BEOIr5cBPiy9PEnmgtLz\n",
      "Document indexed successfully with ID: LnCIr5cBgduF7OtYg61u\n",
      "Document indexed successfully with ID: BUOIr5cBPiy9PEnmg9Lj\n",
      "Document indexed successfully with ID: L3CIr5cBgduF7OtYhK1a\n",
      "Document indexed successfully with ID: BkOIr5cBPiy9PEnmhNLf\n",
      "Document indexed successfully with ID: MHCIr5cBgduF7OtYha1X\n",
      "Document indexed successfully with ID: B0OIr5cBPiy9PEnmhdLk\n",
      "Document indexed successfully with ID: MXCIr5cBgduF7OtYhq1y\n",
      "Document indexed successfully with ID: CEOIr5cBPiy9PEnmhtLm\n",
      "Document indexed successfully with ID: MnCIr5cBgduF7OtYh61t\n",
      "Document indexed successfully with ID: CUOIr5cBPiy9PEnmh9Ln\n",
      "Document indexed successfully with ID: M3CIr5cBgduF7OtYiK12\n",
      "Document indexed successfully with ID: CkOIr5cBPiy9PEnmidIi\n",
      "Document indexed successfully with ID: NHCIr5cBgduF7OtYia2S\n",
      "Document indexed successfully with ID: C0OIr5cBPiy9PEnmitIJ\n",
      "Document indexed successfully with ID: NXCIr5cBgduF7OtYiq2E\n",
      "Document indexed successfully with ID: DEOIr5cBPiy9PEnmitL0\n",
      "Document indexed successfully with ID: NnCIr5cBgduF7OtYi611\n",
      "Document indexed successfully with ID: DUOIr5cBPiy9PEnmi9Ly\n",
      "Document indexed successfully with ID: N3CIr5cBgduF7OtYjK1_\n",
      "Document indexed successfully with ID: DkOIr5cBPiy9PEnmjNL3\n",
      "Document indexed successfully with ID: OHCIr5cBgduF7OtYja12\n",
      "Document indexed successfully with ID: D0OIr5cBPiy9PEnmjdLt\n",
      "Document indexed successfully with ID: OXCIr5cBgduF7OtYjq1t\n",
      "Document indexed successfully with ID: EEOIr5cBPiy9PEnmjtLd\n",
      "Document indexed successfully with ID: OnCIr5cBgduF7OtYj61j\n",
      "Document indexed successfully with ID: EUOIr5cBPiy9PEnmj9Lj\n",
      "Document indexed successfully with ID: O3CIr5cBgduF7OtYkK1w\n",
      "Document indexed successfully with ID: EkOIr5cBPiy9PEnmkNLg\n",
      "Document indexed successfully with ID: PHCIr5cBgduF7OtYka1p\n",
      "Document indexed successfully with ID: E0OIr5cBPiy9PEnmkdL6\n",
      "Document indexed successfully with ID: PXCIr5cBgduF7OtYkq1u\n",
      "Document indexed successfully with ID: FEOIr5cBPiy9PEnmktLf\n",
      "Document indexed successfully with ID: PnCIr5cBgduF7OtYk61s\n",
      "Document indexed successfully with ID: FUOIr5cBPiy9PEnmk9Ln\n",
      "Document indexed successfully with ID: P3CIr5cBgduF7OtYlK1d\n",
      "Document indexed successfully with ID: FkOIr5cBPiy9PEnmlNLU\n",
      "Document indexed successfully with ID: QHCIr5cBgduF7OtYla1T\n",
      "Document indexed successfully with ID: F0OIr5cBPiy9PEnmldLm\n",
      "Document indexed successfully with ID: QXCIr5cBgduF7OtYlq1v\n",
      "Document indexed successfully with ID: GEOIr5cBPiy9PEnmltLk\n",
      "Document indexed successfully with ID: QnCIr5cBgduF7OtYl61n\n",
      "Document indexed successfully with ID: GUOIr5cBPiy9PEnml9Li\n",
      "Document indexed successfully with ID: Q3CIr5cBgduF7OtYmK1b\n",
      "Document indexed successfully with ID: GkOIr5cBPiy9PEnmmNLT\n",
      "Document indexed successfully with ID: RHCIr5cBgduF7OtYma1P\n",
      "Document indexed successfully with ID: G0OIr5cBPiy9PEnmmdLN\n",
      "Document indexed successfully with ID: RXCIr5cBgduF7OtYmq1I\n",
      "Document indexed successfully with ID: HEOIr5cBPiy9PEnmmtLS\n",
      "Document indexed successfully with ID: RnCIr5cBgduF7OtYm61d\n",
      "Document indexed successfully with ID: HUOIr5cBPiy9PEnmm9Lh\n",
      "Document indexed successfully with ID: R3CIr5cBgduF7OtYnK1e\n",
      "Document indexed successfully with ID: HkOIr5cBPiy9PEnmnNLf\n",
      "Document indexed successfully with ID: SHCIr5cBgduF7OtYna1P\n",
      "Document indexed successfully with ID: H0OIr5cBPiy9PEnmndLF\n",
      "Document indexed successfully with ID: SXCIr5cBgduF7OtYnq1W\n",
      "Document indexed successfully with ID: IEOIr5cBPiy9PEnmntLB\n",
      "Document indexed successfully with ID: SnCIr5cBgduF7OtYn61E\n",
      "Document indexed successfully with ID: IUOIr5cBPiy9PEnmn9LD\n",
      "Document indexed successfully with ID: S3CIr5cBgduF7OtYoK1C\n",
      "Document indexed successfully with ID: IkOIr5cBPiy9PEnmoNK8\n",
      "Document indexed successfully with ID: THCIr5cBgduF7OtYoa09\n",
      "Document indexed successfully with ID: I0OIr5cBPiy9PEnmodLM\n",
      "Document indexed successfully with ID: TXCIr5cBgduF7OtYoq09\n",
      "Document indexed successfully with ID: JEOIr5cBPiy9PEnmotK6\n",
      "Document indexed successfully with ID: TnCIr5cBgduF7OtYo60w\n",
      "Document indexed successfully with ID: JUOIr5cBPiy9PEnmo9Ll\n",
      "Document indexed successfully with ID: T3CIr5cBgduF7OtYpK1e\n",
      "Document indexed successfully with ID: JkOIr5cBPiy9PEnmpNLg\n",
      "Document indexed successfully with ID: UHCIr5cBgduF7OtYpa1U\n",
      "Document indexed successfully with ID: J0OIr5cBPiy9PEnmpdK9\n",
      "Document indexed successfully with ID: UXCIr5cBgduF7OtYpq0v\n",
      "Document indexed successfully with ID: KEOIr5cBPiy9PEnmptK7\n",
      "Document indexed successfully with ID: UnCIr5cBgduF7OtYp601\n",
      "Document indexed successfully with ID: KUOIr5cBPiy9PEnmp9Kr\n",
      "Document indexed successfully with ID: U3CIr5cBgduF7OtYqK05\n",
      "Document indexed successfully with ID: KkOIr5cBPiy9PEnmqNK3\n",
      "Document indexed successfully with ID: VHCIr5cBgduF7OtYqa0u\n",
      "Document indexed successfully with ID: K0OIr5cBPiy9PEnmqdKf\n",
      "Document indexed successfully with ID: VXCIr5cBgduF7OtYqq0m\n",
      "Document indexed successfully with ID: LEOIr5cBPiy9PEnmqtKn\n",
      "Document indexed successfully with ID: VnCIr5cBgduF7OtYq60f\n",
      "Document indexed successfully with ID: LUOIr5cBPiy9PEnmq9Kv\n",
      "Document indexed successfully with ID: V3CIr5cBgduF7OtYrK0_\n",
      "Document indexed successfully with ID: LkOIr5cBPiy9PEnmrNLC\n",
      "Document indexed successfully with ID: WHCIr5cBgduF7OtYra0_\n",
      "Document indexed successfully with ID: L0OIr5cBPiy9PEnmrdK3\n",
      "Document indexed successfully with ID: WXCIr5cBgduF7OtYrq01\n",
      "Document indexed successfully with ID: MEOIr5cBPiy9PEnmrtKz\n",
      "Document indexed successfully with ID: WnCIr5cBgduF7OtYr60k\n",
      "Document indexed successfully with ID: MUOIr5cBPiy9PEnmr9Kj\n",
      "Document indexed successfully with ID: W3CIr5cBgduF7OtYsK0Z\n",
      "Document indexed successfully with ID: MkOIr5cBPiy9PEnmsNKb\n",
      "Document indexed successfully with ID: XHCIr5cBgduF7OtYsa0o\n",
      "Document indexed successfully with ID: M0OIr5cBPiy9PEnmsdKh\n",
      "Document indexed successfully with ID: XXCIr5cBgduF7OtYsq0r\n",
      "Document indexed successfully with ID: NEOIr5cBPiy9PEnmstLT\n",
      "Document indexed successfully with ID: XnCIr5cBgduF7OtYs61F\n",
      "Document indexed successfully with ID: NUOIr5cBPiy9PEnms9LA\n",
      "Document indexed successfully with ID: X3CIr5cBgduF7OtYtK09\n",
      "Document indexed successfully with ID: NkOIr5cBPiy9PEnmtNLB\n",
      "Document indexed successfully with ID: YHCIr5cBgduF7OtYta1A\n",
      "Document indexed successfully with ID: N0OIr5cBPiy9PEnmtdK3\n",
      "Document indexed successfully with ID: YXCIr5cBgduF7OtYtq0r\n",
      "Document indexed successfully with ID: OEOIr5cBPiy9PEnmttKh\n",
      "Document indexed successfully with ID: YnCIr5cBgduF7OtYt60Q\n",
      "Document indexed successfully with ID: OUOIr5cBPiy9PEnmt9Kh\n",
      "Document indexed successfully with ID: Y3CIr5cBgduF7OtYuK0Z\n",
      "Document indexed successfully with ID: OkOIr5cBPiy9PEnmuNKW\n",
      "Document indexed successfully with ID: ZHCIr5cBgduF7OtYua0P\n",
      "Document indexed successfully with ID: O0OIr5cBPiy9PEnmudKS\n",
      "Document indexed successfully with ID: ZXCIr5cBgduF7OtYuq0O\n",
      "Document indexed successfully with ID: PEOIr5cBPiy9PEnmutKG\n",
      "Document indexed successfully with ID: ZnCIr5cBgduF7OtYu60G\n",
      "Document indexed successfully with ID: PUOIr5cBPiy9PEnmu9KI\n",
      "Document indexed successfully with ID: Z3CIr5cBgduF7OtYvK0E\n",
      "Document indexed successfully with ID: PkOIr5cBPiy9PEnmvNKM\n",
      "Document indexed successfully with ID: aHCIr5cBgduF7OtYva0J\n",
      "Document indexed successfully with ID: P0OIr5cBPiy9PEnmvdKN\n",
      "Document indexed successfully with ID: aXCIr5cBgduF7OtYvq0G\n",
      "Document indexed successfully with ID: QEOIr5cBPiy9PEnmvtJ8\n",
      "Document indexed successfully with ID: anCIr5cBgduF7OtYv60K\n",
      "Document indexed successfully with ID: QUOIr5cBPiy9PEnmv9KS\n",
      "Document indexed successfully with ID: a3CIr5cBgduF7OtYwK0f\n",
      "Document indexed successfully with ID: QkOIr5cBPiy9PEnmwNKU\n",
      "Document indexed successfully with ID: bHCIr5cBgduF7OtYwa0N\n",
      "Document indexed successfully with ID: Q0OIr5cBPiy9PEnmwdKO\n",
      "Document indexed successfully with ID: bXCIr5cBgduF7OtYwq0L\n",
      "Document indexed successfully with ID: REOIr5cBPiy9PEnmwtKS\n",
      "Document indexed successfully with ID: bnCIr5cBgduF7OtYw60W\n",
      "Document indexed successfully with ID: RUOIr5cBPiy9PEnmw9Kj\n",
      "Document indexed successfully with ID: b3CIr5cBgduF7OtYxK0h\n",
      "Document indexed successfully with ID: RkOIr5cBPiy9PEnmxNKa\n",
      "Document indexed successfully with ID: cHCIr5cBgduF7OtYxa0v\n",
      "Document indexed successfully with ID: R0OIr5cBPiy9PEnmxdKl\n",
      "Document indexed successfully with ID: cXCIr5cBgduF7OtYxq0m\n",
      "Document indexed successfully with ID: SEOIr5cBPiy9PEnmxtKp\n",
      "Document indexed successfully with ID: cnCIr5cBgduF7OtYx60Y\n",
      "Document indexed successfully with ID: SUOIr5cBPiy9PEnmx9KC\n",
      "Document indexed successfully with ID: c3CIr5cBgduF7OtYyK0T\n",
      "Document indexed successfully with ID: SkOIr5cBPiy9PEnmyNK4\n",
      "Document indexed successfully with ID: dHCIr5cBgduF7OtYya1W\n",
      "Document indexed successfully with ID: S0OIr5cBPiy9PEnmydLb\n",
      "Document indexed successfully with ID: dXCIr5cBgduF7OtYyq1X\n",
      "Document indexed successfully with ID: TEOIr5cBPiy9PEnmytLZ\n",
      "Document indexed successfully with ID: dnCIr5cBgduF7OtYy61j\n",
      "Document indexed successfully with ID: TUOIr5cBPiy9PEnmy9Lf\n",
      "Document indexed successfully with ID: d3CIr5cBgduF7OtYzK1f\n",
      "Document indexed successfully with ID: TkOIr5cBPiy9PEnmzNLU\n",
      "Document indexed successfully with ID: eHCIr5cBgduF7OtYza1U\n",
      "Document indexed successfully with ID: T0OIr5cBPiy9PEnmzdLT\n",
      "Document indexed successfully with ID: eXCIr5cBgduF7OtYzq1a\n",
      "Document indexed successfully with ID: UEOIr5cBPiy9PEnmztLO\n",
      "Document indexed successfully with ID: enCIr5cBgduF7OtYz61T\n",
      "Document indexed successfully with ID: UUOIr5cBPiy9PEnmz9LX\n",
      "Document indexed successfully with ID: e3CIr5cBgduF7OtY0K1f\n",
      "Document indexed successfully with ID: UkOIr5cBPiy9PEnm0NLW\n",
      "Document indexed successfully with ID: fHCIr5cBgduF7OtY0a1t\n",
      "Document indexed successfully with ID: U0OIr5cBPiy9PEnm0dLu\n",
      "Document indexed successfully with ID: fXCIr5cBgduF7OtY0q12\n",
      "Document indexed successfully with ID: VEOIr5cBPiy9PEnm0tL5\n",
      "Document indexed successfully with ID: fnCIr5cBgduF7OtY061x\n",
      "Document indexed successfully with ID: VUOIr5cBPiy9PEnm09Lu\n",
      "Document indexed successfully with ID: f3CIr5cBgduF7OtY1K1s\n",
      "Document indexed successfully with ID: VkOIr5cBPiy9PEnm1NLo\n",
      "Document indexed successfully with ID: gHCIr5cBgduF7OtY1a1c\n",
      "Document indexed successfully with ID: V0OIr5cBPiy9PEnm1dLR\n",
      "Document indexed successfully with ID: gXCIr5cBgduF7OtY1q1O\n",
      "Document indexed successfully with ID: WEOIr5cBPiy9PEnm1tLB\n",
      "Document indexed successfully with ID: gnCIr5cBgduF7OtY1606\n",
      "Document indexed successfully with ID: WUOIr5cBPiy9PEnm19Ky\n",
      "Document indexed successfully with ID: g3CIr5cBgduF7OtY2K01\n",
      "Document indexed successfully with ID: WkOIr5cBPiy9PEnm2NK_\n",
      "Document indexed successfully with ID: hHCIr5cBgduF7OtY2a03\n",
      "Document indexed successfully with ID: W0OIr5cBPiy9PEnm2dKt\n",
      "Document indexed successfully with ID: hXCIr5cBgduF7OtY2q0q\n",
      "Document indexed successfully with ID: XEOIr5cBPiy9PEnm2tKm\n",
      "Document indexed successfully with ID: hnCIr5cBgduF7OtY260o\n",
      "Document indexed successfully with ID: XUOIr5cBPiy9PEnm29Kz\n",
      "Document indexed successfully with ID: h3CIr5cBgduF7OtY3K04\n",
      "Document indexed successfully with ID: XkOIr5cBPiy9PEnm3NLJ\n",
      "Document indexed successfully with ID: iHCIr5cBgduF7OtY3a1L\n",
      "Document indexed successfully with ID: X0OIr5cBPiy9PEnm3dLW\n",
      "Document indexed successfully with ID: iXCIr5cBgduF7OtY3q1K\n",
      "Document indexed successfully with ID: YEOIr5cBPiy9PEnm3tLP\n",
      "Document indexed successfully with ID: inCIr5cBgduF7OtY361S\n",
      "Document indexed successfully with ID: YUOIr5cBPiy9PEnm39LX\n",
      "Document indexed successfully with ID: i3CIr5cBgduF7OtY4K1c\n",
      "Document indexed successfully with ID: YkOIr5cBPiy9PEnm4NLU\n",
      "Document indexed successfully with ID: jHCIr5cBgduF7OtY4a1U\n",
      "Document indexed successfully with ID: Y0OIr5cBPiy9PEnm4dLb\n",
      "Document indexed successfully with ID: jXCIr5cBgduF7OtY4q1a\n",
      "Document indexed successfully with ID: ZEOIr5cBPiy9PEnm4tLV\n",
      "Document indexed successfully with ID: jnCIr5cBgduF7OtY461R\n",
      "Document indexed successfully with ID: ZUOIr5cBPiy9PEnm49LT\n",
      "Document indexed successfully with ID: j3CIr5cBgduF7OtY5K1T\n",
      "Document indexed successfully with ID: ZkOIr5cBPiy9PEnm5NLM\n",
      "Document indexed successfully with ID: kHCIr5cBgduF7OtY5a1F\n",
      "Document indexed successfully with ID: Z0OIr5cBPiy9PEnm5dLA\n",
      "Document indexed successfully with ID: kXCIr5cBgduF7OtY5q08\n",
      "Document indexed successfully with ID: aEOIr5cBPiy9PEnm5tK-\n",
      "Document indexed successfully with ID: knCIr5cBgduF7OtY5600\n",
      "Document indexed successfully with ID: aUOIr5cBPiy9PEnm59Ky\n",
      "Document indexed successfully with ID: k3CIr5cBgduF7OtY6K1B\n",
      "Document indexed successfully with ID: akOIr5cBPiy9PEnm6NLG\n",
      "Document indexed successfully with ID: lHCIr5cBgduF7OtY6a05\n",
      "Document indexed successfully with ID: a0OIr5cBPiy9PEnm6dK1\n",
      "Document indexed successfully with ID: lXCIr5cBgduF7OtY6q0q\n",
      "Document indexed successfully with ID: bEOIr5cBPiy9PEnm6tKk\n",
      "Document indexed successfully with ID: lnCIr5cBgduF7OtY660c\n",
      "Document indexed successfully with ID: bUOIr5cBPiy9PEnm69KQ\n",
      "Document indexed successfully with ID: l3CIr5cBgduF7OtY7K0P\n",
      "Document indexed successfully with ID: bkOIr5cBPiy9PEnm7NKF\n",
      "Document indexed successfully with ID: mHCIr5cBgduF7OtY7K32\n",
      "Document indexed successfully with ID: b0OIr5cBPiy9PEnm7dJ5\n",
      "Document indexed successfully with ID: mXCIr5cBgduF7OtY7a34\n",
      "Document indexed successfully with ID: cEOIr5cBPiy9PEnm7tJz\n",
      "Document indexed successfully with ID: mnCIr5cBgduF7OtY7q3m\n",
      "Document indexed successfully with ID: cUOIr5cBPiy9PEnm79JX\n",
      "Document indexed successfully with ID: m3CIr5cBgduF7OtY763Q\n",
      "Document indexed successfully with ID: ckOIr5cBPiy9PEnm8NJI\n",
      "Document indexed successfully with ID: nHCIr5cBgduF7OtY8K23\n",
      "Document indexed successfully with ID: c0OIr5cBPiy9PEnm8dIw\n",
      "Document indexed successfully with ID: nXCIr5cBgduF7OtY8a2n\n",
      "Document indexed successfully with ID: dEOIr5cBPiy9PEnm8tIi\n",
      "Document indexed successfully with ID: nnCIr5cBgduF7OtY8q2k\n",
      "Document indexed successfully with ID: dUOIr5cBPiy9PEnm89Im\n",
      "Document indexed successfully with ID: n3CIr5cBgduF7OtY862m\n",
      "Document indexed successfully with ID: dkOIr5cBPiy9PEnm9NIn\n",
      "Document indexed successfully with ID: oHCIr5cBgduF7OtY9K2s\n",
      "Document indexed successfully with ID: d0OIr5cBPiy9PEnm9dIh\n",
      "Document indexed successfully with ID: oXCIr5cBgduF7OtY9a2h\n",
      "Document indexed successfully with ID: eEOIr5cBPiy9PEnm9tIY\n",
      "Document indexed successfully with ID: onCIr5cBgduF7OtY9q2l\n",
      "Document indexed successfully with ID: eUOIr5cBPiy9PEnm99If\n",
      "Document indexed successfully with ID: o3CIr5cBgduF7OtY962Z\n",
      "Document indexed successfully with ID: ekOIr5cBPiy9PEnm-NIW\n",
      "Document indexed successfully with ID: pHCIr5cBgduF7OtY-K2Y\n",
      "Document indexed successfully with ID: e0OIr5cBPiy9PEnm-dIQ\n",
      "Document indexed successfully with ID: pXCIr5cBgduF7OtY-a2g\n",
      "Document indexed successfully with ID: fEOIr5cBPiy9PEnm-tIc\n",
      "Document indexed successfully with ID: pnCIr5cBgduF7OtY-q2f\n",
      "Document indexed successfully with ID: fUOIr5cBPiy9PEnm-9Ii\n",
      "Document indexed successfully with ID: p3CIr5cBgduF7OtY-62u\n",
      "Document indexed successfully with ID: fkOIr5cBPiy9PEnm_NIu\n",
      "Document indexed successfully with ID: qHCIr5cBgduF7OtY_K2r\n",
      "Document indexed successfully with ID: f0OIr5cBPiy9PEnm_dIm\n",
      "Document indexed successfully with ID: qXCIr5cBgduF7OtY_a2f\n",
      "Document indexed successfully with ID: gEOIr5cBPiy9PEnm_tIS\n",
      "Document indexed successfully with ID: qnCIr5cBgduF7OtY_q2c\n",
      "Document indexed successfully with ID: gUOIr5cBPiy9PEnm_9Id\n",
      "Document indexed successfully with ID: q3CIr5cBgduF7OtY_62T\n",
      "Document indexed successfully with ID: gkOJr5cBPiy9PEnmANIN\n",
      "Document indexed successfully with ID: rHCJr5cBgduF7OtYAK2C\n",
      "Document indexed successfully with ID: g0OJr5cBPiy9PEnmAdIC\n",
      "Document indexed successfully with ID: rXCJr5cBgduF7OtYAa2G\n",
      "Document indexed successfully with ID: hEOJr5cBPiy9PEnmAdL7\n",
      "Document indexed successfully with ID: rnCJr5cBgduF7OtYAq14\n",
      "Document indexed successfully with ID: hUOJr5cBPiy9PEnmAtL2\n",
      "Document indexed successfully with ID: r3CJr5cBgduF7OtYA614\n",
      "Document indexed successfully with ID: hkOJr5cBPiy9PEnmA9L6\n",
      "Document indexed successfully with ID: sHCJr5cBgduF7OtYBK11\n",
      "Document indexed successfully with ID: h0OJr5cBPiy9PEnmBNLv\n",
      "Document indexed successfully with ID: sXCJr5cBgduF7OtYBa1l\n",
      "Document indexed successfully with ID: iEOJr5cBPiy9PEnmBdLn\n",
      "Document indexed successfully with ID: snCJr5cBgduF7OtYBq1g\n",
      "Document indexed successfully with ID: iUOJr5cBPiy9PEnmBtLR\n",
      "Document indexed successfully with ID: s3CJr5cBgduF7OtYB61J\n",
      "Document indexed successfully with ID: ikOJr5cBPiy9PEnmB9LH\n",
      "Document indexed successfully with ID: tHCJr5cBgduF7OtYCK1E\n",
      "Document indexed successfully with ID: i0OJr5cBPiy9PEnmCNLC\n",
      "Document indexed successfully with ID: tXCJr5cBgduF7OtYCa07\n",
      "Document indexed successfully with ID: jEOJr5cBPiy9PEnmCdK4\n",
      "Document indexed successfully with ID: tnCJr5cBgduF7OtYCq0w\n",
      "Document indexed successfully with ID: jUOJr5cBPiy9PEnmCtKp\n",
      "Document indexed successfully with ID: t3CJr5cBgduF7OtYC60n\n",
      "Document indexed successfully with ID: jkOJr5cBPiy9PEnmC9Kf\n",
      "Document indexed successfully with ID: uHCJr5cBgduF7OtYDK0R\n",
      "Document indexed successfully with ID: j0OJr5cBPiy9PEnmDNKH\n",
      "Document indexed successfully with ID: uXCJr5cBgduF7OtYDa0E\n",
      "Document indexed successfully with ID: kEOJr5cBPiy9PEnmDdJ4\n",
      "Document indexed successfully with ID: unCJr5cBgduF7OtYDa3t\n",
      "Document indexed successfully with ID: kUOJr5cBPiy9PEnmDtJv\n",
      "Document indexed successfully with ID: u3CJr5cBgduF7OtYDq3q\n",
      "Document indexed successfully with ID: kkOJr5cBPiy9PEnmD9Jp\n",
      "Document indexed successfully with ID: vHCJr5cBgduF7OtYD636\n",
      "Document indexed successfully with ID: k0OJr5cBPiy9PEnmENKX\n",
      "Document indexed successfully with ID: vXCJr5cBgduF7OtYEa0R\n",
      "Document indexed successfully with ID: lEOJr5cBPiy9PEnmEdKH\n",
      "Document indexed successfully with ID: vnCJr5cBgduF7OtYEa38\n",
      "Document indexed successfully with ID: lUOJr5cBPiy9PEnmEtJ8\n",
      "Document indexed successfully with ID: v3CJr5cBgduF7OtYEq38\n",
      "Document indexed successfully with ID: lkOJr5cBPiy9PEnmE9J6\n",
      "Document indexed successfully with ID: wHCJr5cBgduF7OtYE63_\n",
      "Document indexed successfully with ID: l0OJr5cBPiy9PEnmFNJ0\n",
      "Document indexed successfully with ID: wXCJr5cBgduF7OtYFK38\n",
      "Document indexed successfully with ID: mEOJr5cBPiy9PEnmFdJt\n",
      "Document indexed successfully with ID: wnCJr5cBgduF7OtYFq0I\n",
      "Document indexed successfully with ID: mUOJr5cBPiy9PEnmFtKQ\n",
      "Document indexed successfully with ID: w3CJr5cBgduF7OtYF60X\n",
      "Document indexed successfully with ID: mkOJr5cBPiy9PEnmF9KK\n",
      "Document indexed successfully with ID: xHCJr5cBgduF7OtYGK0G\n",
      "Document indexed successfully with ID: m0OJr5cBPiy9PEnmGNJ3\n",
      "Document indexed successfully with ID: xXCJr5cBgduF7OtYGK3y\n",
      "Document indexed successfully with ID: nEOJr5cBPiy9PEnmGdKC\n",
      "Document indexed successfully with ID: xnCJr5cBgduF7OtYGa39\n",
      "Document indexed successfully with ID: nUOJr5cBPiy9PEnmGtKC\n",
      "Document indexed successfully with ID: x3CJr5cBgduF7OtYGq38\n",
      "Document indexed successfully with ID: nkOJr5cBPiy9PEnmG9Jv\n",
      "Document indexed successfully with ID: yHCJr5cBgduF7OtYG63m\n",
      "Document indexed successfully with ID: n0OJr5cBPiy9PEnmHNJh\n",
      "Document indexed successfully with ID: yXCJr5cBgduF7OtYHK3Z\n",
      "Document indexed successfully with ID: oEOJr5cBPiy9PEnmHdJI\n",
      "Document indexed successfully with ID: ynCJr5cBgduF7OtYHa24\n",
      "Document indexed successfully with ID: oUOJr5cBPiy9PEnmHtIv\n",
      "Document indexed successfully with ID: y3CJr5cBgduF7OtYHq29\n",
      "Document indexed successfully with ID: okOJr5cBPiy9PEnmH9JG\n",
      "Document indexed successfully with ID: zHCJr5cBgduF7OtYH63S\n",
      "Document indexed successfully with ID: o0OJr5cBPiy9PEnmINJL\n",
      "Document indexed successfully with ID: zXCJr5cBgduF7OtYIK3S\n",
      "Document indexed successfully with ID: pEOJr5cBPiy9PEnmIdJP\n",
      "Document indexed successfully with ID: znCJr5cBgduF7OtYIa3Q\n",
      "Document indexed successfully with ID: pUOJr5cBPiy9PEnmItJB\n",
      "Document indexed successfully with ID: z3CJr5cBgduF7OtYIq3J\n",
      "Document indexed successfully with ID: pkOJr5cBPiy9PEnmI9JG\n",
      "Document indexed successfully with ID: 0HCJr5cBgduF7OtYI62_\n",
      "Document indexed successfully with ID: p0OJr5cBPiy9PEnmJNI8\n",
      "Document indexed successfully with ID: 0XCJr5cBgduF7OtYJK28\n",
      "Document indexed successfully with ID: qEOJr5cBPiy9PEnmJdI9\n",
      "Document indexed successfully with ID: 0nCJr5cBgduF7OtYJa26\n",
      "Document indexed successfully with ID: qUOJr5cBPiy9PEnmJtIy\n",
      "Document indexed successfully with ID: 03CJr5cBgduF7OtYJq3H\n",
      "Document indexed successfully with ID: qkOJr5cBPiy9PEnmJ9JD\n",
      "Document indexed successfully with ID: 1HCJr5cBgduF7OtYJ62_\n",
      "Document indexed successfully with ID: q0OJr5cBPiy9PEnmKNIy\n",
      "Document indexed successfully with ID: 1XCJr5cBgduF7OtYKK20\n",
      "Document indexed successfully with ID: rEOJr5cBPiy9PEnmKdIl\n",
      "Document indexed successfully with ID: 1nCJr5cBgduF7OtYKa2i\n",
      "Document indexed successfully with ID: rUOJr5cBPiy9PEnmKtIU\n",
      "Document indexed successfully with ID: 13CJr5cBgduF7OtYKq20\n",
      "Document indexed successfully with ID: rkOJr5cBPiy9PEnmK9Is\n",
      "Document indexed successfully with ID: 2HCJr5cBgduF7OtYK62w\n",
      "Document indexed successfully with ID: r0OJr5cBPiy9PEnmLNIs\n",
      "Document indexed successfully with ID: 2XCJr5cBgduF7OtYLK2p\n",
      "Document indexed successfully with ID: sEOJr5cBPiy9PEnmLdIs\n",
      "Document indexed successfully with ID: 2nCJr5cBgduF7OtYLa2o\n",
      "Document indexed successfully with ID: sUOJr5cBPiy9PEnmLtIc\n",
      "Document indexed successfully with ID: 23CJr5cBgduF7OtYLq3H\n",
      "Document indexed successfully with ID: skOJr5cBPiy9PEnmL9JG\n",
      "Document indexed successfully with ID: 3HCJr5cBgduF7OtYL63E\n",
      "Document indexed successfully with ID: s0OJr5cBPiy9PEnmMNI2\n",
      "Document indexed successfully with ID: 3XCJr5cBgduF7OtYMK2t\n",
      "Document indexed successfully with ID: tEOJr5cBPiy9PEnmMdIW\n",
      "Document indexed successfully with ID: 3nCJr5cBgduF7OtYMa2O\n",
      "Document indexed successfully with ID: tUOJr5cBPiy9PEnmMtIH\n",
      "Document indexed successfully with ID: 33CJr5cBgduF7OtYMq2S\n",
      "Document indexed successfully with ID: tkOJr5cBPiy9PEnmM9IC\n",
      "Document indexed successfully with ID: 4HCJr5cBgduF7OtYM620\n",
      "Document indexed successfully with ID: t0OJr5cBPiy9PEnmNNKL\n",
      "Document indexed successfully with ID: 4XCJr5cBgduF7OtYNa0S\n",
      "Document indexed successfully with ID: uEOJr5cBPiy9PEnmNdKJ\n",
      "Document indexed successfully with ID: 4nCJr5cBgduF7OtYNq0C\n",
      "Document indexed successfully with ID: uUOJr5cBPiy9PEnmNtJ5\n",
      "Document indexed successfully with ID: 43CJr5cBgduF7OtYNq32\n",
      "Document indexed successfully with ID: ukOJr5cBPiy9PEnmN9Jx\n",
      "Document indexed successfully with ID: 5HCJr5cBgduF7OtYN63t\n",
      "Document indexed successfully with ID: u0OJr5cBPiy9PEnmONJ9\n",
      "Document indexed successfully with ID: 5XCJr5cBgduF7OtYOK33\n",
      "Document indexed successfully with ID: vEOJr5cBPiy9PEnmOdJ5\n",
      "Document indexed successfully with ID: 5nCJr5cBgduF7OtYOa33\n",
      "Document indexed successfully with ID: vUOJr5cBPiy9PEnmOtJz\n",
      "Document indexed successfully with ID: 53CJr5cBgduF7OtYOq30\n",
      "Document indexed successfully with ID: vkOJr5cBPiy9PEnmO9J9\n",
      "Document indexed successfully with ID: 6HCJr5cBgduF7OtYO63y\n",
      "Document indexed successfully with ID: v0OJr5cBPiy9PEnmPNJp\n",
      "Document indexed successfully with ID: 6XCJr5cBgduF7OtYPK3u\n",
      "Document indexed successfully with ID: wEOJr5cBPiy9PEnmPdJr\n",
      "Document indexed successfully with ID: 6nCJr5cBgduF7OtYPa3h\n",
      "Document indexed successfully with ID: wUOJr5cBPiy9PEnmPtJi\n",
      "Document indexed successfully with ID: 63CJr5cBgduF7OtYPq3b\n",
      "Document indexed successfully with ID: wkOJr5cBPiy9PEnmP9JS\n",
      "Document indexed successfully with ID: 7HCJr5cBgduF7OtYP63J\n",
      "Document indexed successfully with ID: w0OJr5cBPiy9PEnmQNJB\n",
      "Document indexed successfully with ID: 7XCJr5cBgduF7OtYQK3J\n",
      "Document indexed successfully with ID: xEOJr5cBPiy9PEnmQdJM\n",
      "Document indexed successfully with ID: 7nCJr5cBgduF7OtYQa3P\n",
      "Document indexed successfully with ID: xUOJr5cBPiy9PEnmQtJM\n",
      "Document indexed successfully with ID: 73CJr5cBgduF7OtYQq3U\n",
      "Document indexed successfully with ID: xkOJr5cBPiy9PEnmQ9JF\n",
      "Document indexed successfully with ID: 8HCJr5cBgduF7OtYQ63V\n",
      "Document indexed successfully with ID: x0OJr5cBPiy9PEnmRNJa\n",
      "Document indexed successfully with ID: 8XCJr5cBgduF7OtYRK3i\n",
      "Document indexed successfully with ID: yEOJr5cBPiy9PEnmRdJW\n",
      "Document indexed successfully with ID: 8nCJr5cBgduF7OtYRa3L\n",
      "Document indexed successfully with ID: yUOJr5cBPiy9PEnmRtJS\n",
      "Document indexed successfully with ID: 83CJr5cBgduF7OtYRq3H\n",
      "Document indexed successfully with ID: ykOJr5cBPiy9PEnmR9JP\n",
      "Document indexed successfully with ID: 9HCJr5cBgduF7OtYR63D\n",
      "Document indexed successfully with ID: y0OJr5cBPiy9PEnmSNJN\n",
      "Document indexed successfully with ID: 9XCJr5cBgduF7OtYSK3U\n",
      "Document indexed successfully with ID: zEOJr5cBPiy9PEnmSdJQ\n",
      "Document indexed successfully with ID: 9nCJr5cBgduF7OtYSa3a\n",
      "Document indexed successfully with ID: zUOJr5cBPiy9PEnmStJp\n",
      "Document indexed successfully with ID: 93CJr5cBgduF7OtYSq3w\n",
      "Document indexed successfully with ID: zkOJr5cBPiy9PEnmS9KO\n",
      "Document indexed successfully with ID: -HCJr5cBgduF7OtYTK0c\n",
      "Document indexed successfully with ID: z0OJr5cBPiy9PEnmTNLD\n",
      "Document indexed successfully with ID: -XCJr5cBgduF7OtYTa1R\n",
      "Document indexed successfully with ID: 0EOJr5cBPiy9PEnmTdLU\n",
      "Document indexed successfully with ID: -nCJr5cBgduF7OtYTq1R\n",
      "Document indexed successfully with ID: 0UOJr5cBPiy9PEnmTtLY\n",
      "Document indexed successfully with ID: -3CJr5cBgduF7OtYT61J\n",
      "Document indexed successfully with ID: 0kOJr5cBPiy9PEnmT9LW\n",
      "Document indexed successfully with ID: _HCJr5cBgduF7OtYUK1S\n",
      "Document indexed successfully with ID: 00OJr5cBPiy9PEnmUNLU\n",
      "Document indexed successfully with ID: _XCJr5cBgduF7OtYUa1I\n",
      "Document indexed successfully with ID: 1EOJr5cBPiy9PEnmUdK7\n",
      "Document indexed successfully with ID: _nCJr5cBgduF7OtYUq06\n",
      "Document indexed successfully with ID: 1UOJr5cBPiy9PEnmUtLK\n",
      "Document indexed successfully with ID: _3CJr5cBgduF7OtYU61J\n",
      "Document indexed successfully with ID: 1kOJr5cBPiy9PEnmU9K8\n",
      "Document indexed successfully with ID: AHCJr5cBgduF7OtYVK4z\n",
      "Document indexed successfully with ID: 10OJr5cBPiy9PEnmVNKz\n",
      "Document indexed successfully with ID: AXCJr5cBgduF7OtYVa40\n",
      "Document indexed successfully with ID: 2EOJr5cBPiy9PEnmVdKl\n",
      "Document indexed successfully with ID: AnCJr5cBgduF7OtYVq4d\n",
      "Document indexed successfully with ID: 2UOJr5cBPiy9PEnmVtKh\n",
      "Document indexed successfully with ID: A3CJr5cBgduF7OtYV64z\n",
      "Document indexed successfully with ID: 2kOJr5cBPiy9PEnmV9K1\n",
      "Document indexed successfully with ID: BHCJr5cBgduF7OtYWK46\n",
      "Document indexed successfully with ID: 20OJr5cBPiy9PEnmWNK0\n",
      "Document indexed successfully with ID: BXCJr5cBgduF7OtYWa5U\n",
      "Document indexed successfully with ID: 3EOJr5cBPiy9PEnmWdLP\n",
      "Document indexed successfully with ID: BnCJr5cBgduF7OtYWq5N\n",
      "Document indexed successfully with ID: 3UOJr5cBPiy9PEnmWtLF\n",
      "Document indexed successfully with ID: B3CJr5cBgduF7OtYW65E\n",
      "Document indexed successfully with ID: 3kOJr5cBPiy9PEnmW9LA\n",
      "Document indexed successfully with ID: CHCJr5cBgduF7OtYXK43\n",
      "Document indexed successfully with ID: 30OJr5cBPiy9PEnmXNKz\n",
      "Document indexed successfully with ID: CXCJr5cBgduF7OtYXa4v\n",
      "Document indexed successfully with ID: 4EOJr5cBPiy9PEnmXdKv\n",
      "Document indexed successfully with ID: CnCJr5cBgduF7OtYXq4m\n",
      "Document indexed successfully with ID: 4UOJr5cBPiy9PEnmXtKY\n",
      "Document indexed successfully with ID: C3CJr5cBgduF7OtYX64T\n",
      "Document indexed successfully with ID: 4kOJr5cBPiy9PEnmX9KL\n",
      "Document indexed successfully with ID: DHCJr5cBgduF7OtYYK4G\n",
      "Document indexed successfully with ID: 40OJr5cBPiy9PEnmYNKJ\n",
      "Document indexed successfully with ID: DXCJr5cBgduF7OtYYa4N\n",
      "Document indexed successfully with ID: 5EOJr5cBPiy9PEnmYdKG\n",
      "Document indexed successfully with ID: DnCJr5cBgduF7OtYYq4G\n",
      "Document indexed successfully with ID: 5UOJr5cBPiy9PEnmYtKK\n",
      "Document indexed successfully with ID: D3CJr5cBgduF7OtYY64F\n",
      "Document indexed successfully with ID: 5kOJr5cBPiy9PEnmY9J5\n",
      "Document indexed successfully with ID: EHCJr5cBgduF7OtYY67x\n",
      "Document indexed successfully with ID: 50OJr5cBPiy9PEnmZNJ4\n",
      "Document indexed successfully with ID: EXCJr5cBgduF7OtYZK73\n",
      "Document indexed successfully with ID: 6EOJr5cBPiy9PEnmZdJu\n",
      "Document indexed successfully with ID: EnCJr5cBgduF7OtYZa7e\n",
      "Document indexed successfully with ID: 6UOJr5cBPiy9PEnmZtJO\n",
      "Document indexed successfully with ID: E3CJr5cBgduF7OtYZq6_\n",
      "Document indexed successfully with ID: 6kOJr5cBPiy9PEnmZ9I7\n",
      "Document indexed successfully with ID: FHCJr5cBgduF7OtYZ66x\n",
      "Document indexed successfully with ID: 60OJr5cBPiy9PEnmaNIq\n",
      "Document indexed successfully with ID: FXCJr5cBgduF7OtYaK6f\n",
      "Document indexed successfully with ID: 7EOJr5cBPiy9PEnmadId\n",
      "Document indexed successfully with ID: FnCJr5cBgduF7OtYaa6S\n",
      "Document indexed successfully with ID: 7UOJr5cBPiy9PEnmatIH\n",
      "Document indexed successfully with ID: F3CJr5cBgduF7OtYaq56\n",
      "Document indexed successfully with ID: 7kOJr5cBPiy9PEnmatLt\n",
      "Document indexed successfully with ID: GHCJr5cBgduF7OtYa65g\n",
      "Document indexed successfully with ID: 70OJr5cBPiy9PEnma9Lo\n",
      "Document indexed successfully with ID: GXCJr5cBgduF7OtYbK5Z\n",
      "Document indexed successfully with ID: 8EOJr5cBPiy9PEnmbNLY\n",
      "Document indexed successfully with ID: GnCJr5cBgduF7OtYba5Q\n",
      "Document indexed successfully with ID: 8UOJr5cBPiy9PEnmbdLE\n",
      "Document indexed successfully with ID: G3CJr5cBgduF7OtYbq45\n",
      "Document indexed successfully with ID: 8kOJr5cBPiy9PEnmbtKz\n",
      "Document indexed successfully with ID: HHCJr5cBgduF7OtYb64s\n",
      "Document indexed successfully with ID: 80OJr5cBPiy9PEnmb9Kr\n",
      "Document indexed successfully with ID: HXCJr5cBgduF7OtYcK4v\n",
      "Document indexed successfully with ID: 9EOJr5cBPiy9PEnmcNKn\n",
      "Document indexed successfully with ID: HnCJr5cBgduF7OtYca4k\n",
      "Document indexed successfully with ID: 9UOJr5cBPiy9PEnmcdKo\n",
      "Document indexed successfully with ID: H3CJr5cBgduF7OtYcq4r\n",
      "Document indexed successfully with ID: 9kOJr5cBPiy9PEnmctKd\n",
      "Document indexed successfully with ID: IHCJr5cBgduF7OtYc64d\n",
      "Document indexed successfully with ID: 90OJr5cBPiy9PEnmc9KU\n",
      "Document indexed successfully with ID: IXCJr5cBgduF7OtYdK4W\n",
      "Document indexed successfully with ID: -EOJr5cBPiy9PEnmdNKH\n",
      "Document indexed successfully with ID: InCJr5cBgduF7OtYdK7y\n",
      "Document indexed successfully with ID: -UOJr5cBPiy9PEnmddJk\n",
      "Document indexed successfully with ID: I3CJr5cBgduF7OtYda7X\n",
      "Document indexed successfully with ID: -kOJr5cBPiy9PEnmdtJh\n",
      "Document indexed successfully with ID: JHCJr5cBgduF7OtYdq7c\n",
      "Document indexed successfully with ID: -0OJr5cBPiy9PEnmd9Kv\n",
      "Document indexed successfully with ID: JXCJr5cBgduF7OtYeK6j\n",
      "Document indexed successfully with ID: _EOJr5cBPiy9PEnmedKA\n",
      "Document indexed successfully with ID: JnCJr5cBgduF7OtYea78\n",
      "Document indexed successfully with ID: _UOJr5cBPiy9PEnmetJx\n",
      "Document indexed successfully with ID: J3CJr5cBgduF7OtYeq7_\n",
      "Document indexed successfully with ID: _kOJr5cBPiy9PEnme9KD\n",
      "Document indexed successfully with ID: KHCJr5cBgduF7OtYe678\n",
      "Document indexed successfully with ID: _0OJr5cBPiy9PEnmfNKQ\n",
      "Document indexed successfully with ID: KXCJr5cBgduF7OtYfa4O\n",
      "Document indexed successfully with ID: AEOJr5cBPiy9PEnmfdOA\n",
      "Document indexed successfully with ID: KnCJr5cBgduF7OtYfa78\n",
      "Document indexed successfully with ID: AUOJr5cBPiy9PEnmftN1\n",
      "Document indexed successfully with ID: K3CJr5cBgduF7OtYfq71\n",
      "Document indexed successfully with ID: AkOJr5cBPiy9PEnmf9N7\n",
      "Document indexed successfully with ID: LHCJr5cBgduF7OtYf67r\n",
      "Document indexed successfully with ID: A0OJr5cBPiy9PEnmgNN6\n",
      "Document indexed successfully with ID: LXCJr5cBgduF7OtYgK73\n",
      "Document indexed successfully with ID: BEOJr5cBPiy9PEnmgdNx\n",
      "Document indexed successfully with ID: LnCJr5cBgduF7OtYga7k\n",
      "Document indexed successfully with ID: BUOJr5cBPiy9PEnmgtNy\n",
      "Document indexed successfully with ID: L3CJr5cBgduF7OtYgq7m\n",
      "Document indexed successfully with ID: BkOJr5cBPiy9PEnmg9N-\n",
      "Document indexed successfully with ID: MHCJr5cBgduF7OtYhK4J\n",
      "Document indexed successfully with ID: B0OJr5cBPiy9PEnmhNOA\n",
      "Document indexed successfully with ID: MXCJr5cBgduF7OtYha4i\n",
      "Document indexed successfully with ID: CEOJr5cBPiy9PEnmhdOs\n",
      "Document indexed successfully with ID: MnCJr5cBgduF7OtYhq48\n",
      "Document indexed successfully with ID: CUOJr5cBPiy9PEnmhtO4\n",
      "Document indexed successfully with ID: M3CJr5cBgduF7OtYh641\n",
      "Document indexed successfully with ID: CkOJr5cBPiy9PEnmh9Ou\n",
      "Document indexed successfully with ID: NHCJr5cBgduF7OtYiK40\n",
      "Document indexed successfully with ID: C0OJr5cBPiy9PEnmiNPE\n",
      "Document indexed successfully with ID: NXCJr5cBgduF7OtYia5B\n",
      "Document indexed successfully with ID: DEOJr5cBPiy9PEnmidO7\n",
      "Document indexed successfully with ID: NnCJr5cBgduF7OtYiq4-\n",
      "Document indexed successfully with ID: DUOJr5cBPiy9PEnmitOy\n",
      "Document indexed successfully with ID: N3CJr5cBgduF7OtYi64n\n",
      "Document indexed successfully with ID: DkOJr5cBPiy9PEnmi9Oi\n",
      "Document indexed successfully with ID: OHCJr5cBgduF7OtYjK4V\n",
      "Document indexed successfully with ID: D0OJr5cBPiy9PEnmjNOd\n",
      "Document indexed successfully with ID: OXCJr5cBgduF7OtYja4d\n",
      "Document indexed successfully with ID: EEOJr5cBPiy9PEnmjdOa\n",
      "Document indexed successfully with ID: OnCJr5cBgduF7OtYjq4Q\n",
      "Document indexed successfully with ID: EUOJr5cBPiy9PEnmjtOP\n",
      "Document indexed successfully with ID: O3CJr5cBgduF7OtYj64D\n",
      "Document indexed successfully with ID: EkOJr5cBPiy9PEnmj9N6\n",
      "Document indexed successfully with ID: PHCJr5cBgduF7OtYj67s\n",
      "Document indexed successfully with ID: E0OJr5cBPiy9PEnmkNNh\n",
      "Document indexed successfully with ID: PXCJr5cBgduF7OtYkK7f\n",
      "Document indexed successfully with ID: FEOJr5cBPiy9PEnmkdNa\n",
      "Document indexed successfully with ID: PnCJr5cBgduF7OtYka7i\n",
      "Document indexed successfully with ID: FUOJr5cBPiy9PEnmktNW\n",
      "Document indexed successfully with ID: P3CJr5cBgduF7OtYkq7R\n",
      "Document indexed successfully with ID: FkOJr5cBPiy9PEnmk9NM\n",
      "Document indexed successfully with ID: QHCJr5cBgduF7OtYk67a\n",
      "Document indexed successfully with ID: F0OJr5cBPiy9PEnmlNNv\n",
      "Document indexed successfully with ID: QXCJr5cBgduF7OtYlK7q\n",
      "Document indexed successfully with ID: GEOJr5cBPiy9PEnmldNp\n",
      "Document indexed successfully with ID: QnCJr5cBgduF7OtYla7p\n",
      "Document indexed successfully with ID: GUOJr5cBPiy9PEnmltOZ\n",
      "Document indexed successfully with ID: Q3CJr5cBgduF7OtYl64c\n",
      "Document indexed successfully with ID: GkOJr5cBPiy9PEnml9Og\n",
      "Document indexed successfully with ID: RHCJr5cBgduF7OtYmK4m\n",
      "Document indexed successfully with ID: G0OJr5cBPiy9PEnmmNOW\n",
      "Document indexed successfully with ID: RXCJr5cBgduF7OtYma4R\n",
      "Document indexed successfully with ID: HEOJr5cBPiy9PEnmmdOW\n",
      "Document indexed successfully with ID: RnCJr5cBgduF7OtYmq4W\n",
      "Document indexed successfully with ID: HUOJr5cBPiy9PEnmmtOR\n",
      "Document indexed successfully with ID: R3CJr5cBgduF7OtYm64L\n",
      "Document indexed successfully with ID: HkOJr5cBPiy9PEnmm9OA\n",
      "Document indexed successfully with ID: SHCJr5cBgduF7OtYm67t\n",
      "Document indexed successfully with ID: H0OJr5cBPiy9PEnmnNNw\n",
      "Document indexed successfully with ID: SXCJr5cBgduF7OtYnK7r\n",
      "Document indexed successfully with ID: IEOJr5cBPiy9PEnmndNf\n",
      "Document indexed successfully with ID: SnCJr5cBgduF7OtYna7t\n",
      "Document indexed successfully with ID: IUOJr5cBPiy9PEnmntNh\n",
      "Document indexed successfully with ID: S3CJr5cBgduF7OtYnq7T\n",
      "Document indexed successfully with ID: IkOJr5cBPiy9PEnmn9NM\n",
      "Document indexed successfully with ID: THCJr5cBgduF7OtYn67A\n",
      "Document indexed successfully with ID: I0OJr5cBPiy9PEnmoNM_\n",
      "Document indexed successfully with ID: TXCJr5cBgduF7OtYoK6_\n",
      "Document indexed successfully with ID: JEOJr5cBPiy9PEnmodMt\n",
      "Document indexed successfully with ID: TnCJr5cBgduF7OtYoa6e\n",
      "Document indexed successfully with ID: JUOJr5cBPiy9PEnmotMP\n",
      "Document indexed successfully with ID: T3CJr5cBgduF7OtYoq52\n",
      "Document indexed successfully with ID: JkOJr5cBPiy9PEnmotPs\n",
      "Document indexed successfully with ID: UHCJr5cBgduF7OtYo65r\n",
      "Document indexed successfully with ID: J0OJr5cBPiy9PEnmo9Pg\n",
      "Document indexed successfully with ID: UXCJr5cBgduF7OtYpK5g\n",
      "Document indexed successfully with ID: KEOJr5cBPiy9PEnmpNPX\n",
      "Document indexed successfully with ID: UnCJr5cBgduF7OtYpa5J\n",
      "Document indexed successfully with ID: KUOJr5cBPiy9PEnmpdPE\n",
      "Document indexed successfully with ID: U3CJr5cBgduF7OtYpq5G\n",
      "Document indexed successfully with ID: KkOJr5cBPiy9PEnmptPL\n",
      "Document indexed successfully with ID: VHCJr5cBgduF7OtYp65S\n",
      "Document indexed successfully with ID: K0OJr5cBPiy9PEnmp9PX\n",
      "Document indexed successfully with ID: VXCJr5cBgduF7OtYqK5a\n",
      "Document indexed successfully with ID: LEOJr5cBPiy9PEnmqNPW\n",
      "Document indexed successfully with ID: VnCJr5cBgduF7OtYqa5Q\n",
      "Document indexed successfully with ID: LUOJr5cBPiy9PEnmqdPb\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "\"\"\"\n",
    "This script demonstrates indexing documents into an Amazon OpenSearch Service domain using AWS Identity and Access Management (IAM) for authentication.\n",
    "\"\"\"\n",
    "# Embedding Model\n",
    "model=\"titanv2\" \n",
    "# Use OpenSearch Servelss or Not\n",
    "openserach_serverless=False\n",
    "service = 'aoss' if openserach_serverless else 'es'\n",
    "# replace wit your OpenSearch Service domain/Serverless endpoint\n",
    "domain_endpoint = \"search-finbud-rag-domain-a6nh2uh2cfiitygwr4snnyf3hm.us-east-2.es.amazonaws.com\" if openserach_serverless else \"search-finbud-rag-domain-a6nh2uh2cfiitygwr4snnyf3hm.us-east-2.es.amazonaws.com\" \n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth =  AWSV4SignerAuth(credentials, \"us-east-2\", service)\n",
    "os_ = OpenSearch(\n",
    "    hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "    http_auth = ('giaptom', 'hoangGiap1204@'),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    timeout=120,        \n",
    "    # http_compress = True, # enables gzip compression for request bodies\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Sample Opensearch domain index mapping\n",
    "mapping = {\n",
    "  'settings': {\n",
    "    'index': {  \n",
    "      'knn': True,\n",
    "      \"knn.algo_param.ef_search\": 100,            \n",
    "    }\n",
    "      },\n",
    "\n",
    "      'mappings': {  \n",
    "        'properties': {\n",
    "          'embedding': {\n",
    "            'type': 'knn_vector', \n",
    "            'dimension':model_dimension_mapping[model], #change as per sequence length of Embedding Model\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",       \n",
    "              \"space_type\": \"cosinesimil\",\n",
    "              \"engine\": \"nmslib\",\n",
    "              \"parameters\": {\n",
    "                 \"ef_construction\": 256,\n",
    "                 \"m\":  48\n",
    "               }\n",
    "            }\n",
    "          },\n",
    "\n",
    "          'passage': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "          'doc_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "        \n",
    "          'table': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "                \n",
    "          'list': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "        \n",
    "          'title_headers': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "          'section_header_ids': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "          'section_title_ids': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "domain_index =f\"test2-{model}-new\" #domain index name\n",
    "\n",
    "if not os_.indices.exists(index=domain_index):        \n",
    "    os_.indices.create(index=domain_index, body=mapping)\n",
    "    # Verify that the index has been created\n",
    "    if os_.indices.exists(index=domain_index):\n",
    "        print(f\"Index {domain_index} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create index '{domain_index}'.\")\n",
    "else:\n",
    "    print(f'{domain_index} Index already exists!')\n",
    "\n",
    "i = 1\n",
    "SAGEMAKER=boto3.client('sagemaker-runtime')\n",
    "for ids, chunkks in chunks.items(): # Iterate through the page title chunks \n",
    "    index_adjuster=len(chunk_header_mapping[ids])%len(chunkks)\n",
    "    for chunk_ids,chunk in enumerate(chunkks): # iterating through section header chunks   \n",
    "        chunk_ids+=index_adjuster\n",
    "        passage_chunk=\"\\n\".join(chunk).replace(\"<title>\",\"\").replace(\"</title>\",\"\")\n",
    "        if passage_chunk.strip():\n",
    "            embedding=_get_emb_(passage_chunk, model)       \n",
    "            table=[]\n",
    "            if ids in table_header_dict:\n",
    "                if [x for x in table_header_dict[ids] if x ==chunk_ids]:                \n",
    "                    table=\"\\n\".join(table_header_dict[ids][chunk_ids])\n",
    "            lists=[]\n",
    "            if ids in list_header_dict:\n",
    "                if [x for x in list_header_dict[ids] if x ==chunk_ids]:                \n",
    "                    lists=\"\\n\".join(list_header_dict[ids][chunk_ids])\n",
    "            documentt = { \n",
    "                'doc_id':doc_id, #doc name   \n",
    "                'passage': passage_chunk,\n",
    "                'embedding': embedding,\n",
    "                'table':table,\n",
    "                \"list\":lists,    \n",
    "                \"section_header_ids\":chunk_ids, #Store id of the header section\n",
    "                \"section_title_ids\":ids #Store id of the title section\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = os_.index(index=domain_index, body=documentt)\n",
    "                i += 1\n",
    "                # Check the response to see if the indexing was successful\n",
    "                if response[\"result\"] == \"created\":\n",
    "                    print(f\"Document indexed successfully with ID: {response['_id']}\")\n",
    "                else:\n",
    "                    print(\"Failed to index document.\")\n",
    "            except RequestError as e:\n",
    "                logging.error(f\"Error indexing document to index '{domain_index}': {e}\")\n",
    "        else:\n",
    "            continue        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5cdb71-ec8a-4730-87df-85177bef0e40",
   "metadata": {},
   "source": [
    "**`NOTE`**: This part of this code will not work with OpenSearch Serverless, only works for Amazon OpenSearch Service (Provisioned).\\\n",
    "This search processor only works for Amazon OpenSearch Service version 2.11 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c02911-592f-43fd-807c-eb5ab3419801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import Transport\n",
    "\n",
    "\"\"\"\n",
    "This script initializes a transport client for Amazon OpenSearch Service and creates a search pipeline with a normalization-processor. \n",
    "The normalization technique in the processor is set to min_max, and the combination technique is set to arithmetic_mean.\n",
    "\"\"\"\n",
    "if not openserach_serverless:\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWSV4SignerAuth(credentials, \"us-east-1\", service)\n",
    "    transport = Transport(\n",
    "       hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "        http_auth = ('giaptom', 'hoangGiap1204@'),\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        timeout=120,        \n",
    "        # http_compress = True, # enables gzip compression for request bodies\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    pipeline_name=\"norm-pipeline001\"\n",
    "    pipeline_definition = {\n",
    "        \"description\": \"Post-processor for hybrid search\",\n",
    "        \"phase_results_processors\": [\n",
    "            {\n",
    "                \"normalization-processor\": {\n",
    "                    \"normalization\": {\n",
    "                        \"technique\": \"min_max\"\n",
    "                    },\n",
    "                    \"combination\": {\n",
    "                        \"technique\": \"arithmetic_mean\",\n",
    "                        \"parameters\": {\n",
    "                            \"weights\": [\n",
    "                              0.5,\n",
    "                              0.5\n",
    "                            ]\n",
    "                        }                    \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send the PUT request to create the search pipeline\n",
    "    response = transport.perform_request(\"PUT\", f\"/_search/pipeline/{pipeline_name}\", body=pipeline_definition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7c3cc-beb5-48e2-abaa-66b46c5e8373",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e8791-8b4f-4999-b42a-b89d3a7d3d01",
   "metadata": {},
   "source": [
    "Custom approach to combine lexical and keyword search from OpenSearch Serverless as this is not natively intergrated in the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb35407-0b7b-4858-97dc-4c4b1baa4e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_scores_(scores,normalizer):\n",
    "    \"\"\"\n",
    "    Normalize scores using L2/min-max normalization.\n",
    "    :param scores: The list of scores to normalize.\n",
    "    :param mormalizer: normalizing tekniq\n",
    "    :return: The normalized scores.\n",
    "    \"\"\"\n",
    "    if \"minmax\" in normalizer:\n",
    "        scores = np.array(scores)\n",
    "        return (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
    "    elif \"l2\" in normalizer:\n",
    "        scores = np.array(scores)\n",
    "        return scores / np.linalg.norm(scores)\n",
    "    else:\n",
    "        raise \"enter either minmax or l2 as normalizer\"\n",
    "        \n",
    "def interpolate_scores(lexical_score, semantic_score, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Interpolate lexical and semantic scores using a weighted sum.\n",
    "    :param lexical_score: The normalized score from the lexical search.\n",
    "    :param semantic_score: The normalized score from the semantic search.\n",
    "    :param alpha: The interpolation weight (default: 0.5).\n",
    "    :return: The interpolated score.\n",
    "    \"\"\"\n",
    "    return alpha * lexical_score + (1 - alpha) * semantic_score\n",
    "\n",
    "def reciprocal_rank_fusion(lexical_results, semantic_results, k=60):\n",
    "    \"\"\"\n",
    "    Combine lexical and semantic search results using Reciprocal Rank Fusion (RRF).\n",
    "    :param lexical_results: The results from the lexical search.\n",
    "    :param semantic_results: The results from the semantic search.\n",
    "    :param k: The parameter for RRF (default: 60).\n",
    "    :return: The combined search results.\n",
    "    \"\"\"\n",
    "    combined_results = {}\n",
    "\n",
    "    for hit in lexical_results['hits']['hits']:\n",
    "        doc_id = hit['_id']\n",
    "        if doc_id not in combined_results:\n",
    "            combined_results[doc_id] = {'_id': doc_id, '_source': hit['_source'], '_score': 0}\n",
    "        combined_results[doc_id]['_score'] += 1 / (k + hit['_score'])\n",
    "\n",
    "    for hit in semantic_results['hits']['hits']:\n",
    "        doc_id = hit['_id']\n",
    "        if doc_id not in combined_results:\n",
    "            combined_results[doc_id] = {'_id': doc_id, '_source': hit['_source'], '_score': 0}\n",
    "        combined_results[doc_id]['_score'] += 1 / (k + hit['_score'])\n",
    "\n",
    "    combined_results = list(combined_results.values())\n",
    "    combined_results = sorted(combined_results, key=lambda x: x['_score'], reverse=True)\n",
    "\n",
    "    return {'hits': {'hits': combined_results}}\n",
    "\n",
    "def hybrid_search(top_K_results,lexical_results, semantic_results, interpolation_weight=0.5, normalizer=\"minmax\",use_rrf=False, rrf_k=60):\n",
    "    \"\"\"\n",
    "    Perform hybrid search by combining lexical and semantic search results.\n",
    "    :param lexical_results: The results from the lexical search.\n",
    "    :param semantic_results: The results from the semantic search.\n",
    "    :param interpolation_weight: The interpolation weight for score interpolation.\n",
    "    :param normalizer: The normalization function (default: minmax normalization).\n",
    "    :return: The combined search results.\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_rrf:\n",
    "        return reciprocal_rank_fusion(lexical_results, semantic_results, k=rrf_k)\n",
    "    \n",
    "    combined_results = []\n",
    "\n",
    "    # Normalize the scores from lexical and semantic searches\n",
    "    lexical_scores = [hit['_score'] for hit in lexical_results['hits']['hits']]\n",
    "    semantic_scores = [hit['_score'] for hit in semantic_results['hits']['hits']]\n",
    "    normalized_lexical_scores = normalize_scores_(lexical_scores,normalizer)\n",
    "    normalized_semantic_scores = normalize_scores_(semantic_scores,normalizer)\n",
    "\n",
    "    # Combine the results based on document IDs\n",
    "    lexical_docs = {hit['_id']: (hit, score) for hit, score in zip(lexical_results['hits']['hits'], normalized_lexical_scores)}\n",
    "    semantic_docs = {hit['_id']: (hit, score) for hit, score in zip(semantic_results['hits']['hits'], normalized_semantic_scores)}\n",
    "\n",
    "    for doc_id in set(lexical_docs.keys()) | set(semantic_docs.keys()):\n",
    "        lexical_hit, lexical_score = lexical_docs.get(doc_id, (None, 0))\n",
    "        semantic_hit, semantic_score = semantic_docs.get(doc_id, (None, 0))\n",
    "\n",
    "        if lexical_hit and semantic_hit:\n",
    "            # Interpolate scores if both lexical and semantic results are available\n",
    "            interpolated_score = interpolate_scores(lexical_score, semantic_score, interpolation_weight)      \n",
    "            combined_hit = {\n",
    "                '_id': doc_id,\n",
    "                '_source': {**lexical_hit['_source']},\n",
    "                '_score': interpolated_score,     \n",
    "            }\n",
    "        elif lexical_hit:\n",
    "            # Use lexical hit if only lexical result is available\n",
    "            combined_hit = {\n",
    "                '_id': doc_id,\n",
    "                '_source': lexical_hit['_source'],\n",
    "                '_score': lexical_score\n",
    "            }\n",
    "        else:\n",
    "            # Use semantic hit if only semantic result is available\n",
    "            combined_hit = {\n",
    "                '_id': doc_id,\n",
    "                '_source': semantic_hit['_source'],\n",
    "                '_score': semantic_score\n",
    "            }\n",
    "        combined_results.append(combined_hit)\n",
    "    # Sort the combined results by the blended score\n",
    "    combined_results = sorted(combined_results, key=lambda hit: hit['_score'], reverse=True)\n",
    "    return {'hits': {'hits': combined_results[:top_K_results]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022372f4-4e51-44aa-b5aa-27042cf38885",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HYBRID SEARCH AND FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab3f7c64-1ddb-4c4a-a050-67669632790b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'test2-titanv2-new',\n",
       "  '_id': 'QXCHr5cBgduF7OtYkKw_',\n",
       "  '_score': 1.0,\n",
       "  '_source': {'section_header_ids': 12,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\nCaution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\\nIF the person is your|AND|THEN that person is\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|you can't claim your relative as a dependent|not a qualifying person.\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': 'Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.\\nIF the person is your|AND|THEN that person is\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is single|a qualifying person, whether or not the child meets the Citizen or Resident Test in chapter 3.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can claim the child as a dependent|a qualifying person.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can\\'t claim the child as a dependent|not a qualifying person.\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can claim your parent as a dependent⁵|a qualifying person.⁶\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can\\'t claim your parent as a dependent|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative lived with you more than half the year, and your relative is related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and you can claim your relative as a dependent⁵|a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative didn\\'t live with you more than half the year|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative isn\\'t related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and is your qualifying relative only because your relative lived with you all year as a member of your household|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|you can\\'t claim your relative as a dependent|not a qualifying person.\\n'}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'GEOHr5cBPiy9PEnmkNG0',\n",
       "  '_score': 0.948135,\n",
       "  '_source': {'section_header_ids': 13,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': 'Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\n1 A person can\\'t qualify more than one taxpayer to use the head of household filing status for the year. 2 The term \"qualifying child\" is defined in chapter 3. Note. If you are a noncustodial parent, the term \"qualifying child\" for head of household filing status doesn\\'t include a child who is your qualifying child only because of the rules described under Children of divorced or separated parents (or parents who live apart) under Qualifying Child in chapter 3. If you are the custodial parent and those rules apply, the child is generally your qualifying child for head of household filing status even though the child isn\\'t a qualifying child you can claim as a dependent. 3 This person is a qualifying person if the only reason you can\\'t claim the person as a dependent is that you, or your spouse if filing jointly, can be claimed as a dependent on another taxpayer\\'s return. 4 The term \"qualifying relative\" is defined in chapter 3. 5 If you can claim a person as a dependent only because of a multiple support agreement, that person isn\\'t a qualifying person. See Multiple Support Agreement in chapter 3. 6 See Special rule for parent under Qualifying Person, earlier.',\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'QHCHr5cBgduF7OtYj6w0',\n",
       "  '_score': 0.9463428,\n",
       "  '_source': {'section_header_ids': 10,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\nCaution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\\nIF the person is your|AND|THEN that person is\\nyou qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²|the child is single|a qualifying person, whether or not the child meets the Citizen or Resident Test in chapter 3.\\nyou qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²|the child is married and you can claim the child as a dependent|a qualifying person.\\nyou qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²|the child is married and you can't claim the child as a dependent|not a qualifying person.\\nqualifying relative⁴ who is your father or \\nmother|you can claim your parent as a dependent⁵|a qualifying person.⁶\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': 'Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.\\nIF the person is your|AND|THEN that person is\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is single|a qualifying person, whether or not the child meets the Citizen or Resident Test in chapter 3.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can claim the child as a dependent|a qualifying person.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can\\'t claim the child as a dependent|not a qualifying person.\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can claim your parent as a dependent⁵|a qualifying person.⁶\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can\\'t claim your parent as a dependent|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative lived with you more than half the year, and your relative is related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and you can claim your relative as a dependent⁵|a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative didn\\'t live with you more than half the year|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative isn\\'t related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and is your qualifying relative only because your relative lived with you all year as a member of your household|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|you can\\'t claim your relative as a dependent|not a qualifying person.\\n'}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'PnCHr5cBgduF7OtYjKzy',\n",
       "  '_score': 0.9241184,\n",
       "  '_source': {'section_header_ids': 6,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': 'Qualifying Person\\nAdopted child or foster child. You may be eligible to file as head of household if the per- son who qualifies you for this filing status was an adopted child or foster child. For more infor- mation, see Pub. 501.\\nKidnapped child. You may be eligible to file as head of household even if the child who is your qualifying person has been kidnapped. For more information, see Pub. 501.',\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'F0OHr5cBPiy9PEnmj9HC',\n",
       "  '_score': 0.9122465,\n",
       "  '_source': {'section_header_ids': 11,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\nCaution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\\nIF the person is your|AND|THEN that person is\\nqualifying relative⁴ who is your father or \\nmother|you can't claim your parent as a dependent|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative lived with you more than half the year, and your relative is related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3 and you can claim your relative as a dependent⁵|a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative didn't live with you more than half the year|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative isn't related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3 and is your qualifying relative only because your relative lived with you all year as a member of your household|not a qualifying person.\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': 'Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.\\nIF the person is your|AND|THEN that person is\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is single|a qualifying person, whether or not the child meets the Citizen or Resident Test in chapter 3.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can claim the child as a dependent|a qualifying person.\\n\"you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \\ncertain other tests)²\"|the child is married and you can\\'t claim the child as a dependent|not a qualifying person.\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can claim your parent as a dependent⁵|a qualifying person.⁶\\n\"qualifying relative⁴ who is your father or \\nmother\"|you can\\'t claim your parent as a dependent|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative lived with you more than half the year, and your relative is related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and you can claim your relative as a dependent⁵|a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative didn\\'t live with you more than half the year|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative isn\\'t related to you in one of the ways listed under Relatives who don\\'t have to live with you in chapter 3 and is your qualifying relative only because your relative lived with you all year as a member of your household|not a qualifying person.\\ntests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|you can\\'t claim your relative as a dependent|not a qualifying person.\\n'}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'QnCHr5cBgduF7OtYkawk',\n",
       "  '_score': 0.907725,\n",
       "  '_source': {'section_header_ids': 14,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\nthe Filing Status section. If you don't enter the name, it will take us longer to process your return.:\\nThis child lived in your home all year, ex- cept for temporary absences. See Tempo- rary absences, earlier, under Head of Household. There are also exceptions, de- scribed later, for a child who was born or died during the year and for a kidnapped child. \\nYou paid more than half of the cost of keeping up a home for the year. See Keep- ing Up a Home, earlier, under Head of Household.\\nExample. Your spouse died in 2022 and you haven't remarried. During 2023 and 2024 you continued to keep up a home for you and your child who lives with you and whom you can claim as a dependent. For 2022, you were enti- tled to file a joint return for you and your de- ceased spouse. For 2023 and 2024, you can file\\nas qualifying surviving spouse. After 2024, you can file as head of household if you qualify.\",\n",
       "   'list': \"the Filing Status section. If you don't enter the name, it will take us longer to process your return.\\nThis child lived in your home all year, ex- cept for temporary absences. See Tempo- rary absences, earlier, under Head of Household. There are also exceptions, de- scribed later, for a child who was born or died during the year and for a kidnapped child. \\nYou paid more than half of the cost of keeping up a home for the year. See Keep- ing Up a Home, earlier, under Head of Household.\",\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'D0OHr5cBPiy9PEnmhtHZ',\n",
       "  '_score': 0.7558888,\n",
       "  '_source': {'section_header_ids': 27,\n",
       "   'section_title_ids': 4,\n",
       "   'passage': \"Head of Household\\nYou may be able to file as head of household if you meet all of the following requirements.:\\n1. You are unmarried or considered unmar- ried on the last day of the year. See Marital Status, earlier, and Considered Unmar- ried, later. \\n2. You paid more than half of the cost of keeping up a home for the year. \\n3. A qualifying person lived with you in the home for more than half the year (except for temporary absences, such as school). However, if the qualifying person is your dependent parent, your dependent parent doesn't have to live with you. See Special rule for parent, later, under Qualifying Per- son.\\nIf you qualify to file as head of house- TIP hold, your tax rate will usually be lower than the rates for single or married fil- ing separately. You will also receive a higher standard deduction than if you file as single or married filing separately.\",\n",
       "   'list': \"You may be able to file as head of household if you meet all of the following requirements.\\n1. You are unmarried or considered unmar- ried on the last day of the year. See Marital Status, earlier, and Considered Unmar- ried, later. \\n2. You paid more than half of the cost of keeping up a home for the year. \\n3. A qualifying person lived with you in the home for more than half the year (except for temporary absences, such as school). However, if the qualifying person is your dependent parent, your dependent parent doesn't have to live with you. See Special rule for parent, later, under Qualifying Per- son.\",\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'PHCHr5cBgduF7OtYiqzH',\n",
       "  '_score': 0.7321296,\n",
       "  '_source': {'section_header_ids': 2,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Qualifying Person\\nSee Table 2-1 to see who is a qualifying person. Any person not described in Table 2-1 isn't a qualifying person.\\nExample 1-Child. Your unmarried child lived with you all year and was 18 years old at the end of the year. Your child didn't provide more than half of their own support and doesn't meet the tests to be a qualifying child of anyone else. As a result, this child is your qualifying child (see Qualifying Child in chapter 3) and, because this child is single, this is your qualify- ing person for head of household purposes.\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'GUOHr5cBPiy9PEnmkdGj',\n",
       "  '_score': 0.60995835,\n",
       "  '_source': {'section_header_ids': 15,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\\nDeath or birth. You may be eligible to file as a qualifying surviving spouse if the child who qualifies you for this filing status is born or dies during the year. You must have provided more than half of the cost of keeping up a home that was the child's main home during the entire part of the year the child was alive.\\nAdopted child. You may be eligible to file as a qualifying surviving spouse if the child who qualifies you for this filing status you adopted in 2024 or was lawfully placed with you for legal adoption by you in 2024. The child is consid- ered to have lived with you for all of 2024 if your main home was this child's main home for the entire time since this child was adopted or placed with you in 2024.\\nKidnapped child. You may be eligible to file as a qualifying surviving spouse even if the child who qualifies you for this filing status has been kidnapped. See Pub. 501 for more information.\\n!\\nCAUTION\\nspouse died.\\nAs mentioned earlier, the filing status qualifying surviving spouse is available for only 2 years following the year your\\n3.\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}},\n",
       " {'_index': 'test2-titanv2-new',\n",
       "  '_id': 'E0OHr5cBPiy9PEnmi9Fg',\n",
       "  '_score': 0.5582393,\n",
       "  '_source': {'section_header_ids': 3,\n",
       "   'section_title_ids': 5,\n",
       "   'passage': \"Qualifying Person\\nExample 2-Child who isn't qualifying person. The facts are the same as in Exam- ple 1, except your child was 25 years old at the end of the year and your child's gross income was $6,000. Because your child doesn't meet the age test (explained under Qualifying Child in chapter 3), your child isn't your qualifying child. Because the child doesn't meet the gross in- come test (explained under Qualifying Relative in chapter 3), the child isn't your qualifying rela- tive. As a result, this child isn't your qualifying person for head of household purposes.\\nExample 3-Friend. Your friend lived with you all year. Even though your friend may be your qualifying relative if the gross income and\\nsupport tests (explained in chapter 3) are met, your friend isn't your qualifying person for head of household purposes because your friend isn't related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3. See Table 2-1.\",\n",
       "   'list': [],\n",
       "   'doc_id': 'irs_p17.pdf',\n",
       "   'table': []}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, \"us-east-1\", service, session_token=credentials.token)\n",
    "transport = Transport(\n",
    "   hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "    http_auth = ('giaptom', 'hoangGiap1204@'),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    timeout=120,        \n",
    "    # http_compress = True, # enables gzip compression for request bodies\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "question=\"Who is a Qualifying Person qualifying you to file as head of household?\"\n",
    "embedding=_get_emb_(question, model)\n",
    "# Top K returned results\n",
    "top_K_results=10\n",
    "# Define the search query\n",
    "query = {   \n",
    "    'size': top_K_results,\n",
    "    \"_source\": {\n",
    "    \"exclude\": [\n",
    "      \"embedding\"\n",
    "    ]\n",
    "  },\n",
    "    \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "          {\n",
    "          \"match\": {\n",
    "              \"passage\": question\n",
    "            }\n",
    "        },\n",
    "          {\n",
    "          \"knn\": {\n",
    "          \"embedding\": {\n",
    "            \"vector\": embedding,\n",
    "            \"k\": 3\n",
    "          }\n",
    "        }\n",
    "        },       \n",
    "        \n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  #   \"post_filter\": {\n",
    "  #   \"match\": {\n",
    "  #     \"title_headers\": \"\"\n",
    "  #   }\n",
    "  # }\n",
    "}\n",
    "\n",
    "# IF using non-serverless opensearch, Send the search request with the defined query and search pipeline for normilization and combination of results\n",
    "if not openserach_serverless:\n",
    "    response = transport.perform_request(\"GET\", f\"/{domain_index}/_search?search_pipeline={pipeline_name}\", body=query)\n",
    "else:\n",
    "    search_requests = [\n",
    "        ({}, {\"query\": {\"match\": {\"passage\": question}}, \"size\": top_K_results, \"_source\": {\"exclude\": [\"embedding\"]}}),\n",
    "        ({}, {\"query\": {\"knn\": {\"embedding\": {\"vector\": embedding, \"k\": 3}}}, \"size\": top_K_results, \"_source\": {\"exclude\": [\"embedding\"]}})\n",
    "    ]\n",
    "\n",
    "    # Convert the search requests to NDJSON format\n",
    "    data = \"\"\n",
    "    for metadata, request in search_requests:\n",
    "        data += f\"{json.dumps(metadata)}\\n{json.dumps(request)}\\n\"\n",
    "    response = transport.perform_request(\"GET\", f\"/{domain_index}/_msearch\", body=data)\n",
    "    # Separate the results    \n",
    "    lexical_search_results = response['responses'][0]\n",
    "    semantic_search_results = response['responses'][1]\n",
    "    # Use the custom hybrid search function\n",
    "    hybrid_results = hybrid_search(top_K_results,lexical_search_results, semantic_search_results, \n",
    "                                   interpolation_weight=0.5, normalizer=\"minmax\", use_rrf=False, rrf_k=100)\n",
    "    \n",
    "    # Implement a combination technique or just pass one of either lexical or semantic search back\n",
    "    response= hybrid_results\n",
    "response['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f8a22-0afe-4c58-b3ab-c49843c16962",
   "metadata": {},
   "source": [
    "The capability to select hierarchical information of interest from the retrieved response is based on the corresponding matched passage chunk. This allows for flexibility in determining the amount of information provided to the LLM, accommodating queries that benefit from access to full section information.\n",
    "\n",
    "This approach entails storing chunk hierarchical sections and titles in an S3 bucket. If additional information beyond the passage chunk in the OpenSearch index is necessary, it retrieves the corresponding sections based on the indexed matching IDs associated with the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c3a127e-74c4-4500-a48d-084d026108a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_s3(bucket_name, key, section_content, title_id=None,section_id=None):\n",
    "    \"\"\"\n",
    "    Read a file from an S3 bucket and extract specific sections based on given parameters.\n",
    "    Parameters:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        key (str): The key (path) of the file in the S3 bucket.\n",
    "        section_content (str): Specifies the type of section content to extract.\n",
    "            Possible values: \"section_header\" or \"section_title\".\n",
    "        title_id (str or int, optional): The ID of the title. Required if `section_content` is \"section_header\".\n",
    "        section_id (str or int, optional): The ID of the section. Required if `section_content` is \"section_header\".\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted section content as a string. Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:   \n",
    "        response = s3.get_object(Bucket=bucket_name, Key=key)        \n",
    "        file_content = response['Body'].read().decode('utf-8')\n",
    "        file_content=json.loads(file_content)\n",
    "        if section_content==\"section_header\":\n",
    "            passage=file_content[str(title_id)][str(section_id)]\n",
    "        elif section_content==\"section_title\":     \n",
    "            # Join each sublist into a string\n",
    "            strings = [\"\\n\".join(sublist) for sublist in list(file_content[str(title_id)].values())]\n",
    "            # Convert to a set to remove duplicates\n",
    "            passage = OrderedDict.fromkeys(strings)\n",
    "        return \"\\n\".join(passage)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {key} from S3 bucket {bucket_name}:\", e)\n",
    "        return None\n",
    "\n",
    "class InvalidContentError(Exception):\n",
    "    pass\n",
    "\n",
    "# Extract relevant information from the search response\n",
    "def content_extraction_os_(response:str, table:bool, lyst:bool,section_content:str):\n",
    "    \"\"\"\n",
    "    Extracts content from the OpenSearch response based on specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    response (dict): The response from OpenSearch containing search results.\n",
    "    table (bool): A boolean indicating whether to include table content.\n",
    "    lyst (bool): A boolean indicating whether to include list content.\n",
    "    section_content (str): The type of content to extract. Allowed values are 'passage', 'section_header', or 'section_title'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing concatenated passages and tables.\n",
    "    \"\"\"\n",
    "    allowed_values = {\"passage\", \"section_header\", \"section_title\"}  # Define allowed values\n",
    "    if section_content not in allowed_values:\n",
    "        raise InvalidContentError(f\"Invalid content type '{section_content}'. Allowed values are {', '.join(allowed_values)}.\")\n",
    "    \n",
    "    res=response['hits']['hits']\n",
    "    score = [str(x['_score']) for x in res]  #retrieval score    \n",
    "    # title_names = [x['_source']['title_headers'] for x in res] #doc page number of chunks\n",
    "    doc_name = [x['_source']['doc_id'] for x in res] # doc names\n",
    "    header_ids = [x['_source']['section_header_ids'] for x in res] # section header id\n",
    "    title_ids=[x['_source']['section_title_ids'] for x in res] # section title id\n",
    "    tables=\"\"\n",
    "    lists=\"\"\n",
    "    \n",
    "    if section_content==\"passage\":\n",
    "        passage = [x['_source'][\"passage\"] for x in res] #retrieved passages, here you can choose to retrieve the  complete section header or title instead of the chunk passage\n",
    "        tables=[x['_source']['table'] for x in res] # tables in the corresponding chunk\n",
    "        lists=[x['_source']['list'] for x in res]\n",
    "    else:\n",
    "        passage=[]\n",
    "        for x in range(len(title_ids)):\n",
    "            passage.append(read_file_from_s3(BUCKET, f\"{doc_name[x]}.json\",section_content,title_ids[x],header_ids[x]))\n",
    "        passage=set(passage)      \n",
    "    p = inflect.engine()\n",
    "    ## Concatenate passages and tables to use in prompt template \n",
    "    passages=\"\"\n",
    "    tab=\"\"\n",
    "    lst=\"\"\n",
    "    for  ids,text in enumerate(passage):\n",
    "        passages+=f\"<{p.ordinal(ids+1)}_passage>\\n{text}\\n</{p.ordinal(ids+1)}_passage>\\n\"\n",
    "    if table and tables:\n",
    "        for  ids,text in enumerate(tables):            \n",
    "            tab+=f\"<{p.ordinal(ids+1)}_passage_table>\\n{text}\\n</{p.ordinal(ids+1)}_passage_table>\\n\"  #Table can be coupled with passage chunks to provide more information.\n",
    "    if lyst and lists:\n",
    "        for  ids,text in enumerate(lists):            \n",
    "            lst+=f\"<{p.ordinal(ids+1)}_passage_lists>\\n{text}\\n</{p.ordinal(ids+1)}_passage_lists>\\n\"  \n",
    "    return passages, tab, lst,passage,tables,lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18ec4037-5b45-42ff-bda8-c5dc888b7c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1st_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\n",
      "IF the person is your|AND|THEN that person is\n",
      "tests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|you can't claim your relative as a dependent|not a qualifying person.\n",
      "</1st_passage>\n",
      "<2nd_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "1 A person can't qualify more than one taxpayer to use the head of household filing status for the year. 2 The term \"qualifying child\" is defined in chapter 3. Note. If you are a noncustodial parent, the term \"qualifying child\" for head of household filing status doesn't include a child who is your qualifying child only because of the rules described under Children of divorced or separated parents (or parents who live apart) under Qualifying Child in chapter 3. If you are the custodial parent and those rules apply, the child is generally your qualifying child for head of household filing status even though the child isn't a qualifying child you can claim as a dependent. 3 This person is a qualifying person if the only reason you can't claim the person as a dependent is that you, or your spouse if filing jointly, can be claimed as a dependent on another taxpayer's return. 4 The term \"qualifying relative\" is defined in chapter 3. 5 If you can claim a person as a dependent only because of a multiple support agreement, that person isn't a qualifying person. See Multiple Support Agreement in chapter 3. 6 See Special rule for parent under Qualifying Person, earlier.\n",
      "</2nd_passage>\n",
      "<3rd_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\n",
      "IF the person is your|AND|THEN that person is\n",
      "you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \n",
      "certain other tests)²|the child is single|a qualifying person, whether or not the child meets the Citizen or Resident Test in chapter 3.\n",
      "you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \n",
      "certain other tests)²|the child is married and you can claim the child as a dependent|a qualifying person.\n",
      "you qualifying daughter, more than or child grandchild half (such the as year who a son, and lived meets with \n",
      "certain other tests)²|the child is married and you can't claim the child as a dependent|not a qualifying person.\n",
      "qualifying relative⁴ who is your father or \n",
      "mother|you can claim your parent as a dependent⁵|a qualifying person.⁶\n",
      "</3rd_passage>\n",
      "<4th_passage>\n",
      "Qualifying Person\n",
      "Adopted child or foster child. You may be eligible to file as head of household if the per- son who qualifies you for this filing status was an adopted child or foster child. For more infor- mation, see Pub. 501.\n",
      "Kidnapped child. You may be eligible to file as head of household even if the child who is your qualifying person has been kidnapped. For more information, see Pub. 501.\n",
      "</4th_passage>\n",
      "<5th_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "Caution. See the text of this chapter for the other requirements you must meet to claim head of household filing status.:\n",
      "IF the person is your|AND|THEN that person is\n",
      "qualifying relative⁴ who is your father or \n",
      "mother|you can't claim your parent as a dependent|not a qualifying person.\n",
      "tests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative lived with you more than half the year, and your relative is related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3 and you can claim your relative as a dependent⁵|a qualifying person.\n",
      "tests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative didn't live with you more than half the year|not a qualifying person.\n",
      "tests) qualifying or brother, mother or relative⁴ (such sister as who other a grandparent, meets than certain your father|your relative isn't related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3 and is your qualifying relative only because your relative lived with you all year as a member of your household|not a qualifying person.\n",
      "</5th_passage>\n",
      "<6th_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "the Filing Status section. If you don't enter the name, it will take us longer to process your return.:\n",
      "This child lived in your home all year, ex- cept for temporary absences. See Tempo- rary absences, earlier, under Head of Household. There are also exceptions, de- scribed later, for a child who was born or died during the year and for a kidnapped child. \n",
      "You paid more than half of the cost of keeping up a home for the year. See Keep- ing Up a Home, earlier, under Head of Household.\n",
      "Example. Your spouse died in 2022 and you haven't remarried. During 2023 and 2024 you continued to keep up a home for you and your child who lives with you and whom you can claim as a dependent. For 2022, you were enti- tled to file a joint return for you and your de- ceased spouse. For 2023 and 2024, you can file\n",
      "as qualifying surviving spouse. After 2024, you can file as head of household if you qualify.\n",
      "</6th_passage>\n",
      "<7th_passage>\n",
      "Head of Household\n",
      "You may be able to file as head of household if you meet all of the following requirements.:\n",
      "1. You are unmarried or considered unmar- ried on the last day of the year. See Marital Status, earlier, and Considered Unmar- ried, later. \n",
      "2. You paid more than half of the cost of keeping up a home for the year. \n",
      "3. A qualifying person lived with you in the home for more than half the year (except for temporary absences, such as school). However, if the qualifying person is your dependent parent, your dependent parent doesn't have to live with you. See Special rule for parent, later, under Qualifying Per- son.\n",
      "If you qualify to file as head of house- TIP hold, your tax rate will usually be lower than the rates for single or married fil- ing separately. You will also receive a higher standard deduction than if you file as single or married filing separately.\n",
      "</7th_passage>\n",
      "<8th_passage>\n",
      "Qualifying Person\n",
      "See Table 2-1 to see who is a qualifying person. Any person not described in Table 2-1 isn't a qualifying person.\n",
      "Example 1-Child. Your unmarried child lived with you all year and was 18 years old at the end of the year. Your child didn't provide more than half of their own support and doesn't meet the tests to be a qualifying child of anyone else. As a result, this child is your qualifying child (see Qualifying Child in chapter 3) and, because this child is single, this is your qualify- ing person for head of household purposes.\n",
      "</8th_passage>\n",
      "<9th_passage>\n",
      "Table 2-1. Who Is a Qualifying Person Qualifying You To File as Head of Household?¹\n",
      "Death or birth. You may be eligible to file as a qualifying surviving spouse if the child who qualifies you for this filing status is born or dies during the year. You must have provided more than half of the cost of keeping up a home that was the child's main home during the entire part of the year the child was alive.\n",
      "Adopted child. You may be eligible to file as a qualifying surviving spouse if the child who qualifies you for this filing status you adopted in 2024 or was lawfully placed with you for legal adoption by you in 2024. The child is consid- ered to have lived with you for all of 2024 if your main home was this child's main home for the entire time since this child was adopted or placed with you in 2024.\n",
      "Kidnapped child. You may be eligible to file as a qualifying surviving spouse even if the child who qualifies you for this filing status has been kidnapped. See Pub. 501 for more information.\n",
      "!\n",
      "CAUTION\n",
      "spouse died.\n",
      "As mentioned earlier, the filing status qualifying surviving spouse is available for only 2 years following the year your\n",
      "3.\n",
      "</9th_passage>\n",
      "<10th_passage>\n",
      "Qualifying Person\n",
      "Example 2-Child who isn't qualifying person. The facts are the same as in Exam- ple 1, except your child was 25 years old at the end of the year and your child's gross income was $6,000. Because your child doesn't meet the age test (explained under Qualifying Child in chapter 3), your child isn't your qualifying child. Because the child doesn't meet the gross in- come test (explained under Qualifying Relative in chapter 3), the child isn't your qualifying rela- tive. As a result, this child isn't your qualifying person for head of household purposes.\n",
      "Example 3-Friend. Your friend lived with you all year. Even though your friend may be your qualifying relative if the gross income and\n",
      "support tests (explained in chapter 3) are met, your friend isn't your qualifying person for head of household purposes because your friend isn't related to you in one of the ways listed under Relatives who don't have to live with you in chapter 3. See Table 2-1.\n",
      "</10th_passage>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passages,tab,lyst,passage,tables,lists=content_extraction_os_(response, True,True, \"passage\")\n",
    "print(passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b31839-bd89-4948-a569-ac38a60bbac5",
   "metadata": {},
   "source": [
    "#### RERANKING (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcfb4e-e454-4e94-9cb8-64cec07a3606",
   "metadata": {},
   "source": [
    "This section touches on Reranking. It uses the deployed Sagemaker Jumpstart [BGE M3 model](https://huggingface.co/BAAI/bge-m3). To use this code logic you first have to deploy the model to an endpoint. After which you collect the endpoint name and replace in the logic.\n",
    "\n",
    "**NOTE**: You can skip this section if you do not have a deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b1fcc-2a7c-4834-82e4-3c003ca2507c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initialize sagemaker endpoint\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# SageMaker Jumpstart BGE M3 model endpoint\n",
    "bge_m3_endpoint_name=\"JUMPSTART BGE M3 ENDPOINT\"\n",
    "\n",
    "predictor=Predictor(endpoint_name=bge_m3_endpoint_name,\n",
    "                    serializer=JSONSerializer(),                    \n",
    "                    deserializer=JSONDeserializer(),\n",
    "                   )\n",
    "# Number of top K results to return from reranking\n",
    "top_k_reranked_result=3 \n",
    "payload_nearest_neighbour = {\"corpus\": passage, \"queries\": question, \"top_k\": 3, \"mode\": \"nn_corpus\"}\n",
    "query_response = predictor.predict(payload_nearest_neighbour)\n",
    "print(query_response)\n",
    "\n",
    "rerank_passage = []\n",
    "rerank_tables=[]\n",
    "rerank_list=[]\n",
    "for sublist in query_response:\n",
    "    for item in sublist:\n",
    "        rerank_passage.append(passage[item['corpus_id']])\n",
    "        if tables:\n",
    "            rerank_tables.append(tables[item['corpus_id']])\n",
    "        if lists:\n",
    "            rerank_list.append(lists[item['corpus_id']])\n",
    "p = inflect.engine()\n",
    "\n",
    "## Concatenate passages and tables to use in prompt template \n",
    "passages=\"\"\n",
    "tab=\"\"\n",
    "lst=\"\"\n",
    "for  ids,text in enumerate(rerank_passage):\n",
    "    passages+=f\"<{p.ordinal(ids+1)}_passage>\\n{text}\\n</{p.ordinal(ids+1)}_passage>\\n\"\n",
    "if tables:\n",
    "    for  ids,text in enumerate(rerank_tables):            \n",
    "        tab+=f\"<{p.ordinal(ids+1)}_passage_table>\\n{text}\\n</{p.ordinal(ids+1)}_passage_table>\\n\" \n",
    "if lists:\n",
    "    for  ids,text in enumerate(rerank_list):            \n",
    "        lst+=f\"<{p.ordinal(ids+1)}_passage_lists>\\n{text}\\n</{p.ordinal(ids+1)}_passage_lists>\\n\"  \n",
    "print(passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e4258-95a2-4000-97e1-12659f4eef3a",
   "metadata": {},
   "source": [
    "## Bedrock Anthropic LLM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1161d-5e69-4832-9d27-a8e417a2d4a8",
   "metadata": {},
   "source": [
    "Using the a prompt template with placeholders for the retrieved passages as `passages` under **document** tags and any retrieved standalone tables and list found within each retrieved passages as `tab` under **additional_information** tags.\\\n",
    "Change the `csv_seperator` variable name to what was used during chunking. default is \"|\" pipe character.\\\n",
    "Anthropic Claude models (Claude 3 and 2) is used to generate a response to the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edc0c8ab-5cf5-4200-829f-08c576db2d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of prompt token is 4217\n"
     ]
    }
   ],
   "source": [
    "csv_seperator=\"|\"\n",
    "prompt_template=f\"\"\"You are a helpful, obedient and truthful financial assistance.\n",
    "\n",
    "<document>\n",
    "{passages}\n",
    "</document>          \n",
    "\n",
    "<additional_information>\n",
    "{tab}\n",
    "</additional_information>\n",
    "\n",
    "<instructions>\n",
    "When providing your response based on the document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to passages with relevant information.\n",
    "3. Any tables provided within the document or additional information are delimited by {csv_seperator} character.\n",
    "4. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "5. If the answer is not available in the document, say so.\n",
    "</instructions>\n",
    "\n",
    "Question: {question}\n",
    "if able to answer:\n",
    "    Include in your response before your answer:    \n",
    "    <source>document or additional info tag(s) containing the relevant info</source>\"\"\"\n",
    "\n",
    "print(f' Size of prompt token is {client.count_tokens(prompt_template)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a878e4c1-b90a-4853-9e09-073408bbc2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<source>1st_passage_table, 2nd_passage_table, 3rd_passage_table</source>\n",
      "\n",
      "According to the tables in the document, Amazon's net sales are broken down by the following geographical locations:\n",
      "\n",
      "North America:\n",
      "2022: $315,880 million (61% of consolidated net sales)\n",
      "2023: $352,828 million (61% of consolidated net sales)\n",
      "\n",
      "International:\n",
      "2022: $118,007 million (23% of consolidated net sales)\n",
      "2023: $131,200 million (23% of consolidated net sales)\n",
      "\n",
      "The document also provides net sales figures for specific countries that represent a significant portion of consolidated net sales:\n",
      "\n",
      "United States:\n",
      "2021: $314,006 million\n",
      "2022: $356,113 million \n",
      "2023: $395,637 million\n",
      "\n",
      "Germany: \n",
      "2021: $37,326 million\n",
      "2022: $33,598 million\n",
      "2023: $37,588 million\n",
      "\n",
      "United Kingdom:\n",
      "2021: $31,914 million\n",
      "2022: $30,074 million\n",
      "2023: $33,591 million\n",
      "\n",
      "Japan:\n",
      "2021: $23,071 million\n",
      "2022: $24,396 million\n",
      "2023: $26,002 million\n",
      "Input Tokens: 4302\n",
      "Output Tokens: 288\n"
     ]
    }
   ],
   "source": [
    "model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-3-sonnet-20240229-v1:0\"\"anthropic.claude-v2\",\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_response,input_tokens, output_tokens=_invoke_bedrock_with_retries([], \"\", prompt_template,model_id , [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59db78a-71ce-418b-9566-fc7ca8167490",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ecd86-b1a9-4fb7-8b99-f5d382f53d0d",
   "metadata": {},
   "source": [
    "This notebook showcases the extraction of content from a document while maintaining its layout structure. Additionally, we processed and chunked the document, ensuring the integrity of the information was preserved. Furthermore, we indexed these chunks and associated hierarchical metadata information, offering flexibility in information retrieval. \n",
    "\n",
    "Finally, we conducted a RAG query and generated contextual answers."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "finbud-book-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
